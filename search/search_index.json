{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"DSM2 Documentation","text":""},{"location":"#general-description","title":"General Description","text":"<p>DSM2 is a river, estuary, and land modeling system:</p> <ul> <li>River: Can simulate riverine systems and has been extended from Sacramento to Shasta Dam. It has also been tested with high flow/stage simulations for flood modeling.</li> <li>Estuary: A completely flexible estuary model; stages and flows may be specified at boundary and internal points.</li> <li>Land: Includes effects from land-based processes, such as consumptive use and agricultural runoff.</li> </ul> <p>DSM2 can calculate stages, flows, velocities, and many mass transport processes, including salts, multiple non-conservative constituents, temperature, THM formation potential, and individual particles.</p>"},{"location":"#overview","title":"Overview","text":"<p>See Overview</p>"},{"location":"#modules","title":"Modules","text":"<p>DSM2 currently consists of three modules, all included in the distribution:</p> <ol> <li>HYDRO: Simulates one-dimensional hydrodynamics, including flows, velocities, depth, and water surface elevations. HYDRO provides the flow input for QUAL and PTM.</li> <li>QUAL: Simulates one-dimensional fate and transport of conservative and non-conservative water quality constituents based on the flow field simulated by HYDRO.</li> <li>PTM: Simulates pseudo-3D transport of neutrally buoyant particles based on the flow field simulated by HYDRO. PTM has applications ranging from visualization of flow patterns to simulating discrete organisms such as fish eggs and larvae.</li> <li>GTM: Simulates generalized transport processes, including sediment transport and other mass transport phenomena, extending the capabilities of DSM2 to model additional environmental processes.</li> </ol>"},{"location":"#copyright","title":"Copyright","text":"<p>The model is copyrighted by the State of California, Department of Water Resources. It is licensed under the GNU General Public License, version 2. This means:</p> <ul> <li>It can be copied, distributed, and modified freely.</li> <li>You may not restrict others in their ability to copy, distribute, and modify it.</li> </ul> <p>See the License for more details. Also, note the list of protected routines.</p> <p>DSM2 release packages are available on the CNRA Open Data website.</p> <p>For comments or questions, please contact Min Yu at minyu@water.ca.gov.</p>"},{"location":"#for-more-information","title":"For More Information","text":"<ul> <li>Manual</li> <li>DSM2 Learning Series</li> <li>Tutorials</li> <li>DSM2 Source Code GitHub Repository</li> </ul>"},{"location":"dsm2_overview/","title":"DSM2 Tutorial Overview","text":""},{"location":"dsm2_overview/#dsm2-overview","title":"DSM2 Overview","text":"<p>The Delta Simulation Model II (DSM2) is a one-dimensional mathematical model for dynamic simulation of hydrodynamics, water quality, and particle tracking in a network of riverine or estuarine channels. DSM2 can calculate:</p> <ul> <li>Stages, flows, and velocities</li> <li>Mass transport processes for:</li> <li>Conservative constituents:<ul> <li>Salts</li> </ul> </li> <li>Non-conservative constituents:<ul> <li>Water temperature</li> <li>Dissolved oxygen</li> <li>Sediment</li> <li>Mercury</li> </ul> </li> <li>Transport of individual particles</li> </ul> <p>DSM2 provides a powerful simulation package for analyzing complex hydrodynamic, water quality, and ecological conditions in riverine and estuarine systems.</p>"},{"location":"dsm2_overview/#dsm2-modules","title":"DSM2 Modules","text":"<p>DSM2 currently consists of four modules, as shown in Figure 1 (Schematic of DSM2 Modules):</p> <p></p> <ul> <li>HYDRO: Simulates one-dimensional hydrodynamics, including flows, velocities, depth, and water surface elevations. HYDRO provides the flow input for QUAL and PTM.</li> <li>QUAL: Simulates one-dimensional fate and transport of conservative and non-conservative water quality constituents based on the flow field simulated by HYDRO. This is the older transport model based on BLTM (reference needed) that uses a Lagrangian method with a moving frame of reference.</li> <li>GTM: Simulates transport of conservative and non-conservative water quality constituents using an Eulerian-Lagrangian method with fixed cells. This is also the base model for sediment and mercury modeling.</li> <li>PTM: Simulates pseudo-3D transport of neutrally buoyant particles based on the flow field simulated by HYDRO. PTM has multiple applications, ranging from visualization of flow patterns to simulation of discrete organisms such as fish eggs and larvae.</li> </ul>"},{"location":"dsm2_overview/#dsm2-study-types","title":"DSM2 Study Types","text":"<p>DSM2 is typically used for three types of Delta simulations: historic conditions, forecasting future conditions (real-time), and planning studies. Each type is described below:</p>"},{"location":"dsm2_overview/#recreate-historic-conditions","title":"Recreate Historic Conditions","text":"<p>Historical simulations replicate past operations, hydrologic conditions, water quality, and Delta configurations. These simulations enable calibration and validation of the model by comparing simulation results with field data. Historical simulations also augment available field data to provide a more spatially and temporally complete representation of hydrodynamic and water quality conditions for that time period.</p>"},{"location":"dsm2_overview/#forecasting-simulations","title":"Forecasting Simulations","text":"<p>Forecasting simulations, also known as real-time simulations, use recent field data and forecast data to project Delta conditions into the near future (typically one to ten weeks). Recently collected historical data provide current conditions for the Delta. Recent tidal elevations at Martinez are used with an astronomical tide forecast to project the Martinez tide into the near future. Corresponding hydrodynamic and water quality conditions in the Delta are then simulated. Forecasting simulations can assist State Water Project operations decisions.</p>"},{"location":"dsm2_overview/#planning-studies-of-hypothetical-conditions","title":"Planning Studies of Hypothetical Conditions","text":"<p>Delta planning studies evaluate how hypothetical changes to factors such as hydrologic regimes, water quality standards, system operations, and Delta configurations may impact Delta conditions. To explore the impacts of a given scenario under various hydrologic conditions, DSM2 planning studies are typically run under a 16-year sequence of Delta inflows and exports derived from statewide water transfer and storage simulations using CalSim models. More information on CalSim models can be found on the CalSim website.</p> <p>Planning simulations can use historical or astronomical tidal data, which incorporate influences of the spring-neap tidal cycle, or simulations can use an average repeating tide (typically the 19-year mean tide). Planning simulations typically assess impacts of proposed changes to Delta operations or configurations, such as modified reservoir releases or dredging of channels. Planning studies may also investigate impacts of hypothesized changes in the natural environment, such as sea level rise.</p>"},{"location":"dsm2_overview/#dsm2-modes-of-operation","title":"DSM2 Modes of Operation","text":""},{"location":"dsm2_overview/#parameter-descriptions-for-three-modes-of-dsm2-application","title":"Parameter Descriptions for Three Modes of DSM2 Application","text":"Simulation Parameter Replicate Historic Conditions Forecasting Future Conditions Planning Studies for Hypothetical Conditions Boundary Tide Historic or astronomical tide Historic and projected astronomical forecast tide Historic, astronomical Input Data Historic inflows and exportsAverage Delta consumptive use Recent and current inflows and exportsAverage Delta consumptive use CalSim-II statewide operations studies provide inflows and exportsAverage Delta consumptive use Simulation Period 1990-present are currently possible 1-10 weeks into the future 1921-2021 sequence from CalSim statewide operations studies"},{"location":"getting_started/","title":"Getting Started","text":"<p>Welcome to DSM2. This section of the documentation is intended to help you:</p> <ol> <li>Acquire and install DSM2.</li> <li>Test that it is working.</li> <li>Understand the layout of the distribution.</li> </ol> <p>After completing these steps, you may want to explore the tutorials in the <code>/tutorials</code> folder or consult the documentation and grid map in the <code>/documentation</code> folder. A link to the documentation is available in the start menu for easier access.</p>"},{"location":"getting_started/#getting-dsm2","title":"Getting DSM2","text":"<p>DSM2 is distributed by the California Department of Water Resources Delta Modeling Section. You can find the model at the CNRA Open Data website.</p> <p>Currently, we distribute:</p> <ul> <li>Windows executables</li> <li>Tutorials</li> <li>Source code</li> <li>A data package with templates for common studies</li> </ul>"},{"location":"getting_started/#installing-dsm2","title":"Installing DSM2","text":"<p>DSM2 has been tested on Windows 10.</p> <ul> <li>DSM2 is distributed as a <code>.zip</code> file containing the model executables and input files.</li> <li>Do not unzip it to a location with spaces in the path.</li> <li>Recommended locations: <code>D:\\delta\\dsm2</code> or <code>C:\\delta\\dsm2</code>.</li> <li>Ensure the drive has sufficient space (several gigabytes) for easier in-place usage.</li> </ul>"},{"location":"getting_started/#recommended-third-party-tools","title":"Recommended Third-Party Tools","text":"<p>DSM2 comes with a numerical model and scripting capabilities. To enhance usability, we recommend the following tools:</p>"},{"location":"getting_started/#command-window","title":"Command Window","text":"<p>Follow the instructions here to add the \"Open command window here\" option to the Windows Explorer context menu. This requires administrative privileges and involves modifying the Windows registry. This tool is essential for efficiently running DSM2 models and Python scripts.</p>"},{"location":"getting_started/#text-editor","title":"Text Editor","text":"<ul> <li>Notepad++: A text editor that works well with DSM2 input data and integrates with Windows Explorer. Syntax highlighting support is available for DSM2.</li> </ul>"},{"location":"getting_started/#diff-tool","title":"Diff Tool","text":"<ul> <li>DiffMerge: A free tool for comparing text files.</li> <li>Beyond Compare: A commercial product that is intuitive and supports Word file comparisons.</li> </ul>"},{"location":"getting_started/#dss-tools","title":"DSS Tools","text":"<ul> <li>Vista: A graphical tool for examining data in HEC-DSS format, included in the <code>/dsm2/vista/bin</code> directory.</li> <li>HEC-DSSVUE: Distributed by HEC and actively maintained. Most users prefer DSSVUE as their primary tool, with Vista used for specific tasks. An Excel add-in for DSS data is also available on the HEC website.</li> </ul>"},{"location":"getting_started/#hdf5-tools","title":"HDF5 Tools","text":"<ul> <li>HDFView and HDF-Explorer: Independent browsers for the HDF5 file format. These tools allow you to inspect tidefiles, one of the model's outputs. Only one of these tools is needed.</li> </ul>"},{"location":"getting_started/#test-launching-dsm2","title":"Test Launching DSM2","text":"<p>To verify the installation, open a command prompt and type the following command from any location:</p> <pre><code>C:\\&gt;hydro -v\nDSM2-Hydro 8.2.2  Git Version: 1468 Git GUI: 54a9cc3c\nUsage: Hydro input-file\n</code></pre> <p>If you see a message like the one above, the installation was successful!</p> <p>If instead you see:</p> <pre><code>C:\\&gt;hydro -v\n'hydro' is not recognized as an internal or external command,\noperable program or batch file.\n</code></pre> <p>...you have a path issue that needs to be resolved.</p>"},{"location":"getting_started/#next-steps","title":"Next Steps","text":"<p>Your next step should be to explore the Tutorials.</p> <ul> <li>The Basic Tutorials (Tutorials 1-6) cover most of the model's nuances using a simple grid. They are an excellent way to learn about the model, including subtle features that may be new or confusing.</li> <li>The Delta Tutorial series focuses on applied tasks in the Delta. Starting with Delta Tutorial #1 as a motivator and then tackling the basic tutorials is a quick way to get familiar with the model.</li> </ul>"},{"location":"releasenotes/","title":"Release Notes","text":"<p>Release Notes are on github</p>"},{"location":"build/CMake_Build/","title":"CMake Build","text":""},{"location":"build/CMake_Build/#introduction","title":"Introduction","text":"<p>DSM2 consists of many projects and third party libraries. This has meant running a number of scripts as documented in\u00a0CMake Build. It has also entailed opening multiple solution files and build libraries by hand. Furthermore, the compiler and visual studio upgrades were slow to adopt due to the cost of breaking changes when upgrading versions. And finally and most importantly, the daunting task of building all these files for a different OS such as linux.</p>"},{"location":"build/CMake_Build/#cmake","title":"CMake","text":"<p>cmake is a system that generates the build system. In other words cmake does not help in building the libraries and projects but encompasses the high-level, OS and build system independent instructions for generating those systems. Its introduction and tutorial could be found at https://cmake.org/</p> <p>A first effort at a cmake generated build system is working for VS2015 with the latest intel compiler on Windows. The instructions for this and the files needed have been checked into github master</p> <ul> <li>A\u00a0CmakeLibraryMacro.txt is placed at DSM2 root path, with global     macro and environment settings</li> <li>A CmakeLists is placed under each project/sub-subject to govern its     compilation.</li> <li>build*.bat is created for DSM2, input_storage, oprule,     respectively, to contain the key cmake commands (listed in the     following sections).</li> <li>After compilation, the exe/dll are generated under subfolders     BUILD\\release or\u00a0BUILD\\debug.</li> </ul>"},{"location":"build/CMake_Build/#dsm2-core-project","title":"DSM2 core project","text":"<pre><code>CMake Instructions\nCreate a build directory BUILD under dsm2\n\n\nmkdir BUILD\ncd BUILD\n\nFirst setup path \n\"C:\\Program Files (x86)\\IntelSWTools\\compilers_and_libraries\\windows\\bin\\compilevars.bat\" ia32 vs2015\n\nNext execute for VS2015 the cmake command\ncmake -G \"Visual Studio 14 2015\" ..\\src\n\nFinally open the DSM2.sln file in VS 2008 and compile\n\nor compile from command line with this command\ncmake --build . --target ALL_BUILD --config Debug\ncmake --build . --target ALL_BUILD --config Release\n</code></pre>"},{"location":"build/CMake_Build/#input-storage-and-oprule","title":"Input Storage and Oprule","text":"<p>The libraries input_storage and oprule are built to support DSM2 core project. (confirm their building success before compile core project)</p> <pre><code>cd input_storage\nmkdir BUILD\ncd BUILD\ncmake -G \"Visual Studio 14 2015\" ..\\\ncmake --build . --target ALL_BUILD --config Debug\ncmake --build . --target ALL_BUILD --config Release\n</code></pre> <pre><code>cd oprule\nmkdir BUILD\ncd BUILD\ncmake -G \"Visual Studio 14 2015\" ..\\\ncmake --build . --target ALL_BUILD --config Debug\ncmake --build . --target ALL_BUILD --config Release\n</code></pre>"},{"location":"build/CMake_Build/#third-party","title":"Third Party","text":"<p>DSM2 relies on third parties and requires the required libraries below.\u00a0Usually DSM2 just uses the built-up libraries; however, sometimes (when version/environment changes), the following libraries need re-build. Note DSM2 only requires some specific subsets in these libraries and these specification could be found in CmakeLibraryMacro.txt</p> <p></p>"},{"location":"build/CMake_Build/#boost","title":"boost","text":"<p>Run bootstrap.bat to build b2.exe; then run b2.exe</p> <p>Note b2.exe is required to run in complete mode (the default minimal mode won't build the required library; run b2 --help for details)</p> <p>Linux sample for boost build</p> <pre><code>#use this script to get started with new boost library\n#cp this script into new boost library source directory\n./bootstrap.sh --with-toolset=intel-linux\n./b2 --clean\n./b2 -a toolset=intel-linux link=static variant=release --with-filesystem --with-regex --with-system --with-test\n</code></pre> <p>Windos sample for boost build</p> <pre><code>./b2 -a runtime-link=static --with-filesystem --with-regex --with-system --with-test\n</code></pre>"},{"location":"build/CMake_Build/#hdf5","title":"HDF5","text":"<p>Go to CMake-hdf5-1.8.20 folder to run batch of the relevant version, e.g. build-VS2015-32.bat, which builds HDF5-1.8.20-win32.zip</p> <p>Unzip it and place it under third_party folder.</p> <p>To build the static libraries which is what is needed with DSM2 static build see this\u00a0HDF5 CMake Static Build</p>"},{"location":"build/CMake_Build/#heclib","title":"heclib","text":"<p>Build heclib\\windows_sources\\windows_build_MT_default_settings.bat</p> <ul> <li>MT is for static version as we needed (MD for dynamic)</li> <li>Make sure setting compiler path as the required version (as     exemplified in core project)</li> </ul> <p>For using Visual Studio 2017</p> <p>Install Visual Studio 2017:</p> <p>In addition to the standard Visual Studio 2017 installation, download individual components from the VS2017 installer.  The individual components needed are:</p> <p></p> <p>Run the installer, and click on Modify, then Individual components tab, and check the components, finally click on Modify.</p> <p>Change the lines in\u00a0build_dsm2_vs2015_32b.bat:</p> <p>call \"C:\\Program Files (x86)\\IntelSWTools\\compilers_and_libraries\\windows\\bin\\compilevars.bat\" ia32 vs2015</p> <p>to</p> <p>call \"C:\\Program Files (x86)\\IntelSWTools\\compilers_and_libraries_2019\\windows\\bin\\compilervars.bat\" ia32 vs2017</p> <p>cmake -G \"Visual Studio 14 2015\" ..\\src</p> <p>to\u00a0</p> <p>cmake -G \"Visual Studio 15 2017\" ..\\src</p> <p>Rerun the build scripts as instructed above.</p>"},{"location":"build/CMake_Build/#attachments","title":"Attachments:","text":"<ul> <li>VS2017_individaul_components.jpg (image/jpeg)</li> <li>sum.PNG (image/png)</li> </ul>"},{"location":"build/Cloud_Notes/","title":"Cloud Notes","text":"<p>This page documents important notes for running DSM2 in the cloud.</p> <p>DSM2 has been compiled on Ubuntu Linux and Windows. Only 32 bit version has been compiled, the path to 64 bit conversion is much longer due to C/C++ code that needs cleaning up.</p>"},{"location":"build/Cloud_Notes/#linux","title":"Linux","text":"<p>DSM2 has been compiled and tested on Linux VMs running Red Hat 4.8 with kernel version 3.10. Static linking does not seem to work as documented and the dependencies on Intel fortran libraries are packaged in a lib/ subfolder.</p> <p>AWS Linux version is Red Hat 7.3 running kernel 4.14.\u00a0 The following libraries are needed on top of the base image from AWS</p> <pre><code>#sudo yum upgrade #-- Do this to ensure installs of the below go through\nsudo yum install glibc.i686\nsudo yum install libgcc.i686\nsudo yum install libstdc++.i686\n</code></pre>"},{"location":"build/Cloud_Notes/#windows","title":"Windows","text":"<p>Windows version is statically compiled and so the executables should work without any other dependencies.\u00a0</p> <p>TODO: JRE version and installation\u00a0</p>"},{"location":"build/DSM2_v82_Compiling_and_Packaging/","title":"DSM2 v82 Compiling and Packaging","text":"<p>This memo presents step-by-step instruction on compiling and packaging DSM2 version 8 assuming a minor (bug-fix) release.\u00a0</p>"},{"location":"build/DSM2_v82_Compiling_and_Packaging/#version","title":"Version","text":"<p>The latest DSM2 release version is 8.2.0.\u00a0The first two digits of DSM2 official version; the 3<sup>rd</sup> digit is used for minor (bug-fix) releases.</p> <p>git commit number is also embedded for developer's record, and could be used to check thru '-version' command.</p> <p></p> <p>The Git number is embedded automatically at compilation time. The following 3 files contains relevant version info: \u00a0\\dsm2\\src\\common\\version_generate.py \u00a0\\dsm2_distribute\\dsm2\\install\\copy_hydro_qual_ptmDLL_notepad_style.bat \u00a0\\dsm2_distribute\\dsm2\\install\\DSM2setup_Script_Generate.py</p> <p>*BDO is using an 'unstable trunk' branching strategy in GIT for DSM2 development. *GIT categories: master is for major development and bug fix; branches are usually for separate and time-consuming studies on old versions; Tags are the records for all release versions. *DSM2 version tag (the 3<sup>rd</sup> digit in the version number) is designed to use odd number for internal use, and even number for formal release\u00a0</p>"},{"location":"build/DSM2_v82_Compiling_and_Packaging/#checkout","title":"Checkout","text":"<p>Checkout DSM2 from github: https://github.com/CADWRDeltaModeling/dsm2.git</p> <p></p> <p>place the project in local directory, e.g. D:\\dev\\dsm2_master\\</p> <p></p> <p>Check out could be conduct at command line</p> <pre><code>git clone https://github.com/CADWRDeltaModeling/dsm2.git\n</code></pre> <p>Or use software sourceTree (recommended for later use). Note which branch you're checking out (master by default).</p> <p></p>"},{"location":"build/DSM2_v82_Compiling_and_Packaging/#compile","title":"Compile","text":"<p>Cmake is currently used to control and adapt project compilation (see details\u00a0at CMake Build).</p> <p>From command line, run cmake batch at the project root path. It calls cmake build of oprule, input_storage, then DSM2 sequentially.</p> <pre><code>build_all.bat\n</code></pre> <p>The building results are exe of\u00a0hydro and qual, with dll of ptm, under path \\BUILD\\Release or Debug</p> <p></p> <p>The major part of PTM module is written in Java programming language and placed in the source code folder: \\dsm2_v8_1\\src\\ptm. It could be build\u00a0in two ways:</p> <ul> <li>ANT: the default compiling procedure is set in build.xml; open the     command window and run 'ant' (build.xml as default running file) at     current path. The release version of compiled application file will     be automatically placed in the folder:\u00a0\\dsm2_v8_0\\src\\ptm\\lib     (ptm.jar)</li> <li>Eclipse: DSM2 v82 PTM Compiling with     Eclipse</li> </ul>"},{"location":"build/DSM2_v82_Compiling_and_Packaging/#libraries","title":"Libraries","text":"<p>All the libraries DSM2 needs are precompiled and placed in the folder lib: input storage, oprule, and third parties</p> <p>If compiling is required, refer their compile details\u00a0at CMake Build.  </p> <p></p> <p>Since third_party folder is very big and not easy to copy around, one way to share the same package (w/o increase disk occupance) is to use Windows mklink</p> <p>Use admin right to open a command window and type in:</p> <p>mklink /D {target location}\\third_party {original location}\\third_party</p> <p>For internal users use this command</p> <p>mklink /D third_party\u00a0\\cnrastore-bdo\\Delta_Mod\\Share\\DSM2\\compile_support\\third_party</p>"},{"location":"build/DSM2_v82_Compiling_and_Packaging/#debug","title":"Debug","text":"<p>Cmake also built up the project Visual Studio\u00a0solution, which could be used do the code debug.</p> <p>See the following example for debug setting in Visual Studio 2015.</p> <p></p> <p></p> <p>Make sure the 'debug info' is on the project you're working on (the current VS set it as off by default).</p> <p></p>"},{"location":"build/DSM2_v82_Compiling_and_Packaging/#packaging","title":"Packaging","text":"<p>Download from share folder and change folder name to dsm2_distribute\\</p> <p>under branch (e.g. master), aside with dsm2</p> <p></p> <p>1. Generate tutorial PDF files:</p> <p>a) Delete all PDF files in \"dsm2_distribute\\dsm2\\tutorials\\pdf\"</p> <p>b) Run \"doc2pdf.vbs\" in \"\\dsm2_distribute\\dsm2\\install\\ to generate PDFs from tutorial word documents.</p> <p>2. Copy compiled binaries to distribution folder:</p> <p>a) Check the DSM2 version and paths to the compiled binaries are correct in the batch file \"copy_hydro_qual_ptmDLL_notepad_style.bat\" in the folder \\dsm2_distribute\\dsm2\\install\\</p> <p>Be aware of the version consistence for the 3 control files mentioned in 'DSM2 Versioning'; if not, correct them and re-compile from 6-4 in the previous section</p> <p>b) Run the batch file \"copy_hydro_qual_ptmDLL_notepad_style.bat\".</p> <p>  Manually copy hydro.exe, qual.exe, ptm.dll, ptm.jar to the folder \u00a0\\dsm2_distribute\\dsm2\\bin</p> <p>3. Generate packaging script:</p> <p>a) Check the DSM2 version is correct in the Python script \"DSM2setup_Script_Generate.py\" in the folder \\dsm2_distribute\\dsm2\\install\\</p> <p>Be aware of the version consistence for the 3 control files mentioned in 'DSM2 Versioning'; if not, correct them and re-compile from 6-4 in the previous section</p> <p>b) Run this Python script to generate Inno Setup script \"DSM2setup_v8.iss\".</p> <p>4. Create DSM2 installation file:</p> <p>Run \"DSM2setup_v8.iss\" with Inno Setup Compiler v5.2.3 The installation file named \"DSM2setup_8.X.Xrelease.XXXX.exe\" will be created in the same folder, \\dsm2_distribute\\dsm2\\install\\ </p> <p>5. Quick-test installer:</p> <p>Test installation on a clean machine. Run historical hydro, qual_ec and ptm on study templates.</p> <p>6. Tag and version increment:</p> <p>Create release tag for both \"dsm2\" source code on github and \"dsm2_distribute\" folders on share-folder.\u00a0</p> <p></p> <p>For future usage,\u00a0immediately increment DSM2 version number (3rd digit to the next odd number) in the following three files:</p> <ul> <li>\\dsm2\\src\\common\\version_generate.py \u00a0</li> <li>\\dsm2_distribute\\dsm2\\install\\copy_hydro_qual_ptmDLL_notepad_style.bat     \u00a0</li> <li>\\dsm2_distribute\\dsm2\\install\\DSM2setup_Script_Generate.py</li> </ul>"},{"location":"build/DSM2_v82_Compiling_and_Packaging/#attachments","title":"Attachments:","text":"<p> debug_on.PNG (image/png) package1.PNG (image/png) debug1.png (image/png) debug.png (image/png) vers.PNG (image/png) lib.PNG (image/png) local1.PNG (image/png) github.PNG (image/png) srctree0.PNG (image/png) local0.PNG (image/png) image2017-6-13_15-47-59.png (image/png) image2017-6-13_15-47-43.png (image/png) image2017-6-13_15-47-35.png (image/png) image2017-6-13_15-47-17.png (image/png) image2017-6-13_15-46-58.png (image/png) worddav07c1c902559a15d9cb8d941d966322cb.png (image/png) worddavb0ec1d6cc7478dc4ec73bc27abb42880.png (image/png) worddav0037c3b8067a8dd0d52094029690277b.png (image/png) worddav490ef33751ab42acaa896e9bb7dc2dc7.png (image/png) worddav73b529f2e3382f4bb77f505185a10945.png (image/png) worddavc7e39738a4a8caa213b31d248d81f87b.png (image/png) worddavb58034debe15b3d2514f722580c782ad.png (image/png) worddav1c1324ef8177e0822bb62d9cdf8fdb05.png (image/png) worddava9503121f9ac50fd0060de1b95c6decc.png (image/png)</p>"},{"location":"build/DSM2_v82_PTM_Compiling_with_Eclipse/","title":"DSM2 v82 PTM Compiling with Eclipse","text":"<p>This memo presents step-by-step instruction on establishing editable project, compiling and packaging DSM2 (version 8) PTM module in Eclipse. Note this is the version for PTM source code editing purpose. The version for straight compiling is in the DSM2 compiling memo: DSM2_v8_0_Compiling_and_Packaging.  </p> <p>Required Software and Source Code Check out</p> <ol> <li>Java Development Kit     http://java.sun.com/javase/downloads/index.jsp</li> <li>Eclipse-jee-galileo-win32     +http://www.eclipse.org/downloads/packages/release/galileo/sr2+</li> <li>PTM Java source code are checked out in the DSM2 Compile &amp; Package     document, with the whole DSM2 coding package:</li> </ol> <p>The specific folder directory on DWR SVN server is: +http://dminfo.water.ca.gov:8686/svn/repository/models/branches/dsm2_v8_0/src/ptm/DWR/DMS/PTM+ and could be put at following directory on local computers: +D:\\delta\\models\\dsm2_v8_0\\src\\ptm+  </p> <p>Build up project Method 1: with Eclipse project description file File -&gt; Import -&gt; Existing Projects into Workspace -&gt; Next -&gt; Select root directory (Browse) to where .project resides -&gt; Ok -&gt; Finish </p> <p>Method 2: w/o Eclipse project description file</p> <ol> <li>Set up workspace</li> </ol> <p>Double click to open Eclipse and set up the workspace (default path as d:\\workspace); *This is the path for source code, libraries, compiled bytecode files, and et cetera. Source codes are copied from the location specified in the previous step. *Changes made in workspace could be compiled and tested independently, then copied back to the DSM2 model package folders, and finally committed to the SVN server. </p> <ol> <li>Create the PTM project in workspace</li> </ol> <p>File-&gt;New-&gt;Java Project  'Project name' input e.g. ptm 'Project layout' select as 'Use project folder as root for sources and class files' (with src and bytecode stored at one place; the other option is also ok for use)  Next-&gt;Libraries-&gt;Add External JARs-&gt;Select 3 jar files (COM.jar, edu.jar, xml.jar) at directory: D:\\delta\\models\\dsm2_v8_0\\src\\ptm\\lib Finish </p> <ol> <li>Create Package to include source codes:</li> </ol> <p>Right click ptm in the Package Explorer-&gt;New-&gt;Package  Input Package Name under Source folder ptm: DWR.DMS.PTM  Import java source codes  General-&gt;File System-&gt;Next From directory-&gt;Browse (DSM2 PTM Java src folders) -&gt;OK  Check PTM box on the left window (to include all the java files)-&gt;Finish  *Eclipse would automatically create 2 packages for the sub-folders: DWR.DMS.PTM.behave DWR.DMS.PTM.tools *If some error msg like 'access restriction' comes out: Try to remove the JRE System library from Project ptm-&gt;Properties-&gt;Java Build Path-&gt;Libraries, and add it again Compile and debug PTM</p> <ol> <li>Compile source code to bytecode</li> </ol> <p>Project-&gt;Build Project  Bytecodes (.class) are stored at the same place as source codes (.java) Automatic compilation could be enabled by Project -&gt; Build Automatically</p> <ol> <li>Run/Debug Variables Configuration</li> </ol> <p>Set up the PTM input study file in IDE configuration Menu Run/Debug-&gt; Run/Debug Configuration  Java Application-&gt;New  e.g. Main Tab: Name: MainPTM Main Class: DWR.DMS.PTM.MainPTM Argument Tab: Program arguments: historical_ptm.inp (the sample PTM input file in a DSM2 v8 historical study; make sure the hydro file has been run first and h5 file exists) VM arguments: -ss1m -mx512m -oss1m Other: D:\\delta\\dsm2_v8\\study_templates\\historical Environment Tab: DSM2_HOME d:\\delta\\dsm2_v8 Path d:\\delta\\dsm2_v8\\bin </p> <ol> <li>Run/Debug PTM</li> </ol> <p>Switch to Debug Mode, which enable breakpoint setting and variable tracking Run-&gt;Run/Debug MainPTM for calculation checking </p> <p>Export back to DSM2 package</p> <ol> <li>Combine the PTM Java bytecotes and related libraries into a JAR file</li> </ol> <p>File-&gt;Export-&gt;Java-&gt;JAR file  Check ptm box on the left to select all the bytecode files and libraries Select the export destination: D:\\workspace\\ptm\\ptm.jar </p> <ol> <li>Copy the JAR file to the DSM2 distribution binary folder for     packaging:</li> </ol> <p>+D:\\delta\\models\\dsm2_distribute\\dsm2\\ptm\\lib+ Other PTM related files, e.g. ptm.dll, are compiled in related C++ and Fortran projects of DSM2 Visual Studio solution. Please see the document for details.</p>"},{"location":"build/DSM2_v82_PTM_Compiling_with_Eclipse/#attachments","title":"Attachments:","text":"<p> worddav44179cfae725fe5f39c7b918214b4d68.png (image/png) worddav21d73a7e0e593bfd3b0107605d9a879c.png (image/png) worddave3944aea30dfda849398f8b2d35e5ca0.png (image/png) worddav166bb58986798c0c19da41ec64557bc5.png (image/png) worddav0e06297fcdac03d480d52d4303a8f664.png (image/png) worddav974c8dfc9bac21524c2e729c9ddf7938.png (image/png) worddav4acaef0242e743933e42a19aac4be497.png (image/png) worddava8b154265c7f02598531839e7c2691d2.png (image/png) worddav92a273d21de65a713f3c72c11cdd9d2f.png (image/png) worddavca883dcc4d073032cba8861c22f82023.png (image/png) worddav6787134635058c6f17e54d0d92dfbceb.png (image/png) worddava5ae1f4d3a3483dc4320496d098a0bc6.png (image/png) worddav7728359a07b9c4738d8f2fc9e1dcbe00.png (image/png) worddavba79a0a41e348cbfbb98c6c379e69d73.png (image/png) worddavaa8687d7165c7477fa2fe14970938e00.png (image/png) worddav0497cdcda22d3ff9a889885b64ed225a.png (image/png) worddav911bf0d4648403b4cd5c945075026a73.png (image/png) worddav7a2f929ee2f765ed2f7e2f8c617f4e9a.png (image/png) worddavd39e409d006ca7cd1fececf0ba72f5d5.png (image/png) worddaveec0e1cc149a8a2c30bf9d635c413987.png (image/png) worddav769c68dd95f5a8d049cd75682818d784.png (image/png)</p>"},{"location":"build/Debugging_JNI_code_with_Eclipse_and_Visual_Studio_20xx_/","title":"Debugging JNI code with Eclipse and Visual Studio (20xx)","text":"<p>This document shows how to setup an Eclipse project (e.g. PTM) with JNI (native C/C++/Fortran code) with Visual Studio (e.g. 2015)</p> <ol> <li> <p>Use 32 bit version of Eclipse (e.g.\u00a0eclipse-java-neon-2-win32) and     setup PTM project</p> <ol> <li> <p></p> <p>Browse over to the checked out version of dsm2 and look under dsm2/src/ptm.     2. Create a debug configuration. Make sure to point to the directory where PTM.dll is built in debug mode. E.g. d:\\dev\\dsm2\\master\\dsm2\\BUILD\\Debug\\ is where cmake builds the Debug version of the projects </p> </li> </ol> </li> <li> <p>Start debug from Eclipse and make sure to pause on some line of code     before JNI code is invoked.</p> </li> <li> <p>Use Visual Studio code and make sure to be in Debug configuration.     Then attach to the running Java process in 2 using the remote attach     to process      </p> <p>You will need to be able to identify the process in 2 by its PID or its name.</p> </li> <li> <p>Set breakpoint in native code     </p> </li> <li> <p>Release the paused Java code in the Eclipse debugger. When the     native code trigger is hit it will stop at the above breakpoint.</p> </li> </ol>"},{"location":"build/Debugging_JNI_code_with_Eclipse_and_Visual_Studio_20xx_/#attachments","title":"Attachments","text":""},{"location":"build/Developer_and_Build/","title":"Developer and Build","text":""},{"location":"build/Developer_and_Build/#basic-steps","title":"Basic steps","text":"<ul> <li> <p>Install prerequisite softwares</p> </li> <li> <p>Checkout\u00a0DSM2 from github repository</p> </li> <li> <p>Compile and build input_storage and oprule libraries (may also need     third party)</p> </li> <li> <p>Compile and build DSM2 (That should result in the hydro.exe and     qual.exe and ptm.dll in the build folders)</p> </li> <li> <p>Test and validate the newly compiled</p> </li> <li> <p>Copy and update DSM2 distribution package</p> </li> <li> <p>Package for DSM2 new release</p> </li> </ul> <p>DSM2 version 82* compilation and packaging assuming a minor (bug-fix) release.</p> <ul> <li>DSM2 source codes and its relevant support party are placed on open     source platform github     (https://github.com/CADWRDeltaModeling/dsm2) for     version control,</li> </ul>"},{"location":"build/Developer_and_Build/#required-software","title":"Required Software","text":"<ol> <li>Visual Studio 2015 (check its installation and management details     at\u00a0Intel Compiler Installation for     Windows)</li> <li>Intel Composer for Fortran and C++ (Parallel Studio 2019)</li> <li>Cmake     3.14\u00a0https://cmake.org/\u00a0(better     use a latest stable version, not *rc)</li> <li>Git https://git-scm.com/downloads</li> <li>Source Tree (free git client, optional)     https://www.sourcetreeapp.com/</li> <li>Flex and Bison packages in Cygwin     http://www.cygwin.com/setup.exe\u00a0 (make     sure\u00a0{cywin}/bin in the environment path)</li> <li>Inno Setup Compiler v5.2.3     http://files.jrsoftware.org/is/5/isetup-5.2.3.exe</li> <li>Python 3 http://www.python.org/download/</li> <li>Java Development Kit\u00a0(32-bit)     http://java.sun.com/javase/downloads/index.jsp</li> <li>Microsoft Office 2010</li> <li>Apache ANT http://ant.apache.org/bindownload.cgi</li> </ol> <p>Note: make sure all software have their binary, header, or library folder set in the environment variables. (given that Department Virtual Machines may prohibit editing, users can still edit in their own account. Be aware of the software version which could fail some step of compiling)</p> <p></p>"},{"location":"build/Developer_and_Build/#validation","title":"Validation","text":"<p>To test new compiled DSM2 and see its difference from older version, the following tools are often used:</p> <ul> <li>DSM2-vista\u00a0Compare DSS Files     Tool\u00a0requires     output settings (pathnames) exactly the same, but provides a quick     summary of accumulated difference, especially useful when we want to     confirm if two versions are the same.</li> <li>DSM2-vista\u00a0Compare DSS     Tool is flexible     to compare between different pathnames, yet requires users set up     configuration one-by-one.</li> <li>HEC-DSSVue has a compare function in its 'Tools' menu</li> </ul>"},{"location":"build/Developer_and_Build/#attachments","title":"Attachments","text":"<p> image2019-11-15_10-58-45.png (image/png) image2019-11-15_10-54-29.png (image/png) image2019-11-15_10-53-40.png (image/png) DSM2_v8_0_PTM_Compiling_eclipse.docx (application/vnd.openxmlformats-officedocument.wordprocessingml.document) DSM2_v8_0_Compiling_and_Packaging.docx (application/vnd.openxmlformats-officedocument.wordprocessingml.document)  </p>"},{"location":"build/Docker_Builds/","title":"Docker Builds","text":"<p>Docker is a great way to packaging software for linux and is essential for working in the cloud platforms (AWS and Azure).</p> <p>DSM2 docker build repo is here\u00a0http://dwrrhapp0179.ad.water.ca.gov/gitea/nsandhu/dsm2-docker</p>"},{"location":"build/HDF5_CMake_Static_Build/","title":"HDF5 CMake Static Build","text":"<p>DSM2 binaries are built with static links (no DLLs are needed). However HDF5 1.8.10+ does not support static builds as there are fundamental problems if parallel support is enabled. However DSM2 does not need the parallel support and static builds are very convenient for us.</p> <p>The information here was documented in\u00a0 DSM2-117 - Update HDF5 library to 1.8.19 or later Done \u00a0and the kernel of information is included here for future HDF5 builds for static linking.</p>"},{"location":"build/HDF5_CMake_Static_Build/#this-blog-entry-explains-how-to-build-with-mt-flag","title":"This blog entry explains how to build with /MT flag","text":"<p>https://blog.afach.de/?page_id=421</p>"},{"location":"build/HDF5_CMake_Static_Build/#hdf5-static-with-mt-flag-compilation-auto-compile-script-visual-studio","title":"HDF5 Static (with /MT flag) compilation Auto compile script \u2013 Visual Studio","text":"<p>This is a compile script that compiles HDF5 libraries from source statically with multithread support, i.e., \u201c/MT\u201d flag in Visual Studio. automatically.</p>"},{"location":"build/HDF5_CMake_Static_Build/#warning","title":"Warning","text":"<p>After discussing with one of the programmers\u00a0of HDF5, it was made clear that\u00a0linking statically works safely only in the condition HDF5 library\u00a0wasn\u2019t compiled with parallel support.</p>"},{"location":"build/HDF5_CMake_Static_Build/#the-script","title":"The script","text":"<p>The script involves going to the file config\\cmake\\UserMacros\\Windows_MT.cmake and copying the file\u2019s contents to \u201cUserMacros.cmake\u201d. The same is also done for ZLib and SZip after extracting them, and rezipping them again.</p> <pre><code>@echo off\n::The following is the name of the folder of HDF5 source\nset \"hdffolder=hdf5-1.8.16\"\n\n::add a new line then add /MT compilation options\ncall echo &amp; echo. &gt;&gt; %hdffolder%\\UserMacros.cmake\ncat %hdffolder%\\config\\cmake\\UserMacros\\Windows_MT.cmake &gt;&gt; %hdffolder%\\UserMacros.cmake\nfor %%i in (%hdffolder%\\UserMacros.cmake) do sed -i \"s/\\\"Build With Static CRT Libraries\\\" OFF/\\\"Build With Static CRT Libraries\\\" ON/g\" %%i\n\n::add a new line then add /MT to SZip after extracting it, and then recompress it\ngzip -dc SZip.tar.gz | tar -xf -\nmv SZip.tar.gz SZip-dynamic.tar.gz\ncall echo &amp; echo. &gt;&gt; UserMacros.cmake\ncat SZip\\config\\cmake\\UserMacros\\Windows_MT.cmake &gt;&gt;SZip\\UserMacros.cmake\nfor %%i in (SZip\\UserMacros.cmake) do sed -i \"s/\\\"Build With Static CRT Libraries\\\" OFF/\\\"Build With Static CRT Libraries\\\" ON/g\" %%i\ntar cf SZip.tar SZip\\\ngzip SZip.tar\nrm -r SZip\n\n::do the same to ZLib\ngzip -dc ZLib.tar.gz | tar -xf -\nmv ZLib.tar.gz ZLib-dynamic.tar.gz\ncall echo &amp; echo. &gt;&gt; UserMacros.cmake\ncat ZLib\\config\\cmake\\UserMacros\\Windows_MT.cmake &gt;&gt;ZLib\\UserMacros.cmake\nfor %%i in (ZLib\\UserMacros.cmake) do sed -i \"s/\\\"Build With Static CRT Libraries\\\" OFF/\\\"Build With Static CRT Libraries\\\" ON/g\" %%i\ntar cf ZLib.tar ZLib\\\ngzip ZLib.tar\nrm -r ZLib\n\nbuild-VS2013-32.bat\n</code></pre>"},{"location":"build/HDF5_CMake_Static_Build/#requirements","title":"Requirements","text":"<p>1-\u00a0CMake\u00a0(add its executable folder to path) 2-\u00a0[GOW</p> <p>|                                                                                                                                                               | |---------------------------------------------------------------------------------------------------------------------------------------------------------------| |  rel=\"nofollow\"&gt;https://github.com/bmatzelle/gow/downloads]3-  href=\"https://www.visualstudio.com/en-us/products/visual-studio-express-vs.aspx\"                                                                              rel=\"nofollow\"&gt;Visual Studio or C++ Express\u00a0(this you can get for free from Microsoft, but I assume you know enough about this already since you\u2019re here)  |</p> <p>Note: If CMake won\u2019t show in path in command prompt, run prompt as administrator, or use this command to add the path you want to the environment variable %PATH% \u00a0set PATH=C:\\Program Files (x86)\\CMake\\bin;%PATH% </p> <p>Gow is GNU tools for windows, like tar, gzip and sed. These are important for the script.</p> <p>Whether you\u2019d like to have a 32-bit or 64-bit version of visual studio used depends on the environment variables that are defined.\u00a0The easiest way is to run the run command prompt for the version you want. For example, in Visual Studio 2013, if one goes to Start, then types in quick search \u201cVisual\u201d, you\u2019ll find a folder called \u201cVisual Studio Tools\u201d. This folder will have both command prompts with the relevant environment variables. The following shows this folder:</p> <p></p>"},{"location":"build/HDF5_CMake_Static_Build/#prepare-to-run-the-script","title":"Prepare to run the script","text":"<p>Go to\u00a0this page, and download the CMake source. Extract it; put the script in a file there; if the version you want to compile is different than the one in the script, modify the folder name; and finally run the script. After the script is finished, you\u2019ll have a compressed zip file with compiled source and an installer executable.</p> <p>The file HDF5CompileScript.bat is where I copied the script of compile that I created. Just run this script through the command prompt of visual studio and it\u2019ll compile.</p>"},{"location":"build/PTM_DLL_Static_Build/","title":"PTM DLL Static Build","text":"<p>On windows it is not straightforward to compile to a DLL by mixing /MT (static) and /MD (dynamic) linking flags. However we want to create exes (hydro, qual, gtm) that have no dependencies on system libraries at runtime. This means we compile all libraries with /MT flags.</p> <p>DSM2-174 - PTM DLL compiling issue Resolved</p> <p>PTM is a dll so we have to override the libraries the compiler and linker search for by default.</p> <p>For the standard C/C++ libraries more information at\u00a0https://docs.microsoft.com/en-us/cpp/c-runtime-library/crt-library-features?view=vs-2019</p> <p>For the Intel libraries more information at\u00a0https://software.intel.com/en-us/articles/libraries-provided-by-intelr-c-compiler-for-windows-and-intel-parallel-composer</p> <p>The following libraries are then ignored so that the static versions of the libraries are packaged into the .dll file itself. This increases the size of the DLL but then during runtime there are no other dependencies on any system or intel libraries. In other words, it can stand alone and run.</p> <p>The libraries ignored are\u00a0</p> <pre><code>msvcrt.lib;libmmd.lib; mscvcprt.lib;libucrtd.lib;\n</code></pre> <p>If you ever need to see all the libraries being used turn the /VERBOSE feature on the linker options in Visual Studio</p> <p></p>"},{"location":"build/PTM_DLL_Static_Build/#attachments","title":"Attachments:","text":""},{"location":"build/Troubleshooting/","title":"Troubleshooting","text":"<ul> <li>DSM2 inputs are off by 1DAY</li> <li>DSM2 Seems to indicate missing irregular (gate position) data</li> </ul>"},{"location":"build/Troubleshooting/#attachments","title":"Attachments:","text":"<p> image2020-1-15_8-29-2.png (image/png) image2020-1-15_8-21-6.png (image/png) image2020-1-15_8-20-52.png (image/png) image2020-1-15_8-19-39.png (image/png) image2020-1-15_8-19-11.png (image/png) image2020-1-15_8-18-51.png (image/png) image2020-1-15_8-17-37.png (image/png) image2020-1-15_8-17-16.png (image/png) image2020-1-15_8-16-44.png (image/png)</p>"},{"location":"build/Troubleshooting_Eclipse/","title":"Troubleshooting Eclipse","text":"<p>Eclipse Tips</p>"},{"location":"calibration/Calibration_Memo/","title":"Calibration Memo","text":"<p>Links needed here to the Calibration Memo</p>"},{"location":"calibration/Calibration_Memo/#attachments","title":"Attachments:","text":"<ul> <li>Calibration Refine Notes</li> <li>Memo DSM2 V8.1Beta Calibration</li> <li></li> <li>Hydro Calibration Notes</li> <li>Flow 2009</li> <li>Flow 2008</li> <li>Flow 2007</li> <li>Flow 2002</li> <li>EC Calibration Notes</li> <li></li> <li>EC.pdf</li> <li>Stage 2009</li> <li>Stage 2008</li> <li>Stage 2007</li> <li>Stage 2002</li> </ul>"},{"location":"calibration/Mini_Calibration_2009_/","title":"Mini Calibration (2009)","text":"<p>DSM2_Recalibration_102709_doc.pdf</p>"},{"location":"csdp/CSDP_Network_File_Format/","title":"CSDP Network File Format","text":"<p>The CSDP network file stores centerlines, cross-section lines, and cross-section points. The CSDP uses the information in the file to create cross-section input for DSM2.</p> <p>The format of the CSDP Network file predates the CSDP. It was created by the consultant John Crapuchettes, who developed the Bathymetry Data Display (BDD) application, the predecessor to the CSDP.</p>"},{"location":"csdp/CSDP_Network_File_Format/#file-format-details","title":"File Format Details","text":"<ul> <li>Horizontal Datum: UTMNAD83</li> <li>Horizontal Zone: 10</li> <li>Horizontal Units: Meters</li> <li>Vertical Datum: NAVD88</li> <li>Vertical Units: US Survey Feet</li> <li>File Type: Network</li> <li>Number of Elements: 525</li> </ul> <p>Example:</p> <pre><code>\"1\" 18\n2140064.75,1.3689134E7\n2139796.0,1.3689089E7\n2139320.0,1.3689424E7\n2139205.25,1.3689698E7\n2139663.75,1.3690571E7\n2140078.25,1.3690641E7\n2140395.75,1.369087E7\n2140713.0,1.3691585E7\n2140713.0,1.3691928E7\n2140589.75,1.3692493E7\n2139928.25,1.3693401E7\n2139275.75,1.3693904E7\n2139240.5,1.3694195E7\n2139390.5,1.3694503E7\n2139822.5,1.3695402E7\n2139954.75,1.3695905E7\n2140422.0,1.369669E7\n2140480.5,1.3696834E7\n3\n\"\" 8\n-215.90325927734375,20.427509307861328\n-157.9310302734375,12.60617733001709\n-54.482757568359375,6.409266471862793\n92.41378784179688,-3.2046332359313965\n191.72413635253906,1.6023166179656982\n315.862060546875,4.247311592102051\n346.89654541015625,11.737451553344727\n458.6206970214844,20.54054069519043\n117.23826599121094 890.1288452148438\n\"BT 8/12/2019: cloned from adjacent cross-section to prevent interpolation to improve max area ratio\"\n\"\" 8\n-355.90325927734375,20.427509307861328\n-297.9310302734375,12.60617733001709\n-194.48275756835938,6.409266471862793\n-47.58620834350586,-3.2046332359313965\n51.72413635253906,1.6023166179656982\n175.86207580566406,4.247311592102051\n206.89654541015625,11.737451553344727\n318.6206970214844,20.54054069519043\n662.5869750976562 903.2123413085938\n\"KH,1/30/2019: moved the centerline to better line up with the most recent survey data; re-created the cross-sections\"\n\"\" 8\n-385.9397277832031,32.054264068603516\n-220.38327026367188,14.89919376373291\n-121.08013916015625,6.431451797485352\n-27.00348472595215,1.6935484409332275\n54.0069694519043,-2.036290407180786\n150.69686889648438,-0.32258063554763794\n218.64111328125,19.435483932495117\n341.4634094238281,34.15322494506836\n9270.8427734375 1192.212646484375\n\"KH,1/30/2019: moved the centerline to better line up with the most recent survey data; re-created the cross-sections *nl* *nl* BT 7/24/2019: adjusted to prevent drying up\"\n\n\"2\" 11\n2140424.0,1.3696842E7\n2141014.75,1.3698247E7\n2141700.5,1.369945E7\n2143323.0,1.3700658E7\n2143420.0,1.3701319E7\n2142970.25,1.3701777E7\n2141859.25,1.3701848E7\n2140686.5,1.3701619E7\n2139679.75,1.3701361E7\n</code></pre>"},{"location":"csdp/CSDP_Network_File_Format/#notes","title":"Notes","text":"<ul> <li>Comments in the file provide explanations for various lines.</li> <li>Adjustments to the centerline and cross-sections are documented with timestamps and notes.</li> </ul> <p>For more information, refer to the CSDP Tutorial.</p>"},{"location":"csdp/CSDP_Network_Summary_Report/","title":"CSDP Network Summary Report","text":"<p>The CSDP Network Summary Report is created by the CSDP. It helps identify issues and potential problems with cross-sections in the currently loaded network file. It also contains comparisons of DSM2 Virtual Cross-Section volume with GIS-calculated volumes.</p>"},{"location":"csdp/CSDP_Network_Summary_Report/#input-files","title":"Input Files","text":"<ol> <li>Channels.inp file: Used to get existing channel lengths for comparison and determine channel connectivity.</li> <li>Network file: The currently loaded network file.</li> <li>DSM2 output (.hof) file: Created by running DSM2-Hydro with geometry from the network file.</li> <li>2m DEM CutFill validity file: Created based on visual inspection of channel polygon coverage in the 2m DEM files.</li> <li>CutFill results files: Contain results from CutFill operations for a given DEM.</li> <li>Optional channel groups list: Default is the list of groups for which polygons were created and used in CutFill operations.</li> </ol>"},{"location":"csdp/CSDP_Network_Summary_Report/#report-contents","title":"Report Contents","text":"<ol> <li>Channel: DSM2 channel name/number or group of channels.</li> <li>Length Comparison:</li> <li>Channels.inp length: Length specified in the DSM2 channels file.</li> <li>CSDP length: Length calculated by the CSDP.</li> <li>% Change: Difference between CSDP and Channels.inp lengths.</li> <li>CSDP Average Width: Used for GIS volume estimate validity.</li> <li>Volume Comparison:</li> <li>CSDP Volume: Channel volume calculated by CSDP.</li> <li>DSM2 Volume: Volume from DSM2 output.</li> </ol>"},{"location":"csdp/CSDP_Network_Summary_Report/#creating-the-report","title":"Creating the Report","text":"<ol> <li>Load a bathymetry file.</li> <li>Load or create a network file.</li> <li>Select <code>Network &gt; Reports &gt; Network Summary Report</code>.</li> <li>Fill in the dialog and save the results to a tab-delimited <code>.txt</code> file.</li> <li>Import the file into Excel for further analysis.</li> </ol>"},{"location":"csdp/CSDP_Network_Summary_Report/#attachments","title":"Attachments","text":"<ul> <li>image2019-3-26_14-13-18.png</li> <li>image2019-3-25_16-9-52.png</li> <li>image2019-3-25_16-9-41.png</li> <li>networkSummary.txt</li> <li>image2019-3-25_16-8-30.png</li> <li>networkSummary20190308.xlsx</li> <li>image2019-1-7_14-32-27.png</li> <li>networkSummaryWithoutHof.xltx</li> <li>networkSummaryWithHof.xltx</li> <li>networkSummaryWithoutHof.xlsx</li> <li>networkSummaryWithHof.xlsx</li> <li>networkSummaryWithoutHof.txt</li> </ul>"},{"location":"csdp/CSDP_Tutorial/","title":"CSDP Tutorial","text":"<p>Brad Tom (developer of CSDP) gave a presentation on CSDP in February 2009. A recording of this presentation is available as below.</p> <p>Download PDF</p>"},{"location":"csdp/CSDP_Tutorial/#updates","title":"Updates","text":"<p>In version 8.x the irregular xsection file format has changed. To change this information to the new format run the script under <code>vista/scripts/dsm2/csdp_geom_converter.py</code> with the location of the directory as input:</p> <pre><code>vscript scripts/dsm2/csdp_geom_converter.py &lt;dir_containing_csdp_calculated_xsections&gt;\n</code></pre> <p>This will create a <code>irregular_xsections_dsm2.inp</code> which will contain all the cross sections in that directory in the new 8.x format.</p> <ul> <li>CSDP Network File Format</li> <li>CSDP Network Summary Report</li> <li>Exporting Channel Lengths from CSDP Network file</li> <li>Exporting CSDP Information into GIS</li> <li>Extracting Bathymetry Data From An Irregularly Shaped Region</li> <li>Importing Digital Elevation Maps (DEMs) into CSDP</li> <li>Merging multiple versions of network files</li> </ul>"},{"location":"csdp/CSDP_Tutorial/#attachments","title":"Attachments:","text":"<ul> <li>csdpWebexClass.pdf (application/pdf)</li> <li>csdpWebexClass.ppt (application/vnd.ms-powerpoint)</li> </ul>"},{"location":"csdp/Creating_DSM2_v8.2_GIS_grid_map/","title":"Creating DSM2 v8.2 GIS Grid Map","text":""},{"location":"csdp/Creating_DSM2_v8.2_GIS_grid_map/#creating-shapefiles","title":"Creating Shapefiles","text":"<p>The existing CSDP network file for the DSM2 8.2 network is incomplete and contains errors. The network file for DSM2 v8.3 is accurate but includes additional channels and nodes that were moved. The easiest way to create shapefiles for the DSM2 v8.2 grid is to modify the network and landmark (nodes) for the 8.3 grid.</p>"},{"location":"csdp/Creating_DSM2_v8.2_GIS_grid_map/#verification","title":"Verification","text":"<p>The goal is to display the PDF grid map as a background image in ArcGIS to verify that all the channel and node numbers are correct and in the correct locations.</p> <ol> <li>Use the following command (using Ghostscript in Cygwin) to create a TIFF file from the DSM2 PDF grid map file:    <code>gs -q -dNOPAUSE -sDEVICE=tiffg4 -sOutputFile=gridmappdf.tif \"DSM2_Grid2.0 (1).pdf\" -c quit</code></li> <li>Create a copy of the TIFF file with \"marsh\" in the filename. This is because the PDF grid map has the Suisun Marsh disconnected from the delta and printed in a different scale.</li> <li>Identify a few landmarks that are easily identifiable on both the PDF grid map and the basemap in ArcGIS. Choose three points: one in the north delta, near the confluence, and in the south delta.</li> <li>In QGIS, select <code>Plugins &gt; Manage and Install Plugins</code>:    </li> <li>Search for \"GDAL\", check the box \"Georeferencer GDAL\", then click close:    </li> <li>Select <code>Raster &gt; Georeferencer</code>:    </li> <li>Click the Open Raster button:    </li> <li>Select the PDF file.</li> <li>Select <code>Settings &gt; Transformation Settings</code>:    </li> <li>Use the following settings, including an output filename:     </li> <li>Click on a point in the map, and enter UTM coordinates, then click OK:     </li> <li>When you have specified coordinates for all your points, click the start georeferencing button. A TIFF file will be created, which you can load into ArcGIS:     </li> <li>In ArcGIS, adjust the layer transparency:     </li> <li>Now you can easily compare the PDF grid map to the GIS data.</li> </ol>"},{"location":"csdp/Creating_DSM2_v8.2_GIS_grid_map/#attachments","title":"Attachments","text":"<ul> <li>image2020-5-12_9-25-2.png</li> <li>image2020-5-12_7-18-51.png</li> <li>image2020-5-12_7-17-47.png</li> <li>image2020-5-12_7-17-15.png</li> <li>image2020-5-12_7-16-53.png</li> <li>image2020-5-12_7-15-45.png</li> <li>image2020-5-12_7-14-51.png</li> <li>image2020-5-12_7-14-8.png</li> <li>image2020-5-12_7-12-43.png</li> </ul>"},{"location":"csdp/Cross-Section_Development_Program_CSDP_/","title":"Cross-Section Development Program (CSDP)","text":""},{"location":"csdp/Cross-Section_Development_Program_CSDP_/#introduction","title":"Introduction","text":"<p>Bathymetry data is used by CSDP to draw cross-sections, which are then converted to DSM2-Hydro cross-sectional input. Furthermore, CSDP provides the channel and cross-section locations in GIS projection of NAD 27, NGVD 29.</p> <p>CSDP was developed by Brad Tom in the 1990s and has recently been updated for use in the DSM2 GIS Reference Project.</p>"},{"location":"csdp/Cross-Section_Development_Program_CSDP_/#getting-started","title":"Getting Started","text":"<p>The CSDP Manual is available for reference. A hands-on tutorial and presentation given by Brad Tom in 2009 is also a good resource.</p> <p>In version 8.x, the irregular cross-section file format has changed. To update this information to the new format, run the following script:</p> <pre><code>vscript scripts/dsm2/csdp_geom_converter.py &lt;dir_containing_csdp_calculated_xsections&gt;\n</code></pre> <p>This will create an <code>irregular_xsections_dsm2.inp</code> file containing all the cross-sections in the new 8.x format.</p> <p>CSDP now creates DSM2 geometry input in both the original multi-file format and the newer single-file format, so the above script is no longer needed.</p> <p>An ArcGIS extension was developed as a modern replacement for CSDP by Tom Heinzer. However, this has not been publicly released yet, and the grid and cross-sections are still being developed in this tool.</p>"},{"location":"csdp/Cross-Section_Development_Program_CSDP_/#conversion-to-arcgis","title":"Conversion to ArcGIS","text":"<p>Using WKT (Well-Known Text) format and QGIS capabilities, information from CSDP files was converted to shapefiles. For more details, see DSM2 Geo-referenced Grid.</p>"},{"location":"csdp/Cross-Section_Development_Program_CSDP_/#attachments","title":"Attachments","text":"<ul> <li>CSDP_vs_Channels_inp_Lengths.xlsx</li> <li>Clifton_court_2011.png</li> <li>Clifton_court_1990.png</li> <li>RSAC092_2011.png</li> <li>RSAC092_1990.png</li> <li>RSAN018_2011.png</li> <li>RSAN018_1990.png</li> <li>RSAN007_2011.png</li> <li>RSAN007_1990.png</li> </ul>"},{"location":"csdp/DSM2_Geo_referenced_grid/","title":"DSM2 Geo-Referenced Grid","text":""},{"location":"csdp/DSM2_Geo_referenced_grid/#background","title":"Background","text":"<p>DSM2 input specifies geographically based information such as channel lengths and cross-section distances from the upstream node. However, the geographically referenced node locations or channel outlines are not directly needed for hydrodynamic calculations.</p> <p>Even though this information is not needed directly, it is very important to keep the geographically referenced information in sync with the input parameters such as channel length and cross-section locations in DSM2 input.</p>"},{"location":"csdp/DSM2_Geo_referenced_grid/#sacramento-san-joaquin-delta-grid","title":"Sacramento - San Joaquin Delta Grid","text":"<p>The original DSM2 grid was based on hand-calculated distances using USGS Quad maps of the Delta (circa 1990). These were done on paper maps, and the original information has been lost.</p> <p>A PDF version of this grid was created using AutoCAD. However, the node locations in this version are not geo-referenced to any projection system. Nodes and channels were not always placed carefully, as the map was primarily used to identify approximate locations of nodes, channels, and stations.</p> <p>In the late 1990s or early 2000s, a paper copy of the grid was digitized, resulting in a file containing approximate UTM coordinates of each node. This file was used by the DSM2 Cross-Section Development Program (CSDP) to create representations of DSM2 channels and cross-sections.</p> <p>CSDP was developed by Brad Tom and Nicky Sandhu based upon specifications written by Ralph Finch in 1998 to derive the cross sections from bathymetry data, which at the time were mostly single beam soundings of depth that were available over many decades in the Delta. This tool is the basis of the current grid in 2000 and the recalibrated grid in 2009. As a by-product of this effort, the node locations and channel outlines were stored in UTM coordinates. Originally, these were not used directly to derive channel lengths, but they were used indirectly in determining the distance of a cross-section from the upstream node of a channel. The DSM2 GIS Reference project, which began in 2018, used CSDP centerlines to determine channel lengths.</p> <p>CSDP was developed pre-ArcGIS, and with ArcGIS now being fairly standard in DWR, there is a need to provide this geographical information in ArcGIS format. In recent years, Jane Schafer-Kramer created an ArcGIS version of this grid. Jane developed, under Ralph Finch's guidance, an ArcGIS referenced grid by manually putting nodes at the closest location based on the PDF version of the grid. Again, the channel lengths from these would not match either CSDP or the original grid as it is an independent manual effort. Furthermore, there would be a mismatch to the location of the cross-section positions.</p> <p>In 2012, Tom Heinzer was contracted to develop an ArcGIS-based extension to allow a user to develop cross-sections from DEM, which in turn is based on interpolations of depth sounding data. This again is a work in progress and cannot import the current cross-sectional profiles available in CSDP.</p> <p>In 2017, CSDP grid data for the 2009 calibration was imported into ArcGIS along with the channel outlines and node locations. The channel outlines in ArcGIS were used to calculate lengths for the channels, and these were then compared to the current grid. There were many mismatches discovered, and these should be addressed in future efforts.</p>"},{"location":"csdp/DSM2_Geo_referenced_grid/#2009-grid","title":"2009 Grid","text":"<p>The 2009 Grid is used for DSM2 v8.2. It is similar to the PDF grid map, but it includes some upper Sacramento River changes.</p> <p>The node locations and the associated channel network lengths do have a match with the 2000 calibration files (spot checked). However, the 2009 CH2MHill mini calibration adjusted node positions, channel lengths, and cross-sections for channels 412-418. The changes made in these channels were incorporated into DSM2 and are included in DSM2 versions as recent as v8.2.0, which is the current release as of 10/2019. However, we did not get any CSDP or GIS data from CH2MHill. Node locations were reverse engineered using the mini calibration lengths, starting with the common node position from channel 412. The overall sum of the length (reach 412-418) was unchanged, and this assumption allows for a reasonable reverse engineering effort.</p> <p>This reverse engineered effort is available on the shared drive as shapefiles \\cnrastore-bdo\\Delta_Mod\\Share\\maps\\csdp_2009_calib_converted\\CSDP_Channels_Adjusted_MiniCalib.shp (channels) and \\cnrastore-bdo\\Delta_Mod\\Share\\maps\\csdp_2009_calib_converted\\CSDP_Nodes_Adjusted_MiniCalib.shp. The Nodes shapefile is missing some files and cannot be loaded into ArcGIS. It was loaded into OpenJUMP and exported to \\cnrastore-bdo\\Delta_Mod\\Share\\maps\\csdp_2009_calib_converted\\CSDP_Nodes_Adjusted_MiniCalib_Recovered.shp. These files are also available in \\nasbdo\\Modeling_Data\\DSM2_GIS_CSDP_gridmaps\\GISGridmapV8.2.</p> <p>These layers are the closest approximation to the grid used for DSM2 v8.2.</p> <p>There is a large discrepancy in the channel length for channel 422 between cross channel and northern head of Georgiana slough. CSDP and ArcGIS calculations put it at 3300 feet, while in DSM2 input files it is 5300 feet. This is not an isolated incidence; there are many others as documented in this CSDP_vs_Channels_inp_Lengths.xlsx.</p>"},{"location":"csdp/DSM2_Geo_referenced_grid/#dsm2-v81-and-v82-grid","title":"DSM2 v8.1 and v8.2 Grid","text":"<p>For version 8.1 and 8.2, use this grid for referencing DSM2 elements approximately. The channels and nodes layers are incomplete, not very accurate, and contain errors.</p> <p>Shapefiles are available in \\nasbdo\\Modeling_Data\\DSM2_GIS_CSDP_gridmaps\\GISGridmapV8.2\\</p>"},{"location":"csdp/DSM2_Geo_referenced_grid/#dsm2-v83-grid","title":"DSM2 v8.3 Grid","text":"<p>The 2019 grid is used for DSM2 v8.3, which is under development, and will be the result of the DSM2 GIS Reference Project.</p> <p>Three shapefiles (located here: \\nasbdo\\Modeling_Data\\DSM2_GIS_CSDP_gridmaps\\GISGridmapV8.3) each have been created from the CSDP network (channel centerlines) and landmark (nodes) data for both the 2009 calibration (DSM2 V8.2) and the 2019 calibration (DSM2 V8.3). The shapefiles were created by exporting network and landmark data from the CSDP to WKT files and importing the results into QGIS, then saving to shapefiles. This is intended to be a first step toward creating a georeferenced grid map. Shapefiles are available in \\nasbdo\\Modeling_Data\\DSM2_GIS_CSDP_gridmaps\\GISGridmapV8.3\\</p> <ol> <li>dsm2_channels_centerlines contains the channel centerlines as created in the CSDP. Many channels have endpoints that are not located at the node; this was done to improve the accuracy of the DSM2 channel volume. Also, many centerlines do not follow the actual channel centerline perfectly.</li> <li>dsm2_channels_straightlines contains straight lines connecting the two endpoints of each CSDP centerline.</li> <li>dsm2_nodes contains the CSDP landmark data. The node locations were previously not very accurate; they have now been corrected.</li> <li>dsm2_boundary_flow_nodes contains points placed at the locations of nodes where boundary flows are applied.</li> <li>dsm2_boundary_stage_node contains a point placed at the location of the node where the boundary stage is applied.</li> <li>dsm2_gates contains points placed at the approximate location of the channel centerline near each gate. In DSM2, gates are located at the ends of channels. The points in this layer are intended to represent the approximate location in DSM2, and not necessarily the physical location of the gate.</li> </ol>"},{"location":"csdp/DSM2_Geo_referenced_grid/#future-directions","title":"Future Directions","text":"<p>We need a georeferenced grid map. It should have the following features:</p> <ol> <li>Display nodes as circles with numbers inside.</li> <li>Display straightline channels with numbers, and an arrow indicating positive flow direction.</li> <li>Display channels derived from CSDP centerlines, with numbers, and an arrow indicating positive flow direction.</li> <li>Straightline channels and CSDP centerline channels should be different colors.</li> <li>Useful for printing on a plotter.</li> <li>Easy to modify when CSDP node locations or channels change.</li> <li> <p>Good contrast with background, so we can easily determine connectivity and read all the numbers.</p> </li> <li> <p>Michael Mehrdadi is working on an ArcGIS grid map using the shapefiles for the 2019 grid.</p> </li> <li>Hans Kim is working on a Google Earth grid map. This will likely be a useful training tool and may have other uses.</li> <li>The current version (as of 10/31/2019) of the grid map is found here: DSM2_Grid_191029.kml. It can be opened with Google Earth or imported into Google Map.</li> <li>Updates will be made as new shapefiles become available.</li> </ol>"},{"location":"csdp/DSM2_Geo_referenced_grid/#attachments","title":"Attachments","text":"<ul> <li>DSM2_Grid2.0.pdf</li> <li>DSM2_Grid_191029.kml</li> <li>CSDP_vs_Channels_inp_Lengths.xlsx</li> </ul>"},{"location":"csdp/DSM2_Sacramento_San-Joaquin_Delta_Grid/","title":"DSM2 Sacramento San-Joaquin Delta Grid","text":""},{"location":"csdp/DSM2_Sacramento_San-Joaquin_Delta_Grid/#introduction","title":"Introduction","text":"<p>The main area of application for DSM2 is the Sacramento San-Joaquin Delta. This grid has been developed over the years. A commonly used version is available as a PDF.</p>"},{"location":"csdp/DSM2_Sacramento_San-Joaquin_Delta_Grid/#arcgis-version","title":"ArcGIS Version","text":"<p>In recent years, Jane Schafer-Kramer created an ArcGIS version of this grid. This map representation will be refined and made available with the ArcGIS X-section editing tool, which is now in beta testing.</p> <p>To view channels colored by Manning's or dispersion, <code>channels.inp</code> was imported (from DSM2 v8.1.2) as a table. This was then joined with the \"DSM2 Channels\" table in ArcGIS on the channel number field. Symbology can then be used to display the Manning's attribute in the joined table.</p>"},{"location":"csdp/DSM2_Sacramento_San-Joaquin_Delta_Grid/#attachments","title":"Attachments","text":"<ul> <li>DSM2_Grid2.0_updated.pdf</li> <li>Delta_Stations_with_DSM2_Grid.mpk</li> <li>DSM2_Grid2.0.pdf</li> </ul>"},{"location":"csdp/Exporting_CSDP_Information_into_GIS/","title":"Exporting CSDP Information into GIS","text":""},{"location":"csdp/Exporting_CSDP_Information_into_GIS/#background","title":"Background","text":"<p>CSDP contains channel outlines, cross-section locations, cross-section profiles, and node locations. These are referenced in NAVD88 vertical datum and NAD83 horizontal datum in UTM Zone 10N projection.</p>"},{"location":"csdp/Exporting_CSDP_Information_into_GIS/#methods","title":"Methods","text":"<p>QGIS can import WKT (Well-Known Text) format into a text-based layer, which can then be exported to ArcGIS.</p>"},{"location":"csdp/Exporting_CSDP_Information_into_GIS/#java-utilities","title":"Java Utilities","text":"<ol> <li>Exporting CSDP channel outlines to WKT: CSDPChannelNetworkToWKT.java</li> <li>Exporting CSDP node locations to WKT: CSDPNodeCDLToWKT.java</li> </ol> <p>These utilities should be made standalone and generic for command-line use.</p>"},{"location":"csdp/Exporting_Channel_Lengths_from_CSDP_Network_file/","title":"Exporting Channel Lengths from CSDP Network File","text":"<p>CSDP can output just the channel IDs and lengths.</p> <ol> <li>Use the <code>Network &gt; Export Options</code> menu item to select only channel lengths output in station elevation format.</li> <li>Then use the <code>Network &gt; Save As</code> menu to save the file, which will contain only the channel ID and length in the output.</li> </ol>"},{"location":"csdp/Extracting_Bathymetry_Data_From_An_Irregularly_Shaped_Region/","title":"Extracting Bathymetry Data From an Irregularly Shaped Region","text":"<p>Using the CSDP, create a new centerline using the Centerline &gt; Create menu item.</p> <ol> <li>Add points to the centerline until it outlines the data you want to extract. The endpoints do not need to be in the same place. A polygon will be created whose vertices are all the centerline points, so the first and last points will be connected.</li> <li>Save the network file.</li> </ol> <p>A Java program called <code>ExtractShipChannelLeveesFromYoloBypassDEM</code> can be used to extract the data. This program uses hard-coded filenames for both the input (network file and bathymetry file) and the output (bathymetry file). Eventually, this code will be added to the Bathymetry menu in the CSDP.</p> <p></p>"},{"location":"csdp/Extracting_Bathymetry_Data_From_An_Irregularly_Shaped_Region/#attachments","title":"Attachments:","text":"<p> image2018-12-3_13-53-22.png (image/png) image2018-12-3_13-53-12.png (image/png) image2018-12-3_13-52-46.png (image/png)</p>"},{"location":"csdp/Importing_Digital_Elevation_Maps_DEMs_into_CSDP/","title":"Importing Digital Elevation Maps (DEMs) into CSDP","text":""},{"location":"csdp/Importing_Digital_Elevation_Maps_DEMs_into_CSDP/#background","title":"Background","text":"<p>CSDP was developed in the late 1990s and can only consume point features in its custom format. This document outlines the process of converting modern DEMs in raster form into files that CSDP can use.</p>"},{"location":"csdp/Importing_Digital_Elevation_Maps_DEMs_into_CSDP/#csdp-file-format","title":"CSDP File Format","text":"<p>CSDP supports bathymetry data as point features in XYZ format along with columns for year and source of data. Below is a sample header from a CSDP bathymetry file:</p> <pre><code>;HorizontalDatum:  UTMNAD83\n;HorizontalZone:   10\n;HorizontalUnits:  Meters\n;VerticalDatum:    NAVD88\n;VerticalUnits:    USSurveyFeet\n;Filetype: bathymetry\n;NumElements: 1544252\n563970.000000000 4234180.000000000 112.7323 2012 SF_DEM\n563990.000000000 4234180.000000000 117.6413 2012 SF_DEM\n</code></pre>"},{"location":"csdp/Importing_Digital_Elevation_Maps_DEMs_into_CSDP/#steps","title":"Steps","text":"<ol> <li>Use <code>Arc Toolbox &gt; Conversion Tools &gt; From Raster &gt; Raster to ASCII</code> to output the DEM as a text file.</li> <li>Ensure the \"Output Coordinates\" are in NAD83, Zone 10, in meters in UTM projection, and the vertical datum is NAVD88 in meters.</li> <li>Limit the DEM output to the viewable area by setting the \"Processing Extent\" to \"Same as Display.\"</li> <li>Use the program ASCIIGridToCSDPConverter to convert the raster ASCII file to a CSDP-compatible format.</li> <li>Open the output file in CSDP.</li> <li> <p>You can also use the CSDP:</p> <p> <p>Select Bathymetry-Import Bathymetry from ASCII Raster</p> <p></p> <p>Fill in the dialog. If dataset is more dense than you need, you can enter a factor greater than 1</p> <p></p> </p> </li> </ol>"},{"location":"csdp/Importing_Digital_Elevation_Maps_DEMs_into_CSDP/#attachments","title":"Attachments:","text":""},{"location":"csdp/Merging_multiple_versions_of_network_files/","title":"Merging multiple versions of network files","text":"<p>One way to merge changes from multiple users is to use a file comparison tool such as WinMerge. However, if the users did not start with identical versions of the network file, this won't work.</p> <p>I have added a feature to the CSDP (Network-Save Specified Channels), which helps merge changes from network files submitted by multiple users who did not start with the same version of the network file.</p> <p>Here's how I use it to merge changes into an existing network file:</p> <ol> <li>Get a list of modified centerlines for the new network file.</li> <li>Enter the list into Excel.\u00a0</li> <li>Copy the list, and paste-special-transpose.</li> <li>In the CSDP, load the existing network file, which you might refer     to as the current master version.\u00a0<ol> <li>Select Network-Save Specified Channels.</li> <li>In the file selector dialog, enter a filename for the new master     version.\u00a0</li> <li>Go back to Excel, and copy the transposed list of centerline     numbers. Past them into the dialog that appears (below) in the     Channel Numbers field. (The list will be tab delimited, which is     fine). You may not be able to see all the centerline names in     the text field (I'll have to work on that), but it will work.</li> <li>Before clicking OK, click the \"Don't export specified channels\"     checkbox. Make sure this option is selected before you click     OK. When you click OK, CSDP will create a new network file     containing all centerlines EXCEPT the ones you specified. </li> </ol> </li> <li>Now load the new network file.<ol> <li>Select Network-Save Specified Channels.</li> <li>In the file selector dialog, enter a filename for a temporary     network file.</li> <li>Go back to Excel, and copy the transposed list of centerline     numbers. Paste them into the dialog that appears (below) in the     Channel Numbers field. (The list will be tab delimited, which is     fine). You may not be able to see all the centerline names in     the text field (I'll have to work on that), but it will     work.     </li> <li>Before clicking OK, make sure the \"Don't export specified     channels\" checkbox is NOT selected.\u00a0When you click OK, CSDP     will create a new network file containing only the centerlines     you specified.\u00a0</li> </ol> </li> <li>Use a text editor to copy the contents (excluding the headers at the     top) of the new temporary network file into the new master network     file. Update the numElements field in the header of the new file.     The value should be the sum of the values from the two files.</li> </ol>"},{"location":"csdp/Merging_multiple_versions_of_network_files/#attachments","title":"Attachments:","text":"<p>image2018-12-21_12-8-47.png (image/png) image2018-12-21_12-0-36.png (image/png) image2018-12-21_11-49-11.png (image/png)</p>"},{"location":"csdp/Schematics_and_Boundaries/","title":"Schematics and Boundaries","text":"<p>CalSIM II\u00a0schematic is retrieved from BDO Central Valley modeling website</p> <p>CalSIM II schematic (PDF)</p> <p>BST_CALSIMII_schematic_040110.pdf</p> <p></p> <p>DSM2 (v812) schematic is retrieved from DSM2 release package dsm2\\documentation</p> <p></p> <p>DSM2 inputs are retrieved from CalSIM output</p> DSM2 name DSM2\u00a0node CalSIM II CalSIM 3 <p>BOUNDARY_FLOW</p> <p> </p> <p> </p> calaveras <p>FLOW-CHANNEL</p> <p> </p> 21 C508_R514 C_CLV004 cosumnes 446 C501 C_CSM005 yolo 316 C157 C_CSL005 sac FLOW 330 C169_D168B_D168C C_SAC041 vernalis 17 C639_R644 C_SJR070 moke <p>FLOW-INFLOW</p> 447 I504 <p>C_MOK022</p> <p>SR_60N_MOK019</p> <p>SR_26S_MOK014</p> <p>SOURCE_FLOW</p> AntiochWW <p>FLOW-DELIVERY</p> \u00a046 D406B COSMA1 33 D514A \u00a0D_SJR028_WTPDWS COSMA2 33 D514B FRWP_EBMUD 332 D168B FRWP_SCWA 332 D168C northbay 273 D403B \u00a0C_CSL004B nb_fvb 273 D403D nb_sol 273 D403C ccc 206 D408_RS D408 CCWDVC 191 D408_VC oldr034 80 D408_OR vallejo 320 D403A cvp \u00a0181 D418_TD_ADJ \u00a0D_OMR028_DMC000 SOURCE_FLOW_RESERVOIR swp FLOW-DELIVERY clifton_court D419_TD_ADJ \u00a0D_OMR027_CAA000 NODE_CONCENTRATION vernalis SALINITY-EC 17 VERNWQFINAL VERNWQFINAL"},{"location":"csdp/Schematics_and_Boundaries/#attachments","title":"Attachments:","text":"<p> DSM2_Grid2.0.pdf (application/pdf) BST_CALSIMII_schematic_040110.pdf (application/pdf)</p>"},{"location":"csdp/Symbology/","title":"Symbology","text":"<p>The following symbology is needed:</p> <ol> <li>Arrows on or next to straight line channels and centerline channels.</li> <li>Nodes displayed as circles with numbers in the middle.<ol> <li>In ArcGIS Pro, change the symbol to a circle (double click on     the symbol in the contents pane) and set the circle size to 20     pt, and the font to Tahoma 8. Select Enable scale-based sizing.</li> </ol> </li> <li>Gates displayed using symbol similar to that used in the pdf grid     map. Using Meteorology-Fog, Light.</li> </ol>"},{"location":"csdp/Symbology/#channel-arrows","title":"Channel Arrows","text":"<p>The direction of arrows created using symbology is determined by the order in which the points are defined in each line. Since the channel lines (both straight and centerlines) are derived from CSDP data, the points should always be in order from upstream to downstream.</p> <p>To modify the symbol:</p> <ol> <li>double-click on the symbol below the layer name (see image below).    </li> <li>In the Symbology Panel, under Gallery, select \"Arrow Right Middle\".    </li> <li>Then click on Properties, set the color, and set line width to 15 pt     (or whatever you want, and select \"Enable scale-based sizing\".</li> </ol>"},{"location":"csdp/Symbology/#attachments","title":"Attachments:","text":"<p> image2020-5-14_15-15-14.png (image/png) image2020-5-14_15-14-8.png (image/png)</p>"},{"location":"csdp/overview/","title":"CSDP: Cross-Section Development Program","text":"<p>Cross Section Development Program (CSDP) is a tool used to develop cross-sectional information for the hydrodynamic model DSM2. DSM2 has adopted the NAVD88 as its datum and this update is for the data files associated with CSDP.</p> <p>User Manual</p> <p>The program can be downloaded from Open data portal</p>"},{"location":"faqs/","title":"DSM2 FAQ","text":"<ul> <li>How does DSM2-Hydro calculate channel volume?</li> </ul>"},{"location":"faqs/#how-to-articles","title":"How To articles","text":"<ul> <li>How to read hdf5 output files</li> <li>Delta Tutorial 9 - DO Simulation</li> <li>Delta Tutorial 8 - Temperature Simulation</li> </ul>"},{"location":"faqs/DSM2_-_How_to_read_hdf5_output_files/","title":"DSM2 - How to read hdf5 output files","text":"<p>DSM2 writes output in HDF5 format. This format can be read by Vista and vscript\u00a0</p>"},{"location":"faqs/DSM2_-_How_to_read_hdf5_output_files/#step-by-step-guide","title":"Step-by-step guide","text":"<p>To open a HDF5 file</p> <ol> <li>Open Vista\u00a0</li> <li>Drag and drop or use Session &gt; Open &gt; Tidefile from the menu     options</li> <li>Select the data items needed and use the\u00a0 Data &gt; Export &gt; Export     Data To DSS menu item to export the Data to DSS files</li> </ol> <p>Video of How to read DSM2 - HDF5 files using VISTA</p> <p>Alternatively here is a snippet of vscript code that does something similar</p> <p>Getting average concentrations from Qual HDF5 file</p> <pre><code>from vtidefile import opentidefile\nfrom vdss import writedss\nfrom vutils import *\nimport vdisplay\nfrom vdisplay import plot\nimport sys\nimport string\n\ndef get_avg_conc(tidefile, chan, twstr):\n    tf=opentidefile(tidefile)\n    if twstr != None:\n        print 'Timewindow: %s'%twstr\n        tw=timewindow(twstr)\n    else:\n        tw=None\n    refs=tf.find(['','^%s$'%chan,'AVG CONC'])\n    if refs and len(refs)==1:\n        print \"Getting data %s\"%(str(chan))\n        if tw!=None:\n            ref=DataReference.create(refs[0],tw)\n        else:\n            ref=refs[0]\n        return ref.data\n    else:\n        raise \"No data found for %s in file %s\"%(chan, tidefile)\nif __name__ == '__main__':\n    if len(sys.argv) != 2:\n        print \"Usage: vscript \n    tidefile=sys.argv[1]\n    twstr=\"01JUL2014 0000 - 01AUG2014 0000\"\n    chans=[291,290,436,435,434,433]\n    chan_concs=[]\n    for chan in chans:\n        chan_concs.append(get_avg_conc(tidefile, chan, twstr))\n\n    for conc in chan_concs:\n        plot(conc)\n</code></pre> <p>DSM2-vista also supports export data to hec-dss format (One or multiple timeseries path could be selected) See the following menu option as example.\u00a0</p> <p></p>"},{"location":"faqs/DSM2_Seems_to_indicate_missing_irregular_gate_position_data/","title":"DSM2 Seems to indicate missing irregular (gate position) data","text":""},{"location":"faqs/DSM2_Seems_to_indicate_missing_irregular_gate_position_data/#problem-hydro-displays-the-following","title":"Problem: Hydro displays the following.","text":"<p>Error in reading time-varying data: Current time is 01SEP2001 2400; earliest data time for /HIST+GATE/MTZSL/BOATLOCK_OP//IR-DECADE/DWR-ESO/ is\u00a0</p> <p> </p> <p>Brad Tom\u00a0Related to\u00a0Jira issue. I think we should open an issue there as you are doing the practical fix for this known issue\u00a0</p> Key Summary T Created Updated Due Assignee Reporter P Status Resolution DSM2-106 Timeseries interpolation [&lt;img src=\"http://msb-jira/secure/viewavatar?size=xsmall&amp;avatarId=10303&amp;avatarType=issuetype\" class=\"icon\" alt=\"Bug\" /&gt;](http://msb-jira/browse/DSM2-106?src=confmacro) Nov 30, 2011 Jan 03, 2022 Nicky Sandhu Ines Ferreira &lt;img src=\"http://msb-jira/images/icons/priorities/medium.svg\" class=\"icon\" alt=\"Medium\" /&gt; Resolved Won't Do <p>1 issue</p> <p>But there are data values in this time series with dates before the current date.</p> <p>The problem is: When using an IR-DECADE dss path, there must be a value with a timestamp that is at the beginning of the current decade. In this case, a record is required that has a timestamp of 31DEC2000 2400.</p>"},{"location":"faqs/DSM2_Seems_to_indicate_missing_irregular_gate_position_data/#to-fix-this","title":"To fix this:","text":"<ol> <li>Tabulate the data in HEC-DssVue. There is no beginning of decade timestamp.</li> </ol> <p>![Missing Gate Data Figure 1](../attachments/FAQ_MissingGateData_Figure1.png)</p> <p>2. Turn on \"Allow Editing\"</p> <p>![Missing Gate Data Figure 2](../attachments/FAQ_MissingGateData_Figure2.png)</p> <p>3. Select the row before the end of the previous decade, and select \"Insert Rows\":</p> <p></p> <p>4. Change \"Number Rows\" to 1.</p> <p></p> <p>5.Enter the timestamp that is needed, with a value equal to the value in the previous record:\u00a0</p> <p></p> <p>6. Save the data:</p> <p></p>"},{"location":"faqs/DSM2_Seems_to_indicate_missing_irregular_gate_position_data/#attachments","title":"Attachments:","text":""},{"location":"faqs/DSM2_inputs_are_off_by_1DAY/","title":"DSM2 inputs are off by 1DAY","text":"<p>DSM2-241 - Check daily inflows and warn if INST-VAL Open</p> <p>A problem was reported with a run that was supposed to be a historical run with inflows scaled up by a factor.\u00a0</p> <ol> <li>Here's a plot of model output at Vernalis</li> </ol> <ul> <li>Blue=historical stage output</li> <li>Red=scaled up stage output</li> <li>Green=historical flow output</li> <li>Black=scaled up flow output</li> </ul> <p>On February 14, 1992, both the scaled up flow and stage outputs are higher than historical.</p> <p></p> <p>2. The problem was caused by the \"Type\" of the scaled up flow input tine series. The user had created a series with a type of INST-VAL.</p> <p>This results in changes in inflow taking effect at the end of the day rather than the beginning of the day.</p> <p>One way to tell that a 1DAY time series is PER-AVER vs INST-VAL is that HEC-DSSVue plots PER-AVER time series as a square wave, but not INST-VAL.</p> <p></p> <p> </p> <p>3. To check Type and to change it, in HEC-DSSVue, right click on the series and select Edit, and\u00a0</p> <p></p> <p>use the dropdown to select a new type.</p> <p></p> <p>4. After re-running with the change, results are as expected.</p> <p></p> <p> </p>"},{"location":"faqs/DSM2_inputs_are_off_by_1DAY/#attachments","title":"Attachments:","text":"<p> image2020-1-15_8-19-11.png (image/png) image2020-1-15_8-20-52.png (image/png) image2020-1-15_8-21-6.png (image/png) image2020-1-15_8-29-2.png (image/png) image2020-1-15_8-19-39.png (image/png)  </p>"},{"location":"faqs/Generating_DSM2_Hydro_boundary_conditions_from_the_DCR_2017_CALSIM_II_output/","title":"Generating DSM2 Hydro boundary conditions from the DCR 2017 CALSIM II output","text":"<p>run DCR 2017 with CWF script.zip</p> <p>I tried to generate the\u00a0DSM2 Hydro boundary conditions\u00a0from the DCR 2017 CALSIM II output (with a 2020 development level) .\u00a0 But for this CALSIM II output, we don\u2019t have a corresponding script that can be used to generate the boundary conditions. The closest script available is from CALWATERFix for the previous version of CALSIM II output (with a 2005 development level).\u00a0 So I used this script.\u00a0 After some basic edits (e.g., change directory, file name, etc.),\u00a0 the script ran and generated the boundary conditions.\u00a0 However, because the script and the CALSIM II output are not paired, using the old script for the new output could introduce errors.\u00a0 I compared the boundary conditions generated from DCR 2017\u00a0 with those for CALWATERFix (generated with the same script but from an older version of CALSIM output).\u00a0 The patterns are match but there are noticeable differences at some spots (e.g., Aug 24 1994 or July 19 2001).\u00a0 I haven't figured out what exactly caused the differences, but could be the different assumptions used in the CALSIM II studies.\u00a0\u00a0</p> <p>I have a read me file inside of the zip file to provide the instruction about how to run the script.\u00a0 The zip file is too big to upload so I left two files out.\u00a0 You can download the left out files here:</p> <ol> <li>this file should be in .\\run DCR 2017 with CWF     script\\timeseries\u00a0Planning_Tide_82years.zip</li> <li>this file should be in .\\run DCR 2017 with CWF     script\\studies\\planning\\timeseries\\CALSIM2020D09EDV__2017DCR_OldANN_NewWSIDI-SWPDemand_x64_20171115.zip</li> </ol>"},{"location":"faqs/Generating_DSM2_Hydro_boundary_conditions_from_the_DCR_2017_CALSIM_II_output/#attachments","title":"Attachments:","text":"<p> 2020D09EDV__2017DCR_OldANN_NewWSIDI-SWPDemand_x64_20171115.zip (application/zip) Planning_Tide_82years.zip (application/zip) run DCR 2017 with CWF script.zip (application/zip)  </p>"},{"location":"faqs/How_does_DSM2-Hydro_calculate_channel_volume_/","title":"How does DSM2-Hydro calculate channel volume?","text":"<ul> <li>Hydro creates virtual cross-sections by interpolating cross-section     input (see Tutorial 1: Channels)     to create virtual cross-sections. Virtual cross-sections are created     and used internally in Hydro by interpolating cross-section input.</li> <li>Virtual cross-sections are usually not seen by the user. If the     variable printlevel &gt;= 5 in the SCALAR input section, virtual     cross-sections will be written to output .hof file.</li> <li>The 2012 Annual Report describes a change in the way volume is     calculated: it used to use only the area of the cross-section in the     middle of a computational reach, but now it uses all 3 of the     cross-sections in a computational reach. Also, it describes an     important change to the longitudinal interpolation used to create     virtual cross-sections.</li> <li>The 2016 Annual Report, section 3.4.2 indicates that the volume of a     channel is calculated by multiplying the average of two     cross-sectional areas by the distance between them. This process     would then be repeated twice for each computational reach to find     the volume.</li> <li>Hydro will not converge well if cross-sectional area is not     interpolated correctly. Previously, area at a given elevation     between cross-section layers was calculated by interpolating area     linearly between two layers. It has been changed to a =     a1+(.5*(w1+w2))*h, where<ul> <li>a1 = area at lower elevation</li> <li>w1 = width at lower elevation</li> <li>w2 = width at higher elevation</li> <li>h = distance from lower elevation to given elevation</li> </ul> </li> </ul>"},{"location":"faqs/How_does_DSM2-Hydro_calculate_channel_volume_/#references","title":"References","text":"<p>Annual reports can be found here.</p> <p>Ferreira I. and Sandhu, N. 2016 \"Chapter 3: DSM2 Extension: A GIS-Based Approach.\"\u00a0 In: Methodology for Flow and Salinity Estimates in the Sacramento-San Joaquin Delta and Suisun Marsh. 37th Annual Progress Report to the State Water Resources Control Board. California Department of Water Resources.</p> <p>Liu L., Ateljevich E., and Sandhu P. 2012. \u201cChapter 2: Improved Geometry Interpolation in DSM2-Hydro.\u201d In: Methodology for Flow and Salinity Estimates in the Sacramento-San Joaquin Delta and Suisun Marsh. 33rd Annual Progress Report to the State Water Resources Control Board. California Department of Water Resources.</p> <p>Tom B. 1998. \u201cChapter 6: Cross-Section Development Program.\u201d In: Methodology for Flow and Salinity Estimates in the Sacramento-San Joaquin Delta and Suisun Marsh. 19th Annual Progress Report to the State Water Resources Control Board. California Department of Water Resources.</p>"},{"location":"faqs/PTM_Frequently_Asked_Questions_FAQ_/","title":"PTM Frequently Asked Questions (FAQ)","text":"<p>Moving from ptm source directory in github. This should belong in the documentation and not buried in source code</p>"},{"location":"faqs/PTM_Frequently_Asked_Questions_FAQ_/#q1-what-is-ptm","title":"Q1. What is PTM?","text":"<p>PTM is Particle Tracking Model. It is written partly in Java and partly in Fortran. I/O is mainly handled by Fortran.</p>"},{"location":"faqs/PTM_Frequently_Asked_Questions_FAQ_/#q2-what-are-the-inputs-to-ptm","title":"Q2. What are the inputs to PTM?","text":"<p>1. Hydrodynamic information: This is the dynamic information about flow, flow-area, (therefore velocity) and depth. This information comes from the tidefile which is generated by hydro. One has to make sure that in addition to the tidefile the correct network configuration is being used.\u00a0 This information is typically done by the following io structure.</p> <p>TIDEFILE START_DATE START_TIME END_DATE END_TIME FILENAME generic none length none tidefile.out END</p> <p>2. Network configuration: This defines how channels and reservoirs are linked up and what their characteristics such as x-section, length, etcetra are. This also is read by Fortran. - channels.inp, xsects.inp, junctions.inp, reservoirs.inp, translations.inp</p> <p>Refer to DSM2 docs</p> <p>3. Particle information:</p> <p>a. Type of particle: Uptil now we have been dealing only with neutrally-bouyant or particles with a certain falling velocity. For other kind of particles such as fish no IO had been decided.</p> <p>b. Particle insertion information: Number of particles, time of insertion, location of insertion and duration of insertion. Refer to ptm_insertion.inp.  </p> <p>PARTINP [NODE NPARTS SDATE STIME EDATE ETIME] | 44 500 01jan1990 0133 05jan1990 0333</p> <p>This means that insert 500 particles at 44 evenly distributed from start time to end time</p> <p>[NODE NPARTS SDATE LENGTH] 44 600 01jan1990 5days END  </p> <p>c. Run time information</p> <p>This is similar to run time settings for hydro and qual. Refer to DSM2 docs.</p> <p>d. PTM has the following scalars</p> <p>SCALAR ptm_time_step 15min # PTM time step display_intvl 1hour # how often to display run progress ptm_ivert t # Use Vertical velocity profile ptm_itrans t # Use Transverse velocity profile ptm_iey t # Use transverse mixing ptm_iez t # Use vertical mixing ptm_fallvel 0.05 # settling velociy in ft/s ptm_random_seed 32001 # Starting Random Number Seed ptm_trans_constant 0.06 # Transverse Mixing Constant ptm_vert_constant 0.0067 # Vertical Mixing Constant END</p> <p>d. IO from PTM</p> <p>IO_FILES MODEL TYPE IO INTERVAL FILENAME ptm anim out 15min anim.bin # animation file ptm trace out none trace.bin # trace file ptm restart out 6hours restart.out # restart output file ptm restart in none restart.inp # restart input file END</p> <p>Animation file: Contains the data for the first 100 particles movement every time interval as specified. This is a binary file if the file name does not end in \".out\" else it will be a ascii file. One can use PTM Animator to run the binary file to look at the animation visually</p> <p>Trace file: The trace file contains the trace of every particle in the system. It records the entrance/exit of a particle into a waterbody such as a channel etcetra. \".out\" for ascii file. The trace file is used to calculate the flux and so the flux may be calculated after the PTM run.</p> <p>Restart file: This is a snapshot of the current locations of every particle inthe system. Useful mainly for restarting a run from a previously saved state.</p> <p>Flux information: PARTICLE_FLUX FROM_WB TO_WB INTERVAL FILENAME B_PART chan, 216 | qext, cvp | 15min flux.txt past_CVP res,clfct | qext,swp | 15min flux.txt past_SWP chan,436,53 | chan,442,437 | 15min flux.txt past_Chipps chan, 441 | stage, mtz | 15min flux.txt past_MTZ | qext,div,-cvp, -ccc | 15min flux.txt Ag_Diversions | qext,div | 15min flux.txt All_Diversions END</p> <p>This instructs ptm to calculate flux from the trace file. This can be done at the end of a ptm run or from a trace file later. Its the users responsibility to provide the correct network configuration The filename flux.txt means the user wants the output in ascii format else it would be flux.dss which is output in dss format. The DSS pathnames B part == B_PART from the above table.</p> <p>The interval at which the flux is calculated is as given above in the</p> <p>INTERVAL column</p> <p>The particle flux is calculated from a waterbody to a waterbody. A waterbody is specified by a type string followed by an identifier. The type string is one of chan, res, qext, stage.</p> <p>The identifier is either a number for the the chan or a name as defined in the translations.inp file. If a generic type follows in place of the identifier than the flux tracks all particles entering or leaving that particular type of waterbody. If no from_wb or to_wb is defined it is assumed that it is the same as to all waterbodies.</p> <p>Particle dynamics:</p> <p>The particle can move in x, y and z directions. However as DSM2 is a 1-D model this information is gleaned by applying a vertical and transverse velocity profile to the average velocity available from the tidefile.</p> <p>A particle has a deterministic and random component to its movement.</p> <p>Deterministic component == Average velocity + transformations</p>"},{"location":"learning_series/eco_ptm_studies/","title":"ECO-PTM","text":""},{"location":"learning_series/eco_ptm_studies/#eco-ptm-training","title":"ECO-PTM Training","text":"<p>The ECO-PTM training was held on April 16, 2024.  The slides shown in the training are available on our GitHub repository.</p> <p>This is the  ECO-PTM input reference manual</p>"},{"location":"learning_series/eco_ptm_studies/#training-videos","title":"Training Videos","text":"<p>You can watch the training videos directly below:</p>"},{"location":"learning_series/overview/","title":"DSM2 Learning Series","text":"<p>The DSM2 Learning Series is a series of hybrid live and online classes held by the Delta Modeling Section.   Videos for the Series are available on our YouTube playlist.</p>"},{"location":"learning_series/overview/#training-videos","title":"Training Videos","text":"<p>You can watch the entire series directly below:</p>"},{"location":"learning_series/planning_studies/","title":"Planning Studies","text":""},{"location":"learning_series/planning_studies/#dsm2-planning-studies","title":"DSM2 Planning Studies","text":"<p>The DSM2 Planning studies training was held on October 27, 2023.  The slides shown in the training are available on our GitHub repository.</p>"},{"location":"learning_series/planning_studies/#training-videos","title":"Training Videos","text":"<p>You can watch the training videos directly below:</p>"},{"location":"learning_series/quick_start/","title":"Quick Start","text":"<p>The DSM2 Quick Start Training was held on June 23, 2023.  The slides shown in the training are available on our GitHub repository.</p>"},{"location":"learning_series/quick_start/#training-videos","title":"Training Videos","text":"<p>You can watch the training videos directly below:</p>"},{"location":"manual/reference/Boundary_Flow/","title":"Boundary Flow","text":""},{"location":"manual/reference/Boundary_Flow/#overview","title":"Overview","text":"<p>Boundary Flows are boundary conditions at nodes where flow is known. This view defines these time series and assigns time series data to them.</p>"},{"location":"manual/reference/Boundary_Flow/#tables","title":"Tables","text":""},{"location":"manual/reference/Boundary_Flow/#example","title":"Example","text":"<pre><code># Description:\n# Historical boundary flows to Delta\nBOUNDARY_FLOW\nNAME      NODE SIGN FILLIN   FILE          PATH                                                     \ncalaveras   21    1   last   ${BNDRYINPUT} /FILL+CHAN/RCAL009/FLOW//1DAY/${HISTFLOWVERSION}/         \ncosumnes   446    1   last   ${BNDRYINPUT} /FILL+CHAN/RCSM075/FLOW//1DAY/${HISTFLOWVERSION}/         \nmoke       447    1   last   ${BNDRYINPUT} /FILL+CHAN/RMKL070/FLOW//1DAY/${HISTFLOWVERSION}/         \nnorth_bay  273   -1   last   ${BNDRYINPUT} /FILL+CHAN/SLBAR002/FLOW-EXPORT//1DAY/${HISTFLOWVERSION}/ \nsac        330    1   last   ${BNDRYINPUT} /FILL+CHAN/RSAC155/FLOW//1DAY/${HISTFLOWVERSION}/         \nvernalis    17    1   last   ${BNDRYINPUT} /FILL+CHAN/RSAN112/FLOW//1DAY/${HISTFLOWVERSION}/         \nyolo       316    1   last   ${BNDRYINPUT} /FILL+CHAN/BYOLO040/FLOW//1DAY/${HISTFLOWVERSION}/        \nEND\n</code></pre>"},{"location":"manual/reference/Boundary_Flow/#boundary_flow","title":"BOUNDARY_FLOW","text":"<p>The Boundary Flow table defines flow boundary conditions by giving them names and associating them with a node. The table also assigns a time series to the boundary condition. Boundary Flow is a top-level layered table.</p>"},{"location":"manual/reference/Boundary_Flow/#field-descriptions","title":"Field Descriptions","text":"<ul> <li>NAME: Name assigned to the source. This is the identifier of the boundary and is referred to elsewhere in the input system. If you assign water quality, you will use the same name to match concentration to flow.</li> <li>NODE: Node number at which the source is applied.</li> <li>SIGN: Forces the time series to be a source or a sink. Positive values are normally associated with a source, but the data (especially sinks such as agricultural diversions) are sometimes measured in absolute flow. Use <code>1</code> to force the value to be a positive source or <code>-1</code> to interpret values as a sink.</li> <li>FILLIN: Method for filling in data if the time step of the assigned series is coarser than the time step of the model. See fillin types.</li> <li>FILE: DSS or text file in which data are stored. Use consistent case when referring to the same file. You may also enter the word <code>constant</code> if you would like to assign a constant value to the input (the value will be entered in the next column).</li> <li>PATH: The path within the text or DSS file of the time series data. If you used the <code>constant</code> keyword in the Input File column, enter the value (e.g., <code>4.22</code>) here.</li> </ul>"},{"location":"manual/reference/Boundary_Flow/#table-info","title":"Table Info","text":"<ul> <li>Identifier: NAME</li> <li>Include Block: HYDRO_TIME_SERIES</li> </ul> <p>Multiple sources and sinks can be assigned to a node. They are usually kept separate in order to assign different concentrations to them. HYDRO is able to accept sources and sinks at boundary nodes, but this is not good modeling practice. Use them on the interior.</p>"},{"location":"manual/reference/Boundary_Stage/","title":"Boundary Stage","text":""},{"location":"manual/reference/Boundary_Stage/#overview","title":"Overview","text":"<p>Stage Boundaries are locations where water levels are known. They are often used to represent the tidal boundary of an estuary. This view defines the tidal boundary and assigns a time series to water levels at that boundary.</p>"},{"location":"manual/reference/Boundary_Stage/#tables","title":"Tables","text":""},{"location":"manual/reference/Boundary_Stage/#example","title":"Example","text":"<pre><code># Description:\n# Historical stage at Martinez\nBOUNDARY_STAGE\nNAME  NODE  FILLIN  FILE           PATH                                                \nmtz   361   linear  ${BNDRYINPUT}  /FILL+CHAN/RSAC054/STAGE//15MIN/${HISTSTAGEVERSION}_NAVD/ \nEND\n</code></pre>"},{"location":"manual/reference/Boundary_Stage/#stage-boundary-table","title":"Stage Boundary Table","text":"<p>The Stage Boundary table defines the stage boundary by giving it a name and associating it with a node. The table also assigns a time series to the boundary. Stage Boundary is a top-level layered table.</p>"},{"location":"manual/reference/Boundary_Stage/#field-descriptions","title":"Field Descriptions","text":"<ul> <li>NAME: Name assigned to the source. This is the identifier of the boundary and is referred to elsewhere in the input system. If you assign water quality, you will use the same name to match concentration to flow.</li> <li>NODE: Node number at which the source is applied.</li> <li>FILLIN: Method for filling in data if the time step of the assigned series is coarser than the time step of the model. See fillin types.</li> <li>FILE: DSS or text file in which data are stored. Use consistent case when referring to the same file. You may also enter the word <code>constant</code> if you would like to assign a constant value to the input (the value will be entered in the next column).</li> <li>PATH: The path within the text or DSS file of the time series data. If you used the <code>constant</code> keyword in the Input File column, enter the value (e.g., <code>4.22</code>) here.</li> </ul>"},{"location":"manual/reference/Boundary_Stage/#table-info","title":"Table Info","text":"<ul> <li>Identifier: NAME</li> <li>Include Block: HYDRO_TIME_SERIES</li> </ul> <p>Only one boundary (flow or stage) should be assigned at a node. HYDRO is able to accept sources and sinks at boundary nodes, but this is not good modeling practice.</p>"},{"location":"manual/reference/Channel_Initial_Condition/","title":"Channel Initial Condition","text":""},{"location":"manual/reference/Channel_Initial_Condition/#overview","title":"Overview:","text":"<p>HYDRO requires water surface and flow initial condition. This view allows the user to specify default initial conditions. The default initial condition is required. The default will be overridden if a restart file is used.</p>"},{"location":"manual/reference/Channel_Initial_Condition/#tables","title":"Tables:","text":"<ul> <li> <ul> <li>Channel Initial Conditions</li> </ul> </li> </ul>"},{"location":"manual/reference/Channel_Initial_Condition/#channel_ic","title":"CHANNEL_IC","text":"<p>The table pairs channel locations with default initial values. Interpolation is used between locations. Water surface (stage) and flow must be specified at the upstream and downstream ends of the channel.</p>"},{"location":"manual/reference/Channel_Initial_Condition/#field-descriptions","title":"Field Descriptions","text":""},{"location":"manual/reference/Channel_Initial_Condition/#chan_no","title":"CHAN_NO","text":"<p>Channel number of channel where initial condition is to be applied.</p>"},{"location":"manual/reference/Channel_Initial_Condition/#distance","title":"DISTANCE","text":"<p>Distance along channel where initial condition is to be applied. This may be a numerical distance or the keyword \"length\" to indicate the end of the channel. If you edit an entry that says \"length\", you may see a complicated coded value, which is only for internal use.</p>"},{"location":"manual/reference/Channel_Initial_Condition/#stage","title":"STAGE","text":"<p>Initial water surface elevation.</p>"},{"location":"manual/reference/Channel_Initial_Condition/#flow","title":"FLOW","text":"<p>Initial flow (cfs).</p>"},{"location":"manual/reference/Channel_Initial_Condition/#table-info","title":"Table Info","text":""},{"location":"manual/reference/Channel_Initial_Condition/#identifier","title":"Identifier:","text":"<p>CHAN_NO, DISTANCE</p>"},{"location":"manual/reference/Channel_Initial_Condition/#parent-table","title":"Parent Table:","text":"<p>Table is parent</p>"},{"location":"manual/reference/Channel_Initial_Condition/#include-block","title":"Include Block:","text":"<p>INITIAL_CONDITION</p> <ul> <li>Default initial values specified in the GUI are replaced if a     restart file is used.</li> <li>Reservoir initial surfaces should be matched to the surrounding     channels. Differences imply a flow, and if you haven't accounted for     the flow in your other initial conditions you will have a flow     imbalance or even instability on the first step.</li> </ul>"},{"location":"manual/reference/Channels/","title":"Channels","text":""},{"location":"manual/reference/Channels/#overview","title":"Overview","text":"<p>Channels are the fundamental objects of the DSM2 grid. The Channels table allows you to enter channel connectivity, parameters, and geometry. Channel connectivity is defined by upstream and downstream node numbers. Two child tables describe the locations and geometry of user-defined cross-sections in the selected channel. Note that a default initial condition is required for every channel number in the DSM2 grid, entered separately in the Channel Initial Conditions table.</p>"},{"location":"manual/reference/Channels/#tables","title":"Tables","text":"<ul> <li>CHANNEL</li> <li>XSECT</li> <li>XSECT_LAYER</li> </ul>"},{"location":"manual/reference/Channels/#channel","title":"CHANNEL","text":"<p>The CHANNEL table defines the connectivity, length, friction, and dispersion characteristics of a channel.</p>"},{"location":"manual/reference/Channels/#field-descriptions","title":"Field Descriptions","text":"<ul> <li>CHAN_NO: Channel number. This is the identifier of the channel and corresponds to the number you typically see on a grid map.</li> <li>LENGTH (ft): Length of the channel reach.</li> <li>MANNING: Manning's n friction coefficient for the whole reach.</li> <li>DISPERSION: Dimensional dispersion factor.</li> <li>UPNODE: Number of the upstream node at which the channel is connected.</li> <li>DOWNNODE: Number of the downstream node at which the channel is connected.</li> </ul>"},{"location":"manual/reference/Channels/#table-info","title":"Table Info","text":"<ul> <li>Identifier: CHAN_NO</li> <li>Parent Table: Table is parent</li> <li>Include Block: GRID</li> </ul>"},{"location":"manual/reference/Channels/#xsect","title":"XSECT","text":"<p>This table lists files where bathymetric cross-sections are specified by the user using the CSDP format. The table lists the fraction of the distance along the reach (from upstream to downstream) at which the user cross-section is located. These cross-sections will be interpolated by the model at computational points.</p>"},{"location":"manual/reference/Channels/#field-descriptions_1","title":"Field Descriptions","text":"<ul> <li>CHAN_NO: Channel number where the cross-section is located.</li> <li>DIST: Fraction of distance from upstream node to downstream node where the cross-section is located.</li> <li>FILE: CSDP-formatted file where cross-section geometry is defined.</li> </ul>"},{"location":"manual/reference/Channels/#table-info_1","title":"Table Info","text":"<ul> <li>Identifier: CHAN_NO, DIST</li> <li>Parent Table: CHANNEL</li> <li>Parent Identifier: CHAN_NO</li> <li>Include Block: GRID</li> </ul>"},{"location":"manual/reference/Channels/#xsect_layer","title":"XSECT_LAYER","text":"<p>The Cross-Section Layer Table lists geometric information about each cross-section. This information is in the form of lookup tables of hydraulically important quantities such as area, width, and wetted perimeter.</p>"},{"location":"manual/reference/Channels/#field-descriptions_2","title":"Field Descriptions","text":"<ul> <li>CHAN_NO: Channel number in which the cross-section is located.</li> <li>DIST: Fraction of distance from upstream node to downstream node where the cross-section is located.</li> <li>ELEV: Elevation from the bottom at which properties are known. The area, width, etc., apply to this elevation, and channel properties between elevations are linearly interpolated.</li> <li>AREA: Area of the channel from the bottom to the cross-section (sq ft). Ignored if Area disagrees with the integral of WIDTH.</li> <li>WIDTH: Width of the channel at the top (ft).</li> <li>WET_PERIM: Wetted perimeter of the channel at the given elevation.</li> </ul>"},{"location":"manual/reference/Channels/#table-info_2","title":"Table Info","text":"<ul> <li>Identifier: CHAN_NO, DIST, ELEV</li> <li>Parent Table: CHANNEL</li> <li>Parent Identifier: CHAN_NO</li> <li>Include Block: GRID</li> </ul>"},{"location":"manual/reference/Channels/#examples","title":"Examples","text":""},{"location":"manual/reference/Channels/#channel-with-xsect_layer-cross-section","title":"CHANNEL with XSECT_LAYER cross-section","text":"<pre><code># CHANNEL SPECS\nCHANNEL\nCHAN_NO LENGTH MANNING DISPERSION UPNODE DOWNNODE\n1        15000   0.035        0.3      1        2 \n2        15000   0.035        0.3      2        3\nEND\n\n# XSECT_LAYER\nXSECT_LAYER\nCHAN_NO DIST  ELEV   AREA WIDTH WET_PERIM\n1        0.5 -24.0    0.0  40.0      40.0 \n1        0.5   0.0  960.0  80.0     91.22 \n1        0.5  20.0 2640.0 160.0     133.6 \n2        0.5 -24.0    0.0  40.0      40.0 \n2        0.5   0.0  960.0  80.0     91.22 \n2        0.5  20.0 2640.0 160.0     133.6 \nEND\n</code></pre>"},{"location":"manual/reference/Channels/#channel-with-xsect-csdp-cross-section","title":"CHANNEL with XSECT (csdp) cross-section","text":"<pre><code># CHANNEL SPECS\nCHANNEL\nCHAN_NO LENGTH MANNING DISPERSION UPNODE DOWNNODE\n1        15000   0.035        0.3      1        2 \n2        15000   0.035        0.3      2        3\nEND\n\n# XSECT\nXSECT\nCHAN_NO DIST     FILE\n1           0.5   1_0.50000.txt\n2           0.5   2_0.50000.txt\nEND\n</code></pre>"},{"location":"manual/reference/Channels/#notes","title":"Notes","text":"<ul> <li>All channels must have an initial condition and at least one cross-section.</li> <li>Avoid overspecifying cross-sections longitudinally or vertically. Ensure features are well-resolved by the model's spatial resolution.</li> </ul>"},{"location":"manual/reference/EC_based_Operating_Rule/","title":"EC-based Operating Rule","text":""},{"location":"manual/reference/EC_based_Operating_Rule/#overview","title":"Overview","text":"<p>EC-based Operating rules are user-defined rules that control gate operations based on EC results from GTM module. User need to use coupled Hydro/GTM binary(\"hydro_gtm.exe\") for EC-based operating rule.</p> <p>To run coupled HYDRO/GTM, provide the hydro input file and gtm input file as arguments. For instance, the command to run coupled Hydro/GTM with hydro input hydro.inp and gtm input gtm.inp is:</p> <pre><code>hydro_gtm.exe hydro.inp gtm.inp\n</code></pre> <p>The input file used for EC-based operating rule is the same as the ones used for general operating rules in DSM2. The use of general operating rules in DSM2 is documented in detail in Operating Rule Guide and Operating Rule.</p> <p>This document specifically describes the use of EC-based Operating Rule.</p>"},{"location":"manual/reference/EC_based_Operating_Rule/#tables-of-contents","title":"Tables of Contents:","text":"<ul> <li>EC-BASED OPERATING RULE</li> <li>EC-BASED OPRULE EXPRESSION</li> <li>EC-BASED OPRULE EXAMPLE</li> </ul>"},{"location":"manual/reference/EC_based_Operating_Rule/#ec-based-operating-rule_1","title":"EC-based operating rule","text":"<p>Defines the name, action and trigger of the EC-based operating rule in OPERATING_RULE table in the input file.</p>"},{"location":"manual/reference/EC_based_Operating_Rule/#operating_rule-table-field-descriptions","title":"OPERATING_RULE Table Field Descriptions","text":""},{"location":"manual/reference/EC_based_Operating_Rule/#name","title":"NAME","text":"<p>Same as the general operating rule. This is the identifier of the EC-based operatinf rule.</p>"},{"location":"manual/reference/EC_based_Operating_Rule/#action","title":"ACTION","text":"<p>Definition of the action to be taken when the EC-based trigger transitions from FALSE to TRUE.</p>"},{"location":"manual/reference/EC_based_Operating_Rule/#trigger","title":"TRIGGER","text":"<p>An EC-based trigger that activates the ACTION when it transitions from FALSE to TRUE. It can either be the name of the trigger variable or direct definition of the trigger.</p>"},{"location":"manual/reference/EC_based_Operating_Rule/#ec-based-oprule-expression","title":"EC-based oprule expression","text":"<p>Defines the trigger name and expression in OPRULE_EXPRESSION Table in the input file.</p>"},{"location":"manual/reference/EC_based_Operating_Rule/#oprule_expression-table-field-descriptions","title":"OPRULE_EXPRESSION Table Field Descriptions","text":""},{"location":"manual/reference/EC_based_Operating_Rule/#name_1","title":"NAME","text":"<p>Name for the trigger. It should be consistent with the name used in TRIGGER.</p>"},{"location":"manual/reference/EC_based_Operating_Rule/#definition","title":"DEFINITION","text":"<p>Definition of the expression -- this will be a formula involving model variables. In the general operating rule, time series can be used; however, EC-based operating rule can only use model varaibles.</p> <p>The trigger keyword for using EC-based operating rule is chan_ec.</p> <p>Inside of the trigger keyword, users need to specify the trigger location (i.e., the channel number and the distance of the location on the channel.)</p>"},{"location":"manual/reference/EC_based_Operating_Rule/#ec-based-oprule-example","title":"EC-based oprule example","text":"<p>The following example uses the EC-based operating rule to close the gate named montezuma_salinity_control when EC at location channel number 517 and distance 7876 ft exceeds 8500 \u00b5mho/cm.</p> <pre><code>OPERATING_RULE\nNAME                 ACTION                                                                                                    TRIGGER\nmscs_close           \"SET gate_op(gate=montezuma_salinity_control,device=radial_gates,direction=from_node) TO 0.0\"             \"ec_too_high\"\n\n\nOPRULE_EXPRESSION\nNAME                 DEFINITION\nec_too_high          \"chan_ec(channel=517, dist=7876) &gt; 8500\"\n</code></pre> <p>The operating rule has an identifier mscs_close. The trigger name is ec_too_high, which is the same in both OPERATING_RULE table and OPRULE_EXPRESSION table. When the trigger condition becomes True, the specified Action in OPERATING_RULE table will change the gate operation.</p>"},{"location":"manual/reference/ENVVARS_Section/","title":"ENVVARS Section","text":""},{"location":"manual/reference/ENVVARS_Section/#overview","title":"Overview:","text":"<p>ENVVARs\u00a0are values used in text substitution elsewhere in the input. DSM2 attempts to replace any text that is preceded by a \"$\" and wrapped in curly braces: ${EXAMPLE}. By convention, these variables are always used in upper case. The substitution will be made from either system environmental variables or pseudo-environmental variables defined in this section. For instance, the SCALAR input section might indicate that run_start_time be set to ${START_TIME}. DSM2 will then search the system environmental variables and user-specified environmental variables for START_TIME and substitute the value (or print a warning if it finds nothing).</p> <p>ENVVARs can be specified in text, or set by manipulating the command environmental variables. In production runs, many of the ENVVARs are set in a special file called the \"configuration\" file. Such a file is often included in the main input file using the CONFIGURATION include block.</p>"},{"location":"manual/reference/ENVVARS_Section/#reference","title":"Reference","text":""},{"location":"manual/reference/ENVVARS_Section/#keyword-descriptions","title":"Keyword Descriptions","text":""},{"location":"manual/reference/ENVVARS_Section/#name","title":"NAME","text":"<p>Name of the envvar. This is the alias that will be used elsewhere in the input system where the substitution is desired. For instance, if the NAME is START_TIME, ${START_TIME} would be used elsewhere.</p>"},{"location":"manual/reference/ENVVARS_Section/#value","title":"VALUE","text":"<p>Value assigned during substitution. For instance, for an ENVVAR with name START_TIME, a likely value would be\u00a0\"0000\".</p>"},{"location":"manual/reference/ENVVARS_Section/#table-info","title":"Table Info","text":""},{"location":"manual/reference/ENVVARS_Section/#identifier","title":"Identifier:","text":"<p>NAME</p>"},{"location":"manual/reference/ENVVARS_Section/#include-block","title":"Include Block:","text":"<p>CONFIGURATION</p>"},{"location":"manual/reference/ENVVARS_Section/#examples","title":"Examples:","text":"<p>Definition and use:\u00a0The following example defines an ENVVAR section and then uses the variables later in a SCALAR section.</p> <pre><code>ENVVARS        \nNAME    VALUE   \nSTART_DATE  01JAN1990   # Runtime using envvars\nEND_DATE    01JAN2001   \nSTART_TIME  0000    \nEND_TIME    0000    \nEND\n\n\nSCALAR      \nmodel_name  historical_hydro    \nrun_start_date  ${START_DATE}   \nrun_end_date    ${END_DATE} \nrun_start_time  ${START_TIME}   \nrun_end_time    ${END_TIME} \nEND\n</code></pre> <p>Identifier:Table Info</p> <p>NAME</p>"},{"location":"manual/reference/ENVVARS_Section/#parent-table","title":"Parent Table:","text":"<p>Table is parent</p>"},{"location":"manual/reference/ENVVARS_Section/#include-block_1","title":"Include Block:","text":"<p>CONFIGURATION</p> <ul> <li> <p>ENVVARs can also be used on each other -- in text input that occurs     after the ENVVAR definition.</p> <p>ENVVARS       NAME    VALUE  DSM2MODIFIER             historical_v81               #Study name used for DSM2 output </p> </li> </ul>"},{"location":"manual/reference/ENVVARS_Section/#output","title":"Output","text":"<p>OUTPUTFILE               ${DSM2MODIFIER}.dss</p>"},{"location":"manual/reference/ENVVARS_Section/#hydro","title":"hydro","text":"<p>HYDROOUTDSSFILE          ${DSM2OUTPUTDIR}/${OUTPUTFILE} END </p>"},{"location":"manual/reference/Gate/","title":"Gate","text":""},{"location":"manual/reference/Gate/#overview","title":"Overview:","text":"<p>Gates\u00a0are sites that present a barrier or control on flow. A gate may have an arbitrary number of associated hydraulic devices (pipes and weirs), each of which may be operated independently to control flow.</p> <p>The Gates View is primarily for specifying the physical properties of the gate and some simple operating modes. Gates that are operated simply can be completely specified in this table. Much more elaborate controls are possible using Gate Time Series and Operating Rules, and in addition to manipulating the hydraulic devices you can completely uninstall the gate.</p>"},{"location":"manual/reference/Gate/#tables","title":"Tables:","text":"<ul> <li>Gates</li> <li>Gate Weir Devices</li> <li>Gate Pipe Devices</li> </ul>"},{"location":"manual/reference/Gate/#gate_1","title":"GATE","text":"<p>The Gate table defines the name and connectivity of the gate. Gates are a top-level layered table.</p>"},{"location":"manual/reference/Gate/#field-descriptions","title":"Field Descriptions","text":""},{"location":"manual/reference/Gate/#name","title":"NAME","text":"<p>Name of the gate. This is the identifier of the gate used elsewhere to refer to the gate.</p>"},{"location":"manual/reference/Gate/#from_obj","title":"FROM_OBJ","text":"<p>Type (channel/reservoir) of the water body to which the gate is attached. Gates are always connected from a water body to a node. This column is a picklist that is also connected to the Name/no. column.</p>"},{"location":"manual/reference/Gate/#from_identifier","title":"FROM_IDENTIFIER","text":"<p>Identifier (channel number or reservoir name) of the water body to which the gate is attached.</p>"},{"location":"manual/reference/Gate/#to_node","title":"TO_NODE","text":"<p>Node to which gate is attached.</p>"},{"location":"manual/reference/Gate/#table-info","title":"Table Info","text":"<p>Identifier:</p>"},{"location":"manual/reference/Gate/#name_1","title":"NAME","text":"<p>Parent Table:</p>"},{"location":"manual/reference/Gate/#gate_2","title":"GATE","text":"<p>Include Block:</p>"},{"location":"manual/reference/Gate/#grid","title":"GRID","text":""},{"location":"manual/reference/Gate/#gate_weir_device","title":"GATE_WEIR_DEVICE","text":"<p>This table lists hydraulic structures that exist at the gate site to control flow that resemble weirs or rectangular conduits. In this table, the user specifies physical properties of the device as well as default operations. Both employ the following formulas depending on whether the water surface is higher on the water body or node side of the gate:</p> <p>Q = nC<sub>op_to</sub>C<sub>to</sub>A(z<sub>wb</sub>, p)\u00a0sqrt[ 2g(z<sub>wb</sub>\u00a0- z<sub>node</sub>) ] ... z<sub>wb</sub>\u00a0&gt; z<sub>node</sub></p> <p>Q = nC<sub>op_from</sub>C<sub>from</sub>A(z<sub>node</sub>, p)\u00a0sqrt[ 2g(z<sub>node</sub>\u00a0- z<sub>wb</sub>) ] ... z<sub>wb</sub>\u00a0\\&lt; z<sub>node</sub></p> <p>Where:</p> <ul> <li>n is the number of duplicate devices&gt;</li> <li>C<sub>op_to</sub>\u00a0and C<sub>op_to</sub>\u00a0are operating coefficient     representing controls such as flap gates</li> <li>C<sub>to</sub>\u00a0and C<sub>from</sub>\u00a0are coefficients representing     the hydraulic efficiency of the gate</li> <li>A is the area of flow depending on higher water surface and position     p</li> <li>g is gravity and</li> <li>z<sub>wb</sub>\u00a0and z<sub>node</sub>\u00a0are the water surface elevations     at the water body and node (node surface is assessed by means of a     reference channel that has no gates attached to it).</li> </ul> <p>Please see\u00a0usage notes\u00a0below</p>"},{"location":"manual/reference/Gate/#field-descriptions_1","title":"Field Descriptions","text":""},{"location":"manual/reference/Gate/#gate_name","title":"GATE_NAME","text":"<p>Name of the gate this device in which the device is located.</p>"},{"location":"manual/reference/Gate/#device","title":"DEVICE","text":"<p>Name of the device.</p>"},{"location":"manual/reference/Gate/#nduplicate","title":"NDUPLICATE","text":"<p>Number of exact duplicates, such as a number of similar pipes in parallel. Parameters such as width apply to a single one of the duplicates.</p>"},{"location":"manual/reference/Gate/#width","title":"WIDTH","text":"<p>Maximum width of the device (radius of a pipe, width of a weir).</p>"},{"location":"manual/reference/Gate/#elev","title":"ELEV","text":"<p>Invert elevation or weir crest.</p>"},{"location":"manual/reference/Gate/#height","title":"HEIGHT","text":"<p>Height of the device from the invert elevation. This can be used to represent the height of rectangular flashboards or of a radial gate. If the surface goes above this height, flow will be submerged. Use NA for an open top. If you click in an NA column, you will see that it is encoded using a large number, but you should only use 'NA' or a real height.</p>"},{"location":"manual/reference/Gate/#cf_from_node","title":"CF_FROM_NODE","text":"<p>Flow coefficient of the gate (0 \\&lt; Cto\u00a0\\&lt;= 1.0) describing the efficiency of the gate from node to water body. This parameter is the physical coefficient of flow. It should never be zero and should not be used to describe a control structure or operation such as flap gates or gate openings.</p>"},{"location":"manual/reference/Gate/#cf_to_node","title":"CF_TO_NODE","text":"<p>Same as CF_FROM_NODE, but for the direction from water body to node.</p>"},{"location":"manual/reference/Gate/#default_op","title":"DEFAULT_OP","text":"<p>Default operation mode. The gate operation is a \"magic\" parameter between 0.0 and 1.0 that modulates gate flow. Operating coefficients can be used to represent flap gates, fractions of duplicates operating or other physical controls. The default ops are simple on this table are like initial conditions -- if you want more sophisticated control you will need to use a Gate Time Series or Operating Rule. Nevertheless, the defaults are enough to represent structures that are fully open or closed or operated unidirectionally. Here is how the default operation mode will affect the operating coefficient:</p>"},{"location":"manual/reference/Gate/#gate_open","title":"gate_open","text":"<p>C<sub>op_to</sub>=1.0; C<sub>op_from</sub>=1.0;</p>"},{"location":"manual/reference/Gate/#gate_close","title":"gate_close","text":"<p>C<sub>op_to</sub>=0.0; C<sub>op_from</sub>=0.0;</p>"},{"location":"manual/reference/Gate/#unidir_to","title":"unidir_to","text":"<p>C<sub>op_to</sub>=1.0; C<sub>op_from</sub>=0.0;</p>"},{"location":"manual/reference/Gate/#unidir_from","title":"unidir_from","text":"<p>C<sub>op_to</sub>=0.0; C<sub>op_from</sub>=1.0;</p>"},{"location":"manual/reference/Gate/#table-info_1","title":"Table Info","text":""},{"location":"manual/reference/Gate/#identifier","title":"Identifier:","text":"<p>GATE_NAME, DEVICE</p>"},{"location":"manual/reference/Gate/#parent-table","title":"Parent Table:","text":"<p>GATE</p>"},{"location":"manual/reference/Gate/#parent-identifier","title":"Parent Identifier:","text":"<p>GATE_NAME</p>"},{"location":"manual/reference/Gate/#include-block","title":"Include Block:","text":"<p>GRID</p>"},{"location":"manual/reference/Gate/#gate_pipe_device","title":"GATE_PIPE_DEVICE","text":"<p>This table lists pipes at the gate site. In this table, the user specifies physical properties of the device as well as default operations.</p>"},{"location":"manual/reference/Gate/#field-descriptions_2","title":"Field Descriptions","text":""},{"location":"manual/reference/Gate/#gate_name_1","title":"GATE_NAME","text":"<p>Name of the gate this device in which the device is located;</p>"},{"location":"manual/reference/Gate/#device_1","title":"DEVICE","text":"<p>Name of the device.</p>"},{"location":"manual/reference/Gate/#nduplicate_1","title":"NDUPLICATE","text":"<p>Number of exact duplicates, such as a number of similar pipes in parallel. Parameters such as width apply to a single one of the duplicates.</p>"},{"location":"manual/reference/Gate/#radius","title":"RADIUS","text":"<p>Maximum width of the device (radius of a pipe, width of a weir).</p>"},{"location":"manual/reference/Gate/#elev_1","title":"ELEV","text":"<p>Invert elevation or weir crest.</p>"},{"location":"manual/reference/Gate/#cf_from_node_1","title":"CF_FROM_NODE","text":"<p>Flow coefficient of the gate (0 \\&lt; Cto\u00a0\\&lt;= 1.0) describing the efficiency of the gate from node to water body. This parameter is the physical coefficient of flow. It should never be zero and should not be used to describe a control structure or operation such as flap gates or gate openings.</p>"},{"location":"manual/reference/Gate/#cf_to_node_1","title":"CF_TO_NODE","text":"<p>Same as CF_FROM_NODE, but for the direction from water body to node.</p>"},{"location":"manual/reference/Gate/#default_op_1","title":"DEFAULT_OP","text":"<p>Default operation mode. The gate operation is a \"magic\" parameter between 0.0 and 1.0 that modulates gate flow. Operating coefficients can be used to represent flap gates, fractions of duplicates operating or other physical controls. The default ops are simple on this table are like initial conditions -- if you want more sophisticated control you will need to use a Gate Time Series or Operating Rule. Nevertheless, the defaults are enough to represent structures that are fully open or closed or operated unidirectionally. Here is how the default operation mode will affect the operating coefficient:</p>"},{"location":"manual/reference/Gate/#gate_open_1","title":"gate_open","text":"<p>C<sub>op_to</sub>=1.0; C<sub>op_from</sub>=1.0;</p>"},{"location":"manual/reference/Gate/#gate_close_1","title":"gate_close","text":"<p>C<sub>op_to</sub>=0.0; C<sub>op_from</sub>=0.0;</p>"},{"location":"manual/reference/Gate/#unidir_to_1","title":"unidir_to","text":"<p>C<sub>op_to</sub>=1.0; C<sub>op_from</sub>=0.0;</p>"},{"location":"manual/reference/Gate/#unidir_from_1","title":"unidir_from","text":"<p>C<sub>op_to</sub>=0.0; C<sub>op_from</sub>=1.0;</p>"},{"location":"manual/reference/Gate/#table-info_2","title":"Table Info","text":""},{"location":"manual/reference/Gate/#identifier_1","title":"Identifier:","text":"<p>GATE_NAME, DEVICE</p>"},{"location":"manual/reference/Gate/#parent-table_1","title":"Parent Table:","text":"<p>GATE</p>"},{"location":"manual/reference/Gate/#parent-identifier_1","title":"Parent Identifier:","text":"<p>GATE_NAME</p>"},{"location":"manual/reference/Gate/#include-block_1","title":"Include Block:","text":"<p>GRID</p>"},{"location":"manual/reference/Gate/#several-types-of-time-series-and-operational-controls-can-be-placed-on-gates","title":"Several types of time series and operational controls can be placed on gates","text":"<ul> <li> <p>At least one channel at every node must be ungated.</p> </li> <li> <p>Gates can be removed using an operation rule that sets the     gates\u00a0install variable\u00a0 to zero. Gates that are uninstalled behave     like normal nodes with equal water surface constraints between them.     Operations and time series that manipulate the device operating     coefficients and positions will be applied, but the devices will be     totally ignored in computations to determine flow. flow.</p> </li> <li> <p>Gates can be controlled by a number of variables that are     time-varying and controlled by time series or operating rules:</p> <p>install</p> <ul> <li>Install applies to the whole gate, not individual devices. When     the gate is uninstalled (install=0) the gate ceases to exist,     none of its devices are applied (although the continue to exist     in the background). The gate is totally replaced by an     equal-stage compatibility condition.</li> </ul> </li> <li> <p>op_to_node</p> <ul> <li>Operating coefficient in the direction from water body to node.</li> </ul> </li> <li> <p>op_from_node</p> <ul> <li>Operating coefficient in the direction from node to water body.</li> </ul> </li> <li> <p>op</p> <ul> <li>Operating coefficient in both directions. This is just a     convenience combo of the individual to/from node versions. It is     write-only in operating rules, because it combines two variables     and there is no single value that can be read.</li> </ul> </li> <li> <p>position</p> <ul> <li>Physical operating position whose interpretation depends on the     Position Control setting of the gate device. This is now     deprecated in favor of more direct manipulation of things like     gate elevation.</li> </ul> </li> <li> <p>elev</p> <ul> <li>Weir crest or pipe invert elevation. This can represent     evolution over time or a bottom-operating structure.</li> </ul> </li> <li> <p>width</p> <ul> <li>Weir width or pipe radius. This usually represents evolution     over time.</li> </ul> </li> <li> <p>height</p> <ul> <li>Weir gate height, width of a flashboard. This can represent     evolution over time or a top-operating structure like a radial     gate.</li> </ul> </li> </ul>"},{"location":"manual/reference/Groups/","title":"Groups","text":""},{"location":"manual/reference/Groups/#overview","title":"Overview","text":"<p>GROUPS\u00a0are user-defined groups of model objects, for instance groups of water bodies or groups of boundary inputs. Groups are used a number of places in DSM2, including: tracking of constituents originated from grouped sources, tracking of particles as they reside or move between groups of water bodies and/or boundaries, and assignment of rate coefficients in QUAL to groups of water bodies. In each context, the types of model objects that are allowed in the groups may be slightly different. That validation takes place elsewhere in the object using the group.</p>"},{"location":"manual/reference/Groups/#tables","title":"Tables","text":"<ul> <li>Groups</li> <li>Group Members</li> </ul>"},{"location":"manual/reference/Groups/#group","title":"GROUP","text":"<p>The GROUP table defines the name of a group. It has one column!!! The reason we do this is to provide a top level table for overriding and redefining groups in the layering system.</p>"},{"location":"manual/reference/Groups/#field-descriptions","title":"Field Descriptions","text":""},{"location":"manual/reference/Groups/#name","title":"NAME","text":"<p>Name of the group. This is the identifier for the group used in references elsewhere in the input system.</p>"},{"location":"manual/reference/Groups/#table-info","title":"Table Info","text":""},{"location":"manual/reference/Groups/#identifier","title":"Identifier:","text":"<p>NAME</p>"},{"location":"manual/reference/Groups/#include-block","title":"Include Block:","text":"<p>GROUPS</p>"},{"location":"manual/reference/Groups/#group_member","title":"GROUP_MEMBER","text":"<p>The Group Members Table lists members of the parent group. The group members are identified using patterns written using regular expression and a special syntax for ranges of numbers. If this sounds like nonsense -- don't worry. The examples should cover most of the important ways you would want to define group members. Note also that you can use multiple rows to define the group -- the result will be the union of the members from the individual rows.</p>"},{"location":"manual/reference/Groups/#field-descriptions_1","title":"Field Descriptions","text":""},{"location":"manual/reference/Groups/#member_type","title":"MEMBER_TYPE","text":"<p>The type (channel, etc) of model object.</p>"},{"location":"manual/reference/Groups/#identifierpattern","title":"Identifier/Pattern","text":"<p>A pattern that will be matched against the identifier of the object (channel number, input name, etc). The pattern can be a regular expression or use the special range notation.</p> <p>Here are some examples:</p> <pre><code>range:132-176\n</code></pre> <p>Matches any number in this range,inclusive</p> <pre><code>dicu_drn_.*\n</code></pre> <p>Dot-star is a wildcard matches any name that starts with dicu_drn)</p> <pre><code>mtz\n</code></pre> <p>Exact name</p> <pre><code>(183|184|185)\n</code></pre> <p>A choice of number identifiers</p> <pre><code>(mtz|sjr)\n</code></pre> <p>A choice of names.</p> <pre><code>14[2-7]\n</code></pre> <p>The regular expression way of doing ranges, which works for a single digit</p>"},{"location":"manual/reference/Groups/#table-info_1","title":"Table Info","text":""},{"location":"manual/reference/Groups/#identifier_1","title":"Identifier:","text":"<p>GROUP_NAME,MEMBER_TYPE,PATTERN</p>"},{"location":"manual/reference/Groups/#parent-table","title":"Parent Table:","text":"<p>GROUP</p>"},{"location":"manual/reference/Groups/#parent-identifier","title":"Parent Identifier:","text":"<p>GROUP_NAME</p>"},{"location":"manual/reference/Groups/#include-block_1","title":"Include Block:","text":"<p>GROUPS</p> <ul> <li>A regular expressions description can be found     in\u00a0wikipedia\u00a0and a tutorial and guide can be found     at\u00a0http://www.regular-expressions.info/. You can     probably do most of the group matching you want by modifying the     above sample patterns, but\u00a0the possibilities are endless.</li> </ul>"},{"location":"manual/reference/IO_Files/","title":"IO Files","text":""},{"location":"manual/reference/IO_Files/#overview","title":"Overview:","text":"<p>The\u00a0IO_FILES\u00a0table is where you declare most of the non-dss output from a simulation, including echoed text output files, restart files and output tidefiles from HYDRO(input tidefiles are specified for QUAL and PTM in the\u00a0TIDEFILEsection). IO_FILES can only be specified in the main text input file (hydro.inp, qual.inp, ptm.inp).</p>"},{"location":"manual/reference/IO_Files/#tables","title":"Tables:","text":""},{"location":"manual/reference/IO_Files/#io_file","title":"IO_FILE","text":""},{"location":"manual/reference/IO_Files/#keyword-descriptions","title":"Keyword Descriptions","text":""},{"location":"manual/reference/IO_Files/#model","title":"MODEL","text":"<p>Model generating the file. For a restart file this should be the model (hydro|qual) that is being restarted. For echoed output use the keyword \"output\".</p>"},{"location":"manual/reference/IO_Files/#type","title":"TYPE","text":"<p>Type of file: hdf5, restart,\u00a0 or \"none\" for echoed output.</p>"},{"location":"manual/reference/IO_Files/#io","title":"IO","text":"<p>Type of file \"in\" \"out\" or \"none\" for echoed output.</p>"},{"location":"manual/reference/IO_Files/#interval","title":"INTERVAL","text":"<p>Interval, for hdf5 tidefile output.</p>"},{"location":"manual/reference/IO_Files/#filename","title":"FILENAME","text":"<p>Name of file. Should have a suitable extension:\u00a0*.hrf\u00a0for hydro restart file,\u00a0*.qrffor qual restart file,\u00a0*.h5\u00a0for hdf5 tidefile or\u00a0*.out\u00a0for echoed output.</p>"},{"location":"manual/reference/IO_Files/#table-info","title":"Table Info","text":""},{"location":"manual/reference/IO_Files/#identifier","title":"Identifier:","text":"<p>none: no layering</p>"},{"location":"manual/reference/IO_Files/#include-block","title":"Include Block:","text":"<p>none: launch file only</p>"},{"location":"manual/reference/IO_Files/#examples","title":"Examples:","text":""},{"location":"manual/reference/IO_Files/#hydro-example","title":"HYDRO example:","text":"<p>This example includes standard hydro runtime output file, a restart output file that is regenerated every model day (overwriting the previous day's file), an hdf5 tidefile for passing information to QUAL and an echo file (replicate of input). All of the file names use text substitution -- the value would come from an environmental variable, ENVVARS section in the input file or ENVVARS section of a config file.</p> <pre><code>IO_FILES      \nMODEL  TYPE     IO    INTERVAL FILENAME  \noutput none     none  none     ${HYDROOUTFILE}  \nhydro  restart  out   1day     ${QUALRSTFILE}  \nhydro  hdf5     out   1hour    ${HYDROHDF5FILE}  \nhydro  echo     out   none     ${DSM2MODIFIER}_hydro_echo.inp  \nEND\n</code></pre>"},{"location":"manual/reference/IO_Files/#qual-example","title":"QUAL example:","text":"<p>This example includes a general qual runtime output file, a restart output file that is regenerated every model day (overwriting the previous day's file), a restart file that will be used to generate the initial condition for the run, and an hdf5 tidefile for passing information to QUAL and an echo file (exact replicate of input).</p> <pre><code>IO_FILES      \nMODEL  TYPE    IO   INTERVAL FILENAME  \noutput none    none none     ${QUALOUTFILE}  \nqual   restart out  1day     ${QUALRESTART}  \nqual   restart in   none     qualinit_30SEP1999.qrf  \nqual   hdf5    out  1hour    ${QUALHDF5FILE} \nqual   echo    out  none     ${DSM2MODIFIER}_qual_echo.inp  \nEND\n</code></pre>"},{"location":"manual/reference/IO_Files/#ptm-example","title":"PTM example:","text":"<p>This example includes a PTM trace file (which is required to produce flux DSS output) and an animation file (which is required for animated output).</p> <pre><code>IO_FILES      \nMODEL TYPE  IO  INTERVAL FILENAME  \nptm   trace out none     ${DSM2OUTPUTDIR}/trace.out  \nptm   anim  out 15min    ${DSM2OUTPUTDIR}/anim.out\nptm   echo  out none     ${DSM2MODIFIER}_ptm_echo.inp  \nEND\n</code></pre> <p>The runtime output file from HYDRO is used in the preparation of PTM visualization tools.</p>"},{"location":"manual/reference/Input_Climate/","title":"Input Climate","text":""},{"location":"manual/reference/Input_Climate/#overview","title":"Overview:","text":"<p>Climate inputs are time series assignments to climate variables used in non-conservative constituent runs.</p>"},{"location":"manual/reference/Input_Climate/#tables","title":"Tables:","text":"<ul> <li> <ul> <li>INPUT_CLIMATE</li> </ul> </li> </ul>"},{"location":"manual/reference/Input_Climate/#input_climate","title":"INPUT_CLIMATE","text":"<p>Climate input assigns time varying properties to gate\u00a0parameters, The table assigns a time series data source.</p>"},{"location":"manual/reference/Input_Climate/#field-descriptions","title":"Field Descriptions","text":""},{"location":"manual/reference/Input_Climate/#name","title":"NAME","text":"<p>Name of the input, used for layering.</p>"},{"location":"manual/reference/Input_Climate/#variable","title":"VARIABLE","text":"<p>The variable that is set by this assignment.</p>"},{"location":"manual/reference/Input_Climate/#fillin","title":"FILLIN","text":"<p>Method for filling in data if the time step of the assigned series is coarser than the time step of the model. See\u00a0fillin types</p>"},{"location":"manual/reference/Input_Climate/#file","title":"FILE","text":"<p>DSS or text file in which data are stored. Use consistent case when referring to the same file. You may also enter the word\u00a0constant\u00a0if you would like to assign a constant value to the input (the value will be entered in the next column).</p> <p>PATHThe path within the text or DSS file of the time series data. If you used the\u00a0constant\u00a0keyword in the Input File column, enter the value (e.g.\u00a04.22) here.</p>"},{"location":"manual/reference/Input_Climate/#table-info","title":"Table Info","text":""},{"location":"manual/reference/Input_Climate/#identifier","title":"Identifier:","text":"<p>NAME</p>"},{"location":"manual/reference/Input_Climate/#include-block","title":"Include Block:","text":"<p>QUAL_TIME_SERIES</p>"},{"location":"manual/reference/Input_Gate/","title":"Input Gate","text":""},{"location":"manual/reference/Input_Gate/#overview","title":"Overview","text":"<p>Gate inputs are time series assignments to gate structure physical and operational parameters.</p>"},{"location":"manual/reference/Input_Gate/#tables","title":"Tables","text":"<ul> <li>INPUT_GATE</li> </ul>"},{"location":"manual/reference/Input_Gate/#input_gate","title":"INPUT_GATE","text":"<p>A gate input assigns time-varying properties to gate parameters. The table assigns a time series data source.</p> <p>Gate paths in DSS should be of data type INST-VAL as opposed to PER-AVER (which provide for better viewing in HECDSSVue); otherwise, it is possible that the gate does not operate as expected.</p>"},{"location":"manual/reference/Input_Gate/#field-descriptions","title":"Field Descriptions","text":""},{"location":"manual/reference/Input_Gate/#gate_name","title":"GATE_NAME","text":"<p>This must be the same as the name of the gate.</p>"},{"location":"manual/reference/Input_Gate/#device","title":"DEVICE","text":"<p>This must be the same as the name of the gate device. Generally, all the variables except \"install\" are device-specific. If the variable is \"install,\" set the device to \"none.\"</p>"},{"location":"manual/reference/Input_Gate/#variable","title":"VARIABLE","text":"<p>The variable that is set by this assignment.</p>"},{"location":"manual/reference/Input_Gate/#fillin","title":"FILLIN","text":"<p>Method for filling in data if the time step of the assigned series is coarser than the time step of the model. See fillin types.</p>"},{"location":"manual/reference/Input_Gate/#file","title":"FILE","text":"<p>DSS or text file in which data are stored. Use consistent case when referring to the same file. You may also enter the word \"constant\" if you would like to assign a constant value to the input (the value will be entered in the next column).</p>"},{"location":"manual/reference/Input_Gate/#path","title":"PATH","text":"<p>The path within the text or DSS file of the time series data. If you used the \"constant\" keyword in the Input File column, enter the value (e.g., 4.22) here.</p>"},{"location":"manual/reference/Input_Gate/#table-info","title":"Table Info","text":""},{"location":"manual/reference/Input_Gate/#identifier","title":"Identifier","text":"<p>NAME</p>"},{"location":"manual/reference/Input_Gate/#include-block","title":"Include Block","text":"<p>HYDRO_TIME_SERIES</p>"},{"location":"manual/reference/Input_Transfer_Flow/","title":"Input Transfer Flow","text":""},{"location":"manual/reference/Input_Transfer_Flow/#overview","title":"Overview:","text":"<p>Transfer Flows are flow time series assignments to pre-defined mass transfers.</p>"},{"location":"manual/reference/Input_Transfer_Flow/#tables","title":"Tables:","text":""},{"location":"manual/reference/Input_Transfer_Flow/#input_transfer_flow","title":"INPUT_TRANSFER_FLOW","text":"<p>The transfer flow table assigns time series flows to\u00a0transfers, The table assigns a time series data source to the boundary condition.</p>"},{"location":"manual/reference/Input_Transfer_Flow/#field-descriptions","title":"Field Descriptions","text":""},{"location":"manual/reference/Input_Transfer_Flow/#transfer_name","title":"TRANSFER_NAME","text":"<p>This must be the same as the name of the transfer.</p>"},{"location":"manual/reference/Input_Transfer_Flow/#fillin","title":"FILLIN","text":"<p>Method for filling in data if the time step of the assigned series is coarser than the time step of the model. See\u00a0fillin types</p>"},{"location":"manual/reference/Input_Transfer_Flow/#file","title":"FILE","text":"<p>DSS or text file in which data are stored. Use consistent case when referring to the same file. You may also enter the word\u00a0constant\u00a0if you would like to assign a constant value to the input (the value will be entered in the next column).</p>"},{"location":"manual/reference/Input_Transfer_Flow/#path","title":"PATH","text":"<p>The path within the text or DSS file of the time series data. If you used the\u00a0constant\u00a0keyword in the Input File column, enter the value (e.g.\u00a04.22) here.</p>"},{"location":"manual/reference/Input_Transfer_Flow/#table-info","title":"Table Info","text":""},{"location":"manual/reference/Input_Transfer_Flow/#identifier","title":"Identifier:","text":"<p>NAME</p>"},{"location":"manual/reference/Input_Transfer_Flow/#include-block","title":"Include Block:","text":"<p>HYDRO_TIME_SERIES</p> <p>Only one flow (and no concentration) can be assigned to a transfer.</p>"},{"location":"manual/reference/Layers/","title":"Layers","text":""},{"location":"manual/reference/Layers/#overview","title":"Overview:","text":"<p>DSM2 batches input data into files or \"layers\" in order to achieve the following goals:</p> <ul> <li>To group input into cohesive packages with similar content(examples:     the standard grid, sdip operating rules).</li> <li>To identify which items are changed when a new group of inputs is     added to an existing simulation.</li> </ul> <p>For example, consider the two layers of channels in the figure below. The first layer defines seven channels and would have seven entries in the CHANNEL table. This might represent a \"base\" grid. The second layer changes the properties of Channel 2, adds a Channel 8 and removes Channel 7. The second layer will have only three entries, shown in red. These entries represent the changes relative to Layer 1, and presumably are thematically related.</p> <p></p>"},{"location":"manual/reference/Layers/#example-channel","title":"Example: Channel","text":"<p>Consider the above example using text input. We are going to create CHANNEL\u00a0tables representing the channel connectivity, and assume the geometry is provided with CSDP style cross-sections listed in an\u00a0XSECT\u00a0table (child items are always associated with parent items in the same file).</p> <p>The base data will be in a file\u00a0channel_base.inp:</p> <p>channel_base.inp</p> <pre><code>CHANNEL\nCHAN_NO LENGTH MANNING DISPERSION UP_NODE DOWN_NODE\n1        18000   0.030       0.80       1         2\n2         8000   0.040       0.80       2         3\n3        18000   0.040       0.80       3         4\n4        18000   0.040       0.80       4         5\n5        18000   0.040       0.80       3         5\n6        22000   0.040       0.80       5         6\n7        14000   0.040       0.80       6         7\nEND\n\nXSECT\nCHAN_NO   DISTANCE    FILE\n1         0.200       1_0_200.txt\n1         0.800       1_0_800.txt\n2         0.500       2_0_500.txt\n...\n7         0.900       7_0_900.txt\nEND\n</code></pre> <p>The revisions are in\u00a0channel_revise.inp:</p> <p>channel_revise.inp</p> <pre><code>CHANNEL\nCHAN_NO LENGTH MANNING DISPERSION UP_NODE DOWN_NODE\n2         8000   0.030       0.80       2         3 # Masks + Alters\n#3        9000   0.000       0.00      19        20 # Has no effect\n^7       14000   0.040       0.80       6         7 # Masks + Deletes\n...\n8        16000   0.040       0.80       8         3 # Adds\nEND\n\nXSECT\nCHAN_NO DISTANCE  FILE\n2          0.100  2_0_500.txt  # Masks lower level x-sects \n2          0.700  2_0_500.txt  #\n7          0.900  7_0_900.txt  # Will be ignored\n8          0.500  8_0_500.txt  # \nEND\n</code></pre> <p>The two layers are managed by the model input file that is given directly to the model, in this case\u00a0hydro_layering.inp. The two channel files are listed in a GRID include block that lists the layers in increasing priority.</p> <p>hydro.inp</p> <pre><code>GRID\nchannel_base.inp\nchannel_revise.inp\nEND\n</code></pre> <p>Now lets consider the details...</p>"},{"location":"manual/reference/Layers/#include-blocks","title":"Include Blocks","text":"<p>Include blocks are input blocks in the master file that list other files. The data from these other files is \"included\" in the order listed. Priority is given to files read later, and these are assigned a higher \"layer number\"</p> <p>Include blocks can only contain specific types of input data. For instance, a GRID input block only contains channel, gate, reservoir and transfer physical specifications (not boundary conditions attached to them). So the trick to using include blocks is knowing, say, that a CHANNEL table belongs in a file in a GRID include block and BOUNDARY_FLOW table belongs in a file in a HYDRO_TIME_SERIES block. In the reference documentation, the include blocks should be listed for each table in the\u00a0Table Information\u00a0section..</p> <p>The only exception is the master file that is the one sent to the model on the command line (often named something like hydro.inp, qual.inp, ptm.inp). Data in this file always take precedence over other input.</p>"},{"location":"manual/reference/Layers/#layer-overriding","title":"Layer Overriding","text":"<p>Layer overriding occurs when the same data item is defined in multiple layers (files) in the same model. Files that are read later are given a higher \"layer number\" and take precendence over files read earlier. Within the same file it is an error to redefine an entry.</p>"},{"location":"manual/reference/Layers/#identifiers","title":"Identifiers","text":"<p>To use layering, you have to know what constitutes redefining an entry. Whether two items represent the same data item depends on the identifier for the table, which is some combination of columns that uniquely identify the item using a name or number. Identifiers for each table are listed in the reference documents. In the above example it is channel number CHAN_NO. The trickiest identifiers are in the output, because they involve two (NAME, VARIABLE) or three (NAME, VARIABLE,SOURCE_NAME) columns. In the reference documentation, the identifier is listed for each table in the\u00a0Table Information\u00a0section.</p>"},{"location":"manual/reference/Layers/#parent-child-tables","title":"Parent-child Tables","text":"<p>When parent-child tables are present in a file (e.g., Channels, Cross Section, Cross Section Layer), overriding is assessed at the level of the parent or\u00a0top-level table. When you override on a top-level table, its child table information is completely replaced as well. So, for instance, the cross-section at Channel 2 Distance 0.500 in channel_base.inp in the example is completely ignored. The model makes no attempt to \"mix it in\" with the replacement version of Channel 2.</p> <p>Child tables must be in the same file as their parent tables. This is a departure from earlier versions of DSM2, but is necessary to make layering well-defined.</p>"},{"location":"manual/reference/Layers/#deleting-lower-level-data","title":"Deleting lower level data","text":"<p>Occasionally, the motivation for overriding an item is to eliminate it. You can do this on any top-level table by prepending a carat ^ at the beginning of the line. This will remove items on lower levels with the same identifier. Note that it doesn't matter what other data you put in the parent fields (you do need placeholders). Also you needn't add child information if the only reason for the parent entry is to delete it -- but sometimes it is nice to have the child data there if you are toggling back and forth.</p> <p>Deleting data is quite different from commenting it out (using a # sign). Commenting data out on a high level would merely mean that the input reader would skip over the line. It would not affect any data with the same identifier on a lower level.</p>"},{"location":"manual/reference/Manual/","title":"Contents","text":"<ul> <li>Overview</li> <li>Getting Started\u00a0<ul> <li>Download and     installation</li> <li>Recommended third party     extras</li> <li>Test Launching     DSM2</li> </ul> </li> <li>Layers, Priority, Data Management</li> <li>Operating Rules</li> <li>Reference</li> </ul>"},{"location":"manual/reference/Node_Concentration/","title":"Node Concentration","text":""},{"location":"manual/reference/Node_Concentration/#overview","title":"Overview:","text":"<p>Node concentration represents the concentration of constituents attached to boundary inflows and sources.</p>"},{"location":"manual/reference/Node_Concentration/#tables","title":"Tables:","text":""},{"location":"manual/reference/Node_Concentration/#node_concentration","title":"NODE_CONCENTRATION","text":"<p>The NODE_CONCENTRATION table attaches concentrations to boundary and source flows defined in HYDRO. The table also assigns a time series to the source.</p>"},{"location":"manual/reference/Node_Concentration/#field-descriptions","title":"Field Descriptions","text":""},{"location":"manual/reference/Node_Concentration/#name","title":"NAME","text":"<p>Name assigned to the source. An entry here must have the same name as an entry in the BOUNDARY_STAGE, BOUNDARY_FLOW or SOURCE_FLOW tables -- by matching names you will attach concentrations to the flow.</p>"},{"location":"manual/reference/Node_Concentration/#node_no","title":"NODE_NO","text":"<p>Node number where the flow is applied. This must match the node number given in the original flow table (it is a bit redundant, but easier to look things up).</p>"},{"location":"manual/reference/Node_Concentration/#variable","title":"VARIABLE","text":"<p>Constituent name. If no output is requested for the constituent currently it will be ignored.</p>"},{"location":"manual/reference/Node_Concentration/#fillin","title":"FILLIN","text":"<p>Method for filling in data if the time step of the assigned series is coarser than the time step of the model.</p>"},{"location":"manual/reference/Node_Concentration/#file","title":"FILE","text":"<p>DSS or text file in which data are stored. Use consistent case when referring to the same file. You may also enter the word\u00a0constant\u00a0if you would like to assign a constant value to the input (the value will be entered in the next column).</p>"},{"location":"manual/reference/Node_Concentration/#path","title":"PATH","text":"<p>The path within the text or DSS file of the time series data. If you used the\u00a0constant\u00a0keyword in the Input File column, enter the value (e.g.\u00a04.22) here.</p>"},{"location":"manual/reference/Node_Concentration/#table-info","title":"Table Info","text":""},{"location":"manual/reference/Node_Concentration/#identifier","title":"Identifier:","text":"<p>NAME</p>"},{"location":"manual/reference/Node_Concentration/#include-block","title":"Include Block:","text":"<p>QUAL_TIME_SERIES</p> <p>Multiple sources and sinks can be assigned to a node. They are often kept separate in order to assign different concentrations to them</p>"},{"location":"manual/reference/Operating_Rule/","title":"Operating Rule","text":""},{"location":"manual/reference/Operating_Rule/#overview","title":"Overview","text":"<p>Operating rules are user-written rules that manipulate model inputs such as gate operations, boundary flows based on observations of the current state of the running model. Operating rules are documented in detail in the DSM2 Op Rule Guide. The Operating Rules table lists the time series, expressions, and rule definitions.</p>"},{"location":"manual/reference/Operating_Rule/#tables","title":"Tables","text":"<ul> <li>OPERATING RULE</li> <li>OPRULE TIME SERIES</li> <li>OPRULE EXPRESSION</li> </ul>"},{"location":"manual/reference/Operating_Rule/#operating-rule_1","title":"OPERATING RULE","text":"<p>Defines the name, action, and trigger of the operating rule.</p>"},{"location":"manual/reference/Operating_Rule/#field-descriptions","title":"Field Descriptions","text":""},{"location":"manual/reference/Operating_Rule/#name","title":"NAME","text":"<p>Name of the operating rule. This is the identifier of the rule.</p>"},{"location":"manual/reference/Operating_Rule/#action","title":"ACTION","text":"<p>Definition of the action to be taken when the trigger transitions from FALSE to TRUE.</p>"},{"location":"manual/reference/Operating_Rule/#trigger","title":"TRIGGER","text":"<p>Trigger that activates the rule when it transitions from FALSE to TRUE. If the trigger is NULL, it will become the trivial TRUE trigger, which is assumed to make a transition from FALSE to TRUE at startup (it is not \"always\" active).</p>"},{"location":"manual/reference/Operating_Rule/#table-info","title":"Table Info","text":""},{"location":"manual/reference/Operating_Rule/#identifier","title":"Identifier","text":"<p>NAME</p>"},{"location":"manual/reference/Operating_Rule/#parent-table","title":"Parent Table","text":"<p>Table is parent</p>"},{"location":"manual/reference/Operating_Rule/#include-block","title":"Include Block","text":"<p>OPERATIONS</p>"},{"location":"manual/reference/Operating_Rule/#oprule-time-series","title":"OPRULE TIME SERIES","text":"<p>This table lists time series that are used in forming action and trigger definitions. The table is not a child table -- it is a top-level layered table.</p>"},{"location":"manual/reference/Operating_Rule/#field-descriptions_1","title":"Field Descriptions","text":""},{"location":"manual/reference/Operating_Rule/#name_1","title":"NAME","text":"<p>Name assigned to the time series. This is the identifier of the series. It is also the name used to refer to the series in expressions.</p>"},{"location":"manual/reference/Operating_Rule/#fillin","title":"FILLIN","text":"<p>Method used to interpolate when the model time step is finer than the time series time step. Use \"last\" to use the last time stamp in the period (a HEC-DSS convention) and \"linear\" to interpolate linearly.</p>"},{"location":"manual/reference/Operating_Rule/#file","title":"FILE","text":"<p>Input file (HEC-DSS or text file in HEC-DSS format) storing the time series or the word \"constant\" if the series is assigned a fixed value.</p>"},{"location":"manual/reference/Operating_Rule/#path","title":"PATH","text":"<p>HEC-DSS path of the data within the Input File or the value (e.g., 2.0) if the series is assigned a fixed value.</p>"},{"location":"manual/reference/Operating_Rule/#table-info_1","title":"Table Info","text":""},{"location":"manual/reference/Operating_Rule/#identifier_1","title":"Identifier","text":"<p>NAME</p>"},{"location":"manual/reference/Operating_Rule/#parent-table_1","title":"Parent Table","text":"<p>Table is parent</p>"},{"location":"manual/reference/Operating_Rule/#include-block_1","title":"Include Block","text":"<p>OPERATIONS</p>"},{"location":"manual/reference/Operating_Rule/#oprule-expression","title":"OPRULE EXPRESSION","text":"<p>This table allows the user to list expressions that can be reused later in operating rule actions and triggers. Expressions cannot depend on other expressions. Expressions are not a child table -- the table is a top-level layered table.</p>"},{"location":"manual/reference/Operating_Rule/#field-descriptions_2","title":"Field Descriptions","text":""},{"location":"manual/reference/Operating_Rule/#name_2","title":"NAME","text":"<p>Name of the expression. This is the identifier of the expression. It is also the name used to refer to the expression in expressions.</p>"},{"location":"manual/reference/Operating_Rule/#definition","title":"DEFINITION","text":"<p>Definition of the expression -- this will be a formula involving model variables, seasons, and time series. The time series can be from the above time series table or elsewhere in the Input Time Series section. Please see the Operating Rules Guide for more details.</p>"},{"location":"manual/reference/Operating_Rule/#table-info_2","title":"Table Info","text":""},{"location":"manual/reference/Operating_Rule/#identifier_2","title":"Identifier","text":"<p>NAME</p>"},{"location":"manual/reference/Operating_Rule/#parent-table_2","title":"Parent Table","text":"<p>Table is parent</p>"},{"location":"manual/reference/Operating_Rule/#include-block_2","title":"Include Block","text":"<p>OPERATIONS</p> <ul> <li>Numerous usage comments in the Operating Rules Guide.</li> <li>Time series referenced in the operating rules may be defined in an OPRULE TIME SERIES table or they may be time series defined elsewhere, such as the name of a boundary flow.</li> <li>Neither the OPRULE EXPRESSION nor OPRULE TIME SERIES table is a child table of OPERATING RULE. However, it is common to put related items in the same file.</li> </ul>"},{"location":"manual/reference/Operating_Rule_Guide/","title":"Operating Rule Guide","text":""},{"location":"manual/reference/Operating_Rule_Guide/#operating-rule-guide_1","title":"Operating Rule Guide","text":""},{"location":"manual/reference/Operating_Rule_Guide/#introduction","title":"Introduction","text":"<p>DSM2 uses a text language for operating rules, and the rules are stored in the database. Operating rules combine trigger and action directives, each of which is an expression based on observed model states, seasonal information and exogenous time series input as well as other expressions.</p> <p>Actions are things the operating rule does. In DSM2-DB, the actions affect either gate devices or source/sink flow boundaries. For gate devices the operating flow coefficient can be changed. For sources and sinks, flow may be set to a new constant value or a new time series. Expressions for actions tend to be of the form:</p> <pre><code>SET model_object TO numerical_expression\n</code></pre> <p>The action becomes applicable when a corresponding trigger goes from false to true. Triggers are written with expressions that evaluate true or false:</p> <pre><code>chan_stage(channel=132, dist=1000) &lt; -0.1\n</code></pre> <p>Some rules are immediate responses to model conditions (close the gate when stage dips below 0.5). Other rules use triggers to describe seasons or situations where the action is applicable (reduce a boundary flow when the month is between May and September). Still others rules apply from the beginning of the run and the trigger column is just a nuisance \u2013.</p>"},{"location":"manual/reference/Operating_Rule_Guide/#expressions","title":"Expressions","text":"<p>An expression is just a named quantity that is derived from model data, outside time series data, math and time functions. An example of a simple numerical expression based on current DSM2-DB flow looks like this:</p> <pre><code>ebb := chan_flow(channel=132, dist=1000) &gt; 0.01\n</code></pre> <p>This example samples the current time step model flow 1,000 ft downstream of the upstream node in channel 132 and checks whether it is greater than 0.01 cfs. The expression assigns the answer the name ebb, so it can be reused in later expressions. Note that ebb is a logical expression which evaluates to true or false depending on the model time step. Numerical expressions will be introduced shortly.</p> <p>Assignments of named expressions always start with a name the assignment operator \u201c:=\u201d. Spaces around the assignment and greater-than operators are optional. The assignment operator isn\u2019t actually used in the GUI, because there is a separate column for the name and definition.</p> <p>The chan_flow part of the expression represents the value of a model variable. Model variables typically require identifiers, which are included in parenthesis and are a comma-separated list with elements that depend on the context (see the section below on DSM2 model variable identifiers). These identifiers can be numerical or text strings:</p> <pre><code>chan_flow(channel=132, dist=1000) ...numerical\n\ngate_op(gate=middle_river_barrier, device=weir) ...strings\n</code></pre> <p>The examples thus far have been logical expressions. Logical expressions usually appear in triggers rather than actions. Besides logical expressions, expressions that evaluate to numerical values can be defined:</p> <pre><code>ebbmagnitude := log(chan_flow(channel=132, dist=1000))\n</code></pre> <p>and expressions can also involve simple math operators. For instance:</p> <pre><code>ebbmagnitude := log(chan_flow(channel=132, dist=1000))\n</code></pre> <p>is an expression that evaluates flow, applies the log function to it and then assigns it to the variable name ebbmagnitude. For details, see the section below on Math Operators)</p> <p>Model time can also be used in expressions. The following expression describes the VAMP season for San Joaquin river management:</p> <pre><code>vamp := (MONTH == APR) or (MONTH == MAY)\n</code></pre> <p>The definition could also include the date, day of the month, or time of day.</p> <p>month, or time of day. Finally, the following example combines a model state (stage/water surface) observation, an external time series (called tide_level) and some simple arithmetic. The expression might be used with a slowly fluctuating tide or sea level datum to provide an idea of critical stage in the South Delta compared to ambient tide conditions.</p> <pre><code>critical_stage := chan_stage(channel=132,dist=1000)&lt;(tide_level-1.0)\n</code></pre>"},{"location":"manual/reference/Operating_Rule_Guide/#operating-rules","title":"Operating Rules","text":"<p>It is now straightforward to use expressions in operating rules. The following example is based on expressions that were developed above. Bold face words correspond to tables or columns of the GUI.</p>"},{"location":"manual/reference/Operating_Rule_Guide/#name","title":"Name","text":"<pre><code>middle_vamp_ebb\n</code></pre>"},{"location":"manual/reference/Operating_Rule_Guide/#expressions_1","title":"Expressions","text":"<pre><code>ebb := chan_flow(channel=132, dist=1000) &gt; 0.01\nvamp := (month == Apr) OR (month == May)\n</code></pre>"},{"location":"manual/reference/Operating_Rule_Guide/#trigger","title":"Trigger","text":"<pre><code>vamp AND ebb\n</code></pre>"},{"location":"manual/reference/Operating_Rule_Guide/#action","title":"Action","text":"<pre><code>SET gate_op(gate=middle_river_barrier, device=weir) TO ts(new_time_series)\n</code></pre> <p>The middle_vamp_ebb operating rule lies dormant until the first time step when vamp and ebb (a compound expression based on the expressions vamp and ebb) becomes true. At that point the action will be taken and the weir operating coefficient will start to operate according to the values in the DSS time series new_time_series. Note that except for the expression definitions, the parts of this operating rule can be united using the name assignment (:=) and WHERE directives:</p> <pre><code>middle_vamp_ebb := SET gate_op(gate= middle_river_barrier,device = weir) TO ts(new_time_series) WHERE (vamp AND ebb)\n</code></pre> <p>This is the form of the operating rule that would be used, say, when parsing a text file rather than using the GUI.</p>"},{"location":"manual/reference/Operating_Rule_Guide/#prediction","title":"Prediction","text":"<p>Anticipation using linear or quadratic extrapolation can be added to numerical expressions in expressions using the PREDICT function. What is nice about PREDICT is that it allows trigger expressions to more accurately express the intent of a rule, because you don't need \"buffers\" which are confusing and inaccurate.</p> <p>For instance lets say you want to take some action like close a gate to protect stage in channel 206 in the South Delta from going below zero. If you use a buffer, you write the following:</p> <pre><code>SET [some action] WHEN chan_stage(chan=206, dist=0) &lt;1);\n</code></pre> <p>This is confusing because the value \"1\" is used as the trigger criterion when the intent has to do with stage of 0 and not 1. It is inaccurate because it will go off no matter what the trend is. With anticipation, the same rule would look like this:</p> <pre><code>SET [some action] WHEN PREDICT(chan_stage(chan=206, dist=0),LINEAR, 30MIN) &lt; 0;\n</code></pre> <p>This states the trigger clearly in terms of the value 0. It is also much less likely to go off by accident, because the time trend is used (stage going below 1 is not significant if it is dropping very slowly and not likely to make it to 0). In addition to LINEAR extrapolation quadratic predictions are available using QUAD as the second argument to PREDICT. Over time periods of less than an hour (and not right next to a gate or reservoir), quadratic interpolation is markedly more accurate than linear.</p>"},{"location":"manual/reference/Operating_Rule_Guide/#ramp-transition","title":"RAMP (transition)","text":"<p>For actions, there is also a way to smooth time. The keyword RAMP after an action (together with a number of minutes) will transition in the action gradually, if such a transition makes physical sense.</p> <p>For instance, a ramping version of middle_vamp_ebb might use the definition for ebb:</p> <pre><code>SET gate_op( gate=middle_r_barrier, device=radial) TO ts(new_time_series) RAMP 60min\n</code></pre>"},{"location":"manual/reference/Operating_Rule_Guide/#complementary-triggers-and-ifelse","title":"Complementary Triggers and IFELSE","text":"<p>Often, an operating rule is paired with a complimentary rule that will reverse its action. For instance, to complement the above rule for ebb flow the following operating rule for flood flow might be added:</p>"},{"location":"manual/reference/Operating_Rule_Guide/#name_1","title":"Name","text":"<p>middle_vamp_flood</p>"},{"location":"manual/reference/Operating_Rule_Guide/#expressions_2","title":"Expressions","text":"<pre><code>flood := chan_flow(channel=132, dist=1000) &lt; -0.01\nvamp := (month == Apr) or (month == May)\n</code></pre>"},{"location":"manual/reference/Operating_Rule_Guide/#trigger_1","title":"Trigger","text":"<pre><code>vamp and flood\n</code></pre>"},{"location":"manual/reference/Operating_Rule_Guide/#action_1","title":"Action","text":"<pre><code>SET gate_op( gate=middle_r_barrier, device=barrier,direction=to_node) TO old_time_series\n</code></pre> <p>This rule effectively undoes the ebb action. The example underscores a necessary but somewhat unintuitive point about triggers: they are one-time and unidirectional. A rule whose trigger is vamp and ebb will activate when this expression changes from false to true but will not do anything or even notice if vamp and ebb subsequently becomes false again. If the complementary behavior is desired, this intent must be specified in a second rule. Often the complementary rule is subtly different from the exact negation of the original; for instance, the trigger vamp and flood is not the same as not(vamp and ebb). In the case of the Montezuma Salinity Control Structure, the flood and ebb triggers are not even based on the same variable (the gate is opened based on a head difference, closed based on velocity).</p> <p>The middle_vamp_ebb example combines vamp, which is the seasonal applicability of the rule with ebb, which is a tidal phenomenon. There are also meaningful operating rules that do not need a trigger at all. For instance, the user might want to operate SWP and CVP pumping based on a time series but bound it by some fraction of Sacramento inflow. The trigger in this case is \u201cTRUE\u201d and it will go off once at startup. This is the default in the GUI if you leave the trigger blank.</p> <p>If what you really want is a trigger that continuously monitors a true-false condition and applies a value accordingly, you may want to consider using the IFELSE function and no trigger. For instance:</p> <pre><code>SET ext_flow(node=17) TO IFELSE( vamp, ts1, ts2)\n</code></pre> <p>will set the boundary flow at node 17 (San Joaquin River) to time series ts1 whenever vamp is true and to ts2 when vamp is not true.</p>"},{"location":"manual/reference/Operating_Rule_Guide/#misfires-and-redundant-triggering","title":"Misfires and Redundant Triggering","text":"<p>Extra triggering and rule activation may seem harmless when you consider one rule in isolation. Rerunning an action hurts performance, but the action is redundant rather than harmful. The real problem with rules that misfire is that they are active too often and tend to interfere with (\u201clock out\u201d or \u201cbump\u201d) other rules that are trying to manipulate the same model variable.</p> <p>Here is an example of misfiring trigger based on an expression using date terms:</p> <pre><code>(YEAR &gt;= 1990 AND MONTH&gt;=APR AND DAY&gt;=14)\n</code></pre> <p>(note: a much better way to write this expression using the DATE keyword is given in the reference section)</p> <p>Because of the ANDs, this expression requires three conditions to be true at once in order to evaluate to TRUE. It goes off as intended on or about 14APR1990. But what happens on 01MAY1990? On 14MAY1990? This trigger is going to evaluate to FALSE and then back to TRUE. When it makes the FALSE-TRUE transition it will cause the trigger to go off, which is probably not what was intended.</p> <p>There is a fix for the above expression (not the recommended on) that illustrates that the only thing that matter are FALSE-TO-TRUE transitions. There is one more curious point about this example is that the correct behavior is obtained using:</p> <pre><code>(YEAR == 1990 AND MONTH == APR AND DAY &gt;= 14)\n</code></pre> <p>Why? The rule will evaluate FALSE on or about 01MAY1990, but it will stay false!</p> <p>These date examples are so common that there is a special way of dealing with them. See the function reference for DATE and SEASON.</p>"},{"location":"manual/reference/Operating_Rule_Guide/#default-true-trigger","title":"Default (TRUE) Trigger","text":"<p>If you leave the trigger definition blank in the GUI the trigger expression will be set to WHEN TRUE.</p> <p>The TRUE trigger is roughly equivalent to \"at startup\" and you should be sure not to confuse it with \"always true\". Recall it is transitions that are important, and this trigger makes its only nominal FALSE-TO-TRUE transition once at the very beginning of the run. Once displaced by an overlapping action, the rule will never activate again</p> <p>A rule that evaluates to a trivial FALSE will never do anything.</p> <p>As an example of a situation where these concepts matter, consider a rule that toggles use of a gate for the entire simulation. By default, a gate in the model is installed. Assume we have set up an expression named use_barriers or remove_barriers indicating whether we want to use gates. Three possibilities for writing the rule are:</p> <pre><code>   TRIGGER                ACTION \n1. TRUE                SET gate_install(gate=...) TO use_gate\n2. use_gate            SET gate_install(gate...) TO INSTALL\n3. remove_gate         SET gate_install(gate=...) TO REMOVE\n</code></pre> <p>Option 1 uses the default trigger. It will be activated at startup and the gate installation will be set to the expression variable use_gate. Option 2 is interesting because it will never do anything useful. It will be evaluated once at the start of the run, but it will never trigger if use_gate is FALSE. It will trigger if use_gate is TRUE, but this merely carries out the default. Option 3 remedies this by using remove_gate -- the non-default -- as the trigger. Different users seem to regard different options (1) and (3) more intuitive.</p>"},{"location":"manual/reference/Operating_Rule_Guide/#conflicts","title":"Conflicts","text":"<p>When a rule is triggered, it will be activated unless it conflicts with another, active rule. Rules conflict when they operate on the same model variable. For instance, two rules that act to change a weir coefficient in the same gate/weir conflict.</p> <p>Two specifications govern conflicts:</p> <p>1. When a rule conflicts with an active rule it is deferred. Deferred rules are not activated, but they are tricked into thinking they evaluated FALSE so that the can possibly make a FALSE-TRUE transition again the next time step.</p> <p>2. When a rule conflicts with another potentially activating rule, the results are \u201cundefined\u201d. We are unaware of any universal solution in this situation. The best solution is to write rules that don\u2019t do this \u2013 we are currently working on a better warning system to detect when this happens.</p>"},{"location":"manual/reference/Operating_Rule_Guide/#dsm2-variable-and-function-reference","title":"DSM2 Variable and Function Reference:","text":""},{"location":"manual/reference/Operating_Rule_Guide/#variables","title":"Variables","text":"<p>The variables from DSM2 that can be used in operating rules include boundary and grid variables that can be changed and those that are merely observable (read-only). The observable variables are divided between variables that can be set to time series (Dynamic Variables) that will apply ever-after and variables that can only be set to new static values (Static Variables)</p>"},{"location":"manual/reference/Operating_Rule_Guide/#dynamic-control-variables","title":"Dynamic Control Variables","text":"<p>These variables are dynamically controllable and can be set to a time series. Once the new time series is set, the boundary or structure being controlled will have no memory of its old controlling time series. Most dynamic variables are gate and boundary data.</p> <pre><code>gate_op(gate=textname,device=textname, direction=[to_node|from_node|to_from_node])\n</code></pre> <p>Device operating coefficients (0..1) in corresponding direction. Use keywords CLOSE (=0) and OPEN (=1) to make rules more readable. The option \"to_from_node\" is write-only -- a convenience feature that writes to two otherwise separate variables.</p> <pre><code>gate_position(gate=textname,device=textname)\n</code></pre> <p>Physical operation of control structure such as radial gate height (physical units). The interpretation of \"position\" is dependent on the \"control_type\" of the gate. If it is gated from the bottom, position indicates elevation and is the same as elev. If the control type is gated from the top, as in a radial gate, the position is the height. This variable is deprecated now, in favor of directly using \"elev\" or \"height\".</p> <pre><code>gate_height(gate=textname,device=textname)\n</code></pre> <p>Height of gate device.</p> <pre><code>gate_elev(gate=textname,device=textname)\n</code></pre> <p>Crest elevation or invert elevation of gate device.</p> <pre><code>gate_width(gate=textname,device=textname)\n</code></pre> <p>Width or radius of gate device.</p> <pre><code>ext_flow(name=textname)\n</code></pre> <p>External flow (boundary flows, source/sink)</p> <pre><code>transfer_flow(transfer=textname)\n</code></pre> <p>Flows in object-to-object transfers</p>"},{"location":"manual/reference/Operating_Rule_Guide/#static-control-variables","title":"Static Control Variables","text":"<p>These are variables that are normally static. You can set them to a constant. If you set them to a time series, the model will not complain, but the result may not be what you expect. The model variable will only be set to the current value of the series at the time the rule was activated. The variable won't keep changing with the time series.</p> <pre><code>gate_install(gate=textname)\n</code></pre> <p>Determines or inquires whether the given gate is installed.</p> <pre><code>SET gate_install(...) TO [REMOVE|FALSE]\n</code></pre> <p>completely removes the gate and restores an equal-stage compatibility condition to the channel junction.</p> <pre><code>SET gate_install(...) TO [INSTALL|TRUE]\n</code></pre> <p>installs the gate.</p> <pre><code>gate_coef(gate=textname,device=textname,direction=[to_node|from_node])\n</code></pre> <p>Gate coefficient of the device in the given direction. This is a physical quantitity of the structure, representing the roughness or efficiency of flow. It should not be used for operating controls such as flap gates. The coefficients will change only rarely when the actual site is altered and should never leave the range (0,1).</p> <pre><code>gate_nduplicate(gate=textname,device=textname)\n</code></pre> <p>Number of duplicate devices.</p>"},{"location":"manual/reference/Operating_Rule_Guide/#observable-variables","title":"Observable Variables","text":"<p>These are read-only model variables that cannot be manipulated directly, but can be observed and used in expressions for triggers and actions.</p> <pre><code>chan_flow(channel=number,dist=[number|length])\n</code></pre> <p>Flow in channel.dist=length indicates the end of the channel.</p> <pre><code>chan_vel(channel=number, dist=[number|length])\n</code></pre> <p>Velocity at given channel and distance.</p> <pre><code>chan_stage(channel=number,dist=[number|length])\n</code></pre> <p>Electrical Conductivity from GTM at given channel and distance.</p> <pre><code>chan_ec(channel=number,dist=[number|length])\n</code></pre> <p>Water Surface at given channel and distance.</p> <pre><code>chan_surf\n</code></pre> <p>Same as stage (water surface) in channel</p> <pre><code>res_stage(res=textname)\n</code></pre> <p>Water surface in reservoir</p> <pre><code>res_flow(res=textname, node=number)\n</code></pre> <p>Flow from reservoir to node</p> <pre><code>ts(name=textname)\n</code></pre> <p>Any time series named in the Operating Rule View of the GUI may be used by referencing the name. Time series evaluate to their value at the current time step.</p>"},{"location":"manual/reference/Operating_Rule_Guide/#model-time-queries","title":"Model Time Queries","text":"<p>The following commands retrieve model date or seasonal information:</p> <pre><code>YEAR, MONTH, DAY\n</code></pre> <p>Retrieves the year, month and day associated with the current model time step. These are returned as numbers. When testing them, you can (for clarity) use 3-letter abbreviations for the months. Examples:</p> <pre><code>YEAR &gt;= 1991\n\nMONTH + 1 &lt; MAY\n\nHOUR, MIN\n</code></pre> <p>Retrieve the (24 hour) hour and minute associated with the current model time step.</p> <pre><code>DATE\n</code></pre> <p>Returns a time stamp corresponding to the beginning of the day on the current model date. Example:</p> <pre><code>DATE &gt;= 11OCT1992 (not time part)\n\nDT\n</code></pre> <p>Represents the model time step in seconds. This is often useful for use with ACCUMULATE</p> <pre><code>DATETIME\n</code></pre> <p>Returns a time stamp corresponding to the current model date and time. Example:</p> <pre><code>DATETIME &gt; 04FEB1990 00:00 (date plus time)\n\nSEASON\n</code></pre> <p>Returns a time stamp relative to the beginning of the year corresponding to the beginning of the day on the current model date and time. Comparisons such as SEASON &gt; 15APR AND SEASON \\&lt;01MAY avoid common logical mistakes from building this from scratch. There is one other gotcha with SEASON that comes up at the end of time periods because the timestamp is always at 00:00. Compare SEASON &gt; 15APR AND SEASON \\&lt;01MAY SEASON &gt; 15APR AND SEASON \u226430APR and notice that the latter does not include the entire day 30APR.</p> <p>Note SEASON and DATE/DATETIME to combined expressions built from atomic expressions like day and month. They are clearer and avoid some curious gotchas. For instance DATE &gt;= 14APR1990 will evaluate true only once per year, whereas (YEAR &gt;= 1990 AND MONTH&gt;=APR AND DAY&gt;=14) will evaluate true on Apr 14, false on May 1 and true again on May 14. You could get the intended behavior with (YEAR == 1990 AND MONTH == APR AND DAY&gt;=14), which will go from false to true only once, but the fix hardly seems worth the trouble.</p>"},{"location":"manual/reference/Operating_Rule_Guide/#numerical-operations","title":"Numerical Operations","text":"<p>The following operators and functions are available</p> <pre><code>+, -, *, /\n</code></pre> <p>Arithmetic operators with standard precedence of operations. You can use parenthesis to change the evaluation order.</p> <pre><code>x^3, x^y\n</code></pre> <p>Power of x and x to the power of y</p> <pre><code>MIN2(x,y)\n</code></pre> <p>Minimum of two arguments.</p> <pre><code>MAX2(x,y)\n</code></pre> <p>Maximum of two arguments.</p> <pre><code>MIN3(x,y,z)\n</code></pre> <p>Minimum of three arguments.</p> <pre><code>MAX3(x,y,z)\n</code></pre> <p>Maximum of three arguments.</p> <pre><code>SQRT(x)\n</code></pre> <p>Square root of x</p> <pre><code>EXP(x)\n</code></pre> <p>Exponent function (e to the power of x)</p> <pre><code>LN(x)\n</code></pre> <p>Natural log of x</p> <pre><code>LOG(x)\n</code></pre> <p>Base 10 log of x</p>"},{"location":"manual/reference/Operating_Rule_Guide/#logical-operations","title":"Logical Operations","text":"<pre><code>x==y\n</code></pre> <p>Tests equality.</p> <pre><code>x&lt;&gt;y\n</code></pre> <p>Tests inequality.</p> <pre><code>x&lt;y,x&gt;y, x&lt;=y, x&gt;=y\n</code></pre> <p>Comparisons.</p> <pre><code>TRUE\n</code></pre> <p>The value TRUE</p> <pre><code>FALSE\n</code></pre> <p>The value FALSE</p> <pre><code>NOT expression\n</code></pre> <p>Negation of expression, as in NOT(x \\&lt; y)</p> <pre><code>expr1 AND expr2\n</code></pre> <p>Logical \u2018and\u2019, which evaluates to TRUE only if both the expressions it joins are true. Expression (expr2) will not be evaluated if expr1 evaluates to FALSE.</p> <pre><code>expr1 OR expr2\n</code></pre> <p>Logical \u2018or\u2019</p>"},{"location":"manual/reference/Operating_Rule_Guide/#special-functions","title":"Special Functions","text":"<pre><code>ACCUMULATE(expression, initval [,resetcond])\n</code></pre> <p>Cumulative value function. Accumulates additively the value of expression using initval (another numerical expression) as the initial condition and resetting the total anytime the resetcond evaluates to true. If you want to integrate you should multiply the expression by DT or else your rule won't be robust if someone changes the time step.</p> <pre><code>IFELSE(boolexpr, valexp1,valexpr2)\n</code></pre> <p>The ternary operator. If boolexpr returns true, returns the value given by valexpr1. If boolexpr returns false, returns the value given by valexpr2.</p> <pre><code>LOOKUP(expression, lookup_array,value_array)\n</code></pre> <p>Lookup values from a small user supplied table. The lookup array is provided using a bracketed, comma-separated list of values such as [1000.,2000.,3000.]. The value_array return values are similar but must have a length one smaller than the number of lookup values. The array values must be hard-wired numbers at the present time -- expressions are not allowed. The LOOKUP compares expression to elements of lookup_array. The highest element of the lookup table is currently a limit, not an actual lookup slot. The function returns the component of value_array corresponding to the highest index in lookup array that is \\&lt;= expression, e.g.:</p> <pre><code>LOOKUP(1000.,[1000.,2000.,3000.], [1.,2.]) returns 1.\n\nLOOKUP(2000.,[1000.,2000.,3000.], [1.,2.]) returns 2.\n\nLOOKUP(3000.,[1000.,2000.,3000.], [1.,2.]) is an error.\n\nPID( PID(expression,target,low,high,K, Ti,Td,Tt,b)\n</code></pre> <p>Use PID (Proportional, Integral, Derivative) control to try to guide expression towards target. The parameters are as follows low: lower bound on control representing the minimum value the control value can take (e.g. for gate height this might be zero).</p> <p>high: upper bound on control.</p> <p>K: The constant representing the Proportion component of the control. The constant multiplies (expression-target) to change a control value, so choose a factor that is reasonable that takes the scaling of the expression to the scaling of the control.</p> <p>Ti: Integral time constant of control</p> <p>Td: Derivative time constant of control.</p> <p>Tt: Time basis of \"anti-windup\"</p> <p>b: Set-point weighting (use 1.0 if you are new to PID).</p>"},{"location":"manual/reference/Output_Channel/","title":"Output Channel","text":""},{"location":"manual/reference/Output_Channel/#overview","title":"Overview:","text":"<p>The OUTPUT_CHANNEL table is used by both HYDRO and QUAL to specify output requests inside of the channel. Output is HEC-DSS or text format. The variables that can be requested vary by model.</p>"},{"location":"manual/reference/Output_Channel/#tables","title":"Tables:","text":"<ul> <li>OUTPUT_CHANNEL</li> <li>OUTPUT_CHANNEL_SOURCE_TRACK</li> </ul>"},{"location":"manual/reference/Output_Channel/#output_channel","title":"OUTPUT_CHANNEL","text":"<p>The table specifies the name for output request, as well as the location, variable being output, time aggregation and destination file.</p>"},{"location":"manual/reference/Output_Channel/#field-descriptions","title":"Field Descriptions","text":""},{"location":"manual/reference/Output_Channel/#name","title":"NAME","text":"<p>Name of the output request. This is part of the identifier of the table and will be used in the B_PART of the output if it is in DSS format. Generally, non-modelers will have an easier time understanding your output if this is a station name that is geographically fixed (e.g. \"vernalis\" or \"RSAC075\") than if it is a modeling construct (\"ch101\"). Similarly, avoid using avoid using VARIABLE inside this name -- this causes redundancy in the output DSS path and the layering won't work as well.</p>"},{"location":"manual/reference/Output_Channel/#chan_no","title":"CHAN_NO","text":"<p>Channel number in which output is requested.</p>"},{"location":"manual/reference/Output_Channel/#distance","title":"DISTANCE","text":"<p>Distance along channel (from upstream node to downstream), typically in feet. Results will be interpolated between Eulerian (HYDRO) or Lagrangian (QUAL) computational points.</p>"},{"location":"manual/reference/Output_Channel/#variable","title":"VARIABLE","text":"<p>Model variable to be output. In HYDRO, you can request\u00a0stage,flow,vel. In QUAL you can request\u00a0stage,flow\u00a0or the name of any constituent in the model. When no output request is made for a constituent that is not required for reaction kinetics, it is not calculted.</p>"},{"location":"manual/reference/Output_Channel/#interval","title":"INTERVAL","text":"<p>Time Interval of the output. Can be any DSS-compliant interval with a unit that is not calendar dependent (MIN, HOUR, DAY). This is a departure from previous versions of DSM2, which offered monthly output.</p>"},{"location":"manual/reference/Output_Channel/#period_op","title":"PERIOD_OP","text":"<p>Period aggregation performed to convert the model time step into the time interval of the output. May be INST or AVE, which produce instantaneous</p>"},{"location":"manual/reference/Output_Channel/#file","title":"FILE","text":"<p>Name of the output file where the data will be stored. If the extension .txt is given, the output is automatically in text format. If a .dss extension is used, output is in HEC-DSS format.</p>"},{"location":"manual/reference/Output_Channel/#table-info","title":"Table Info","text":""},{"location":"manual/reference/Output_Channel/#identifier","title":"Identifier:","text":"<p>NAME, VARIABLE</p>"},{"location":"manual/reference/Output_Channel/#parent-table","title":"Parent Table:","text":"<p>Table is parent</p>"},{"location":"manual/reference/Output_Channel/#include-block","title":"Include Block:","text":"<p>OUTPUT_TIME_SERIES</p>"},{"location":"manual/reference/Output_Channel/#output_channel_source_track","title":"OUTPUT_CHANNEL_SOURCE_TRACK","text":"<p>This table is identical to OUTPUT_CHANNEL except it is only used in QUAL and it contains one additional field for tracking constituent sources.</p>"},{"location":"manual/reference/Output_Channel/#field-descriptions_1","title":"Field Descriptions","text":""},{"location":"manual/reference/Output_Channel/#name_1","title":"NAME","text":"<p>Name of the output request. See comments above, and note that in this case you should also avoid using the SOURCE_NAME in the output name.</p>"},{"location":"manual/reference/Output_Channel/#chan_no_1","title":"CHAN_NO","text":"<p>Channel number in which output is requested.DISTANCEDistance along channel (from upstream node to downstream), typically in feet. Results will be interpolated between Eulerian (HYDRO) or Lagrangian (QUAL) computational points.</p>"},{"location":"manual/reference/Output_Channel/#variable_1","title":"VARIABLE","text":"<p>Model variable to be output. In HYDRO, you can request\u00a0stage,flow,vel. In QUAL you can request\u00a0stage,flow\u00a0or the name of any constituent in the model. When no output request is made for a constituent that is not required for reaction kinetics, it is not calculted.</p>"},{"location":"manual/reference/Output_Channel/#source_group","title":"SOURCE_GROUP","text":"<p>Name of the source group that is being tracked in this output request. To learn how to define a group, see\u00a0group\u00a0reference. The group used must consist entirely of boundary or source locations -- not water bodies.</p>"},{"location":"manual/reference/Output_Channel/#interval_1","title":"INTERVAL","text":"<p>Time Interval of the output. Can be any DSS-compliant interval with a unit that is not calendar dependent (MIN, HOUR, DAY). This is a departure from previous versions of DSM2, which offered monthly output.</p>"},{"location":"manual/reference/Output_Channel/#period_op_1","title":"PERIOD_OP","text":"<p>Period aggregation performed to convert the model time step into the time interval of the output. May be INST or AVE, which produce instantaneous</p>"},{"location":"manual/reference/Output_Channel/#file_1","title":"FILE","text":"<p>Name of the output file where the data will be stored. If the extension .txt is given, the output is automatically in text format. If a .dss extension is used, output is in HEC-DSS format.</p>"},{"location":"manual/reference/Output_Channel/#table-info_1","title":"Table Info","text":""},{"location":"manual/reference/Output_Channel/#identifier_1","title":"Identifier:","text":"<p>NAME, VARIABLE, SOURCE_GROUP</p>"},{"location":"manual/reference/Output_Channel/#parent-table_1","title":"Parent Table:","text":"<p>Table is parent</p>"},{"location":"manual/reference/Output_Channel/#include-block_1","title":"Include Block:","text":"<p>OUTPUT_TIME_SERIES</p> <ul> <li>Finer output is preferred to daily. You can easily average to daily     later outside the model using a script or time series application.     Tidal data are poorly represented by daily aggregations, and     numerous incorrect conclusions have arisen from aliasing     (fluctuations over two weeks) when a 24 hour daily averaging     operation is imposed on a naturally 25-hour phenomenon. Monthly     output is no longer allowed.</li> </ul>"},{"location":"manual/reference/Output_Gate/","title":"Output Gate","text":""},{"location":"manual/reference/Output_Gate/#overview","title":"Overview","text":"<p>The OUTPUT_GATE table is used by both HYDRO to specify output requests at a gate. Output is HEC-DSS or text format.</p>"},{"location":"manual/reference/Output_Gate/#tables","title":"Tables","text":"<ul> <li>OUTPUT_GATE</li> </ul>"},{"location":"manual/reference/Output_Gate/#output_gate","title":"OUTPUT_GATE","text":"<p>The table specifies the name for output request, as well as the location, variable being output, time aggregation and destination file.</p>"},{"location":"manual/reference/Output_Gate/#field-descriptions","title":"Field Descriptions","text":""},{"location":"manual/reference/Output_Gate/#name","title":"NAME","text":"<p>Name of the output request. This is the identifier of the table and will be used in the B_PART of the output if it is in DSS format. It can be the same as the gate name but it doesn't have to be. Avoid using VARIABLE inside this name -- this causes redundancy in the output and the layering won't work correctly.</p>"},{"location":"manual/reference/Output_Gate/#gate_name","title":"GATE_NAME","text":"<p>Name of the gate at which output is requested.</p>"},{"location":"manual/reference/Output_Gate/#device","title":"DEVICE","text":"<p>Name of the gate device, if applicable. You can request operational or physical data from a device as well as flow. You can also request some gate output (install,flow) that is not linked to a particular device. In this case, the field should be set to none.</p>"},{"location":"manual/reference/Output_Gate/#variable","title":"VARIABLE","text":"<p>Model variable to be output. From a device you can request some physical data (width, height, elev), operational data (op_to_node, op_from_node, position) or flow oriented from water body to node. From a gate with device=none you can request the variables install, or total flow oriented from water body to node.</p>"},{"location":"manual/reference/Output_Gate/#interval","title":"INTERVAL","text":"<p>Time Interval of the output. Can be any DSS-compliant interval with a unit that is not calendar dependent (MIN, HOUR, DAY). This is a departure from previous versions of DSM2, which offered monthly output.</p>"},{"location":"manual/reference/Output_Gate/#period_op","title":"PERIOD_OP","text":"<p>Period aggregation performed to convert the model time step into the time interval of the output. May be INST or AVE, but AVE can be meaningless for a lot of gate variables.</p>"},{"location":"manual/reference/Output_Gate/#file","title":"FILE","text":"<p>Name of the output file where the data will be stored. If the extension .txt is given, the output is automatically in text format. If a .dss extension is used, output is in HEC-DSS format.</p>"},{"location":"manual/reference/Output_Gate/#table-info","title":"Table Info","text":""},{"location":"manual/reference/Output_Gate/#identifier","title":"Identifier","text":"<p>NAME, VARIABLE</p>"},{"location":"manual/reference/Output_Gate/#parent-table","title":"Parent Table","text":"<p>Table is parent</p>"},{"location":"manual/reference/Output_Gate/#include-block","title":"Include Block","text":"<p>OUTPUT_TIME_SERIES</p>"},{"location":"manual/reference/Output_Gate/#examples","title":"Examples","text":"<p>output_gate_example.inp</p> <ul> <li>Initially, the thing that is hard to get about gate output is the   flow orientation. The output for the gate is oriented with the gate,   which may or may not be in the upstream-downstream direction.</li> </ul>"},{"location":"manual/reference/Output_Reservoir/","title":"Output Reservoir","text":""},{"location":"manual/reference/Output_Reservoir/#overview","title":"Overview","text":"<p>The OUTPUT_RESERVOIR table is used by both HYDRO and QUAL to specify output requests inside a reservoir. Output is in HEC-DSS or text format. The variables that can be requested vary by model.</p>"},{"location":"manual/reference/Output_Reservoir/#tables","title":"Tables","text":""},{"location":"manual/reference/Output_Reservoir/#output_reservoir","title":"OUTPUT_RESERVOIR","text":"<p>The table specifies the name for the output request, as well as the location, variable being output, time aggregation, and destination file.</p>"},{"location":"manual/reference/Output_Reservoir/#field-descriptions","title":"Field Descriptions","text":"<ul> <li>NAME: Name of the output request. This is part of the identifier of the table and will be used in the B_PART of the output if it is in DSS format. Avoid using VARIABLE inside this name to prevent redundancy in the output DSS path.</li> <li>RES_NAME: Name of the reservoir in which output is requested.</li> <li>NODE: Node number, if the request is for a flow to a particular connected node.</li> <li>VARIABLE: Model variable to be output. In HYDRO, you can request <code>stage</code>, <code>flow</code>, or <code>vel</code>. In QUAL, you can request <code>stage</code>, <code>flow</code>, or the name of any constituent in the model.</li> <li>INTERVAL: Time interval of the output. Can be any DSS-compliant interval with a unit that is not calendar-dependent (e.g., MIN, HOUR, DAY).</li> <li>PERIOD_OP: Period aggregation performed to convert the model time step into the time interval of the output. May be <code>INST</code> or <code>AVE</code>.</li> <li>FILE: Name of the output file where the data will be stored. If the extension is <code>.txt</code>, the output is in text format. If a <code>.dss</code> extension is used, output is in HEC-DSS format.</li> </ul>"},{"location":"manual/reference/Output_Reservoir/#table-info","title":"Table Info","text":"<ul> <li>Identifier: NAME, VARIABLE</li> <li>Include Block: OUTPUT_TIME_SERIES</li> </ul>"},{"location":"manual/reference/Output_Reservoir/#output_reservoir_source_track","title":"OUTPUT_RESERVOIR_SOURCE_TRACK","text":"<p>This table is identical to OUTPUT_RESERVOIR except it is only used in QUAL and contains one additional field for tracking constituent sources.</p>"},{"location":"manual/reference/Output_Reservoir/#field-descriptions_1","title":"Field Descriptions","text":"<ul> <li>NAME: Name of the output request. This is part of the identifier of the table and will be used in the B_PART of the output if it is in DSS format. Avoid using SOURCE_NAME in the output name.</li> <li>RES_NAME: Name of the reservoir in which output is requested.</li> <li>NODE: Node number, if the request is for a flow to a particular connected node. Otherwise, use <code>none</code>.</li> <li>VARIABLE: Model variable to be output. In HYDRO, you can request <code>stage</code>, <code>flow</code>, or <code>vel</code>. In QUAL, you can request <code>stage</code>, <code>flow</code>, or the name of any constituent in the model.</li> <li>SOURCE_GROUP: Name of the source group that is being tracked in this output request. The group used must consist entirely of boundary or source locations\u2014not water bodies.</li> <li>INTERVAL: Time interval of the output. Can be any DSS-compliant interval with a unit that is not calendar-dependent (e.g., MIN, HOUR, DAY).</li> <li>PERIOD_OP: Period aggregation performed to convert the model time step into the time interval of the output. May be <code>INST</code> or <code>AVE</code>.</li> <li>FILE: Name of the output file where the data will be stored. If the extension is <code>.txt</code>, the output is in text format. If a <code>.dss</code> extension is used, output is in HEC-DSS format.</li> </ul>"},{"location":"manual/reference/Output_Reservoir/#table-info_1","title":"Table Info","text":"<ul> <li>Identifier: NAME, VARIABLE, SOURCE_GROUP</li> <li>Include Block: OUTPUT_TIME_SERIES</li> </ul> <p>Finer output is preferred to daily. You can easily average to daily later outside the model using a script or time series application. Tidal data are poorly represented by daily aggregations, and numerous incorrect conclusions have arisen from aliasing when a 24-hour daily averaging operation is imposed on a naturally 25-hour phenomenon.</p>"},{"location":"manual/reference/PTM_Output_Files/","title":"PTM Output Files","text":""},{"location":"manual/reference/PTM_Output_Files/#overview","title":"Overview","text":"<p>PTM outputs a <code>trace.out</code> and <code>animation.bin</code> file in addition to the <code>.dss</code> files.</p> <ul> <li>The animation binary file outputs in Java binary format the snapshot location of all particles in the simulation.</li> <li>The trace output file only records the event (timestamp) when each particle passes from one waterbody to another waterbody.</li> </ul> <p>All indices are internal global indices of the grid. All times are in Julian time.</p>"},{"location":"manual/reference/PTM_Output_Files/#traceout-content","title":"Trace.out Content","text":"Column Description 1 Start time 2 End time 3 Time step 4 Total particle number Event Event time ID Particle ID Node Node ID particle passing Water Waterbody particle entering <p><code>trace.out</code> is written by ParticleObserver, which is incorporated in each particle, then read by the flux class.</p> <p>Time is in Julian minute.</p>"},{"location":"manual/reference/Particle_Filter/","title":"Particle Filter","text":""},{"location":"manual/reference/Particle_Filter/#overview","title":"Overview","text":"<p>Particle Filter is a section in the PTM input where you set up particle filters. It is designed to modify the particle flux at a node without changing hydrodynamic conditions by keeping particles from entering the specified waterbody.</p>"},{"location":"manual/reference/Particle_Filter/#tables","title":"Tables","text":""},{"location":"manual/reference/Particle_Filter/#example","title":"Example","text":"<pre><code>PARTICLE_FILTER\nNAME NODE WATERBODY FILLIN FILE PATH\nfilter_hor 8 chan:54 last constant 0\nfilter_nf 280 chan:357 last ./Filter_OP_NF.dss /HIST+FILTER/FILTER_NF/FILTER_OP//IR-DECADE/DWR-BDO/\nEND\n</code></pre> <p>The PARTICLE_FILTER table defines particle filters by giving them names, associating them to a node and a waterbody, and setting up the passing efficiency (which could be a constant value or time-varying data in DSS).</p>"},{"location":"manual/reference/Particle_Filter/#field-descriptions","title":"Field Descriptions","text":"<ul> <li>NAME: Name assigned to the particle filter. This is the identifier of the filter used elsewhere to refer to the filter.</li> <li>NODE: The ID of the node to which the filter is attached.</li> <li>WATERBODY: The type and ID of the waterbody to which the filter is attached.</li> <li>FILLIN: Method for filling in data if the time step of the assigned series is coarser than the time step of the model. See fillin types.</li> <li>FILE: DSS or text file in which data are stored. Use consistent case when referring to the same file. You may also enter the word <code>constant</code> if you would like to assign a constant value to the input (the value will be entered in the next column).</li> <li>PATH: The path within the text or DSS file of the time series data. If you used the <code>constant</code> keyword in the Input File column, enter the value here. The stored variable is particle passing efficiency, a float value between 0 (block) and 1 (totally pass).</li> </ul> <p>Filters are 2-directional and function differently for each direction: - Waterbody \u2192 Node: Filter serves as a total block with passing efficiency 0. - Node \u2192 Waterbody: Filter adjusts particle decision-making with passing efficiency as a re-adjusting factor.</p>"},{"location":"manual/reference/Particle_Flux_Output/","title":"Particle Flux Output","text":""},{"location":"manual/reference/Particle_Flux_Output/#overview","title":"Overview","text":"<p>PARTICLE_FLUX_OUTPUT is a section in the PTM text input that specifies how the PTM records the number of particles in a group of water bodies into a DSS output.</p>"},{"location":"manual/reference/Particle_Flux_Output/#example","title":"Example","text":"<pre><code>FLUX_OUTPUT\nNAME FROM_WB TO_WB INTERVAL FILENAME\nTWITCHELL res:clifton_court group:swp 15MIN ${PTMOUTPUTFILE}\nEMMATON chan:216 group:cvp 15MIN ${PTMOUTPUTFILE}\nDIVERSION_AG group:all group:ag_div 15MIN ${PTMOUTPUTFILE}\nEND\n</code></pre> <p>The PARTICLE_FLUX_OUTPUT table defines how particle flux is recorded between water bodies or groups.</p>"},{"location":"manual/reference/Particle_Flux_Output/#field-descriptions","title":"Field Descriptions","text":"<ul> <li>NAME: This is the name that will go in the B_PART of the output.</li> <li>FROM_WB: Name of the water body or group that is the \"from\" location of the flux.</li> <li>TO_WB: Name of the water body or group that is the \"to\" destination in the flux.</li> <li>INTERVAL: Interval at which to record residence.</li> <li>FILENAME: The name of the output file. If the file extension is <code>.dss</code>, output is in DSS format. If the file extension is <code>.txt</code>, a text file output is produced.</li> </ul>"},{"location":"manual/reference/Particle_Flux_Output/#table-info","title":"Table Info","text":"<ul> <li>Identifier: NAME</li> </ul> <p>Particle flux output can be in absolute number of particles or percentage of injection. The option is set by PTM_FLUX_PERCENT in the SCALAR section.</p>"},{"location":"manual/reference/Particle_Group_Output/","title":"Particle Group Output","text":""},{"location":"manual/reference/Particle_Group_Output/#overview","title":"Overview","text":"<p>PARTICLE_GROUP_OUTPUT is a section in the PTM input that specifies DSS output to record the residence of particles in a group of water bodies.</p>"},{"location":"manual/reference/Particle_Group_Output/#example","title":"Example","text":"<pre><code>PARTICLE_GROUP_OUTPUT\nNAME GROUP_NAME INTERVAL FILENAME\nTWITCHELL twitchell 1HOUR ${PTMOUTPUTFILE}\nEMMATON emmaton 1HOUR ${PTMOUTPUTFILE}\nEND\n</code></pre> <p>The PARTICLE_GROUP_OUTPUT table defines how particle residence is recorded for groups of water bodies.</p>"},{"location":"manual/reference/Particle_Group_Output/#field-descriptions","title":"Field Descriptions","text":"<ul> <li>NAME: This is the output name that will go in the B_PART of the output.</li> <li>GROUP_NAME: Name of the group defined in GROUP.</li> <li>INTERVAL: Interval at which to record residence.</li> <li>FILENAME: The name of the output file. If the file extension is <code>.dss</code>, output is in DSS format. If the file extension is <code>.txt</code>, a text file output is produced.</li> </ul>"},{"location":"manual/reference/Particle_Group_Output/#table-info","title":"Table Info","text":"<ul> <li>Identifier: NAME</li> </ul> <p>Similar to particle flux output.</p>"},{"location":"manual/reference/Particle_Insertion/","title":"Particle Insertion","text":""},{"location":"manual/reference/Particle_Insertion/#overview","title":"Overview","text":"<p>Particle Insertion is a section in the PTM input that specifies the insertion of particles in water bodies over time. The PTM can insert multiple sets of particles.</p>"},{"location":"manual/reference/Particle_Insertion/#tables","title":"Tables","text":""},{"location":"manual/reference/Particle_Insertion/#example","title":"Example","text":"<pre><code>PARTICLE_INSERTION\nNODE NPARTS DELAY DURATION\n1 1000 0hour 1day\n13 1000 1day 0hour\nEND\n</code></pre> <p>The PARTICLE_INSERTION table defines the insertion of particles at specific nodes over a given time interval.</p>"},{"location":"manual/reference/Particle_Insertion/#field-descriptions","title":"Field Descriptions","text":"<ul> <li>NODE: The node at which the insertion is made.</li> <li>NPARTS: Number of particles.</li> <li>DELAY: Delay before the first insertion after the beginning of the PTM run. The unit of time needs to be attached without spaces.</li> <li>DURATION: Interval over which insertion is evenly distributed in time. If the time is set as zero, all the particles are inserted instantaneously. The unit of time needs to be attached without spaces.</li> </ul>"},{"location":"manual/reference/Particle_Reservoir_Filter/","title":"Particle Reservoir Filter","text":""},{"location":"manual/reference/Particle_Reservoir_Filter/#overview","title":"Overview","text":"<p>Particle Reservoir Filter is a section in the PTM input where you set up particle filters. It is designed to modify the particle flux at a reservoir without changing hydrodynamic conditions by keeping particles from entering the specified waterbody.</p>"},{"location":"manual/reference/Particle_Reservoir_Filter/#tables","title":"Tables","text":""},{"location":"manual/reference/Particle_Reservoir_Filter/#example","title":"Example","text":"<pre><code>PARTICLE_RES_FILTER\nNAME RES_NAME WATERBODY FILLIN FILE PATH\nclfc_div_bbid clifton_court qext:dicu_div_bbid last ./filterOp.dss /HIST+FILTER/CLFC_DIV/FILTER_OP//IR-DECADE/DWR-BDO/\nEND\n</code></pre> <p>This is a special filter located at a reservoir directly connecting to a source flow. The PARTICLE_RES_FILTER table defines particle filters by giving them names, associating them to a reservoir and one of its directly connecting waterbodies, and setting up the passing efficiency (which could be a constant value or time-varying data in DSS).</p>"},{"location":"manual/reference/Particle_Reservoir_Filter/#field-descriptions","title":"Field Descriptions","text":"<ul> <li>NAME: Name assigned to the particle filter. This is the identifier of the filter used elsewhere to refer to the filter.</li> <li>RES_NAME: The name of the reservoir to which the filter is applied.</li> <li>WATERBODY: The type and ID of the waterbody to which the filter is attached.</li> <li>FILLIN: Method for filling in data if the time step of the assigned series is coarser than the time step of the model. See fillin types.</li> <li>FILE: DSS or text file in which data are stored. Use consistent case when referring to the same file. You may also enter the word <code>constant</code> if you would like to assign a constant value to the input (the value will be entered in the next column).</li> <li>PATH: The path within the text or DSS file of the time series data. If you used the <code>constant</code> keyword in the Input File column, enter the value here. The stored variable is particle passing efficiency, a float value between 0 (block) and 1 (totally pass).</li> </ul> <p>Similar to Particle Filter.</p>"},{"location":"manual/reference/Rate_Coefficients/","title":"Rate Coefficients","text":""},{"location":"manual/reference/Rate_Coefficients/#overview","title":"Overview","text":"<p>Rate Coefficients are reaction and growth rates assigned to non-conservative constituents. This table assigns the rates to groups of water bodies (there usually aren't enough data to support individual assignments).</p>"},{"location":"manual/reference/Rate_Coefficients/#tables","title":"Tables","text":""},{"location":"manual/reference/Rate_Coefficients/#example","title":"Example","text":"<pre><code># Sample algae rate coefficients in a channel group\nRATE_COEFFICIENT\nGROUP_NAME CONSTITUENT VARIABLE VALUE\nchan_10_15 algae alg_die 0.2\nchan_10_15 algae alg_grow 1.5\nchan_10_15 algae alg_resp 0.15\nchan_10_15 algae settle 0.2\nEND\n</code></pre> <p>The Rate Coefficient Table lists reaction rate coefficients for non-conservative constituents. Different rates can be assigned to different water bodies. The assignment is done using groups\u2014first you define a group and then you assign rate coefficients to the group.</p>"},{"location":"manual/reference/Rate_Coefficients/#field-descriptions","title":"Field Descriptions","text":"<ul> <li>GROUP_NAME: Name of the group to which the coefficient entry is assigned.</li> <li>CONSTITUENT: Non-conservative constituent with which the coefficient is associated.</li> <li>VARIABLE: Physical process governed by the coefficient.</li> <li>VALUE: Value assigned to the coefficient.</li> </ul>"},{"location":"manual/reference/Rate_Coefficients/#table-info","title":"Table Info","text":"<ul> <li>Identifier: NAME</li> <li>Include Block: QUAL_SPATIAL</li> </ul> <p>Assignments on higher layers supersede assignments on lower layers, even if the patterns that cause the assignment are not the same. All channels must have rate coefficients for non-conservative DO runs.</p>"},{"location":"manual/reference/Reservoir/","title":"Reservoir","text":""},{"location":"manual/reference/Reservoir/#overview","title":"Overview","text":"<p>Reservoirs are open bodies of water that store flow and are connected to nodes by means of an energy-based equation. Reservoirs are considered instantly well-mixed.</p> <ul> <li>The Reservoirs Table specifies the identity and physical properties of the reservoir.</li> <li>Connections to nodes are specified in the Reservoir Connections table.</li> <li>Reservoir area as a function of elevation is specified in the Reservoir Volume table, while volume is calculated in code (since 8.2).</li> </ul>"},{"location":"manual/reference/Reservoir/#reservoir-table","title":"RESERVOIR Table","text":""},{"location":"manual/reference/Reservoir/#example","title":"Example","text":"<pre><code># Description:\n# Setting of Clifton Court Forebay\nRESERVOIR\nNAME AREA BOT_ELEV\nclifton_court 91.868000 -7.748\nEND\n</code></pre> <p>The RESERVOIR table defines the name and physical properties of the reservoir. In the case of a \"tank\"-like reservoir, the area and volume are simply defined by the constant area and bottom elevation times the constant area, respectively.</p>"},{"location":"manual/reference/Reservoir/#field-descriptions","title":"Field Descriptions","text":"<ul> <li>NAME: Name of the reservoir. This is the identifier of the reservoir used in other tables.</li> <li>AREA: Surface area (in units of million sq ft) of the reservoir at typical depth. This area is used to calculate volume changes.</li> <li>BOT_ELEV: Elevation (ft) of the bottom of the reservoir.</li> </ul>"},{"location":"manual/reference/Reservoir/#table-info","title":"Table Info","text":"<ul> <li>Identifier: NAME</li> <li>Include Block: GRID</li> </ul>"},{"location":"manual/reference/Reservoir/#reservoir_connection-table","title":"RESERVOIR_CONNECTION Table","text":""},{"location":"manual/reference/Reservoir/#example_1","title":"Example","text":"<pre><code># Description:\n# Setting of Frank Tract Connections\nRESERVOIR_CONNECTION\n\nRES_NAME NODE COEF_IN COEF_OUT\nfranks_tract 103 2250.000 2250.000\nfranks_tract 216 1500.000 1500.000\nEND\n</code></pre> <p>The RESERVOIR_CONNECTION table lists reservoir connections to neighboring nodes. Flow through reservoir connections is calculated using the following formula:</p> <p>Q = C<sub>to</sub> sqrt[ 2g(z<sub>node</sub> - z<sub>res</sub>) ] ... z<sub>res</sub> \\&lt; z<sub>node</sub></p> <p>Q = C<sub>from</sub> sqrt[ 2g(z<sub>res</sub> - z<sub>node</sub>) ] ... z<sub>res</sub> &gt; z<sub>node</sub></p> <p>Where:</p> <ul> <li>C<sub>to</sub> and C<sub>from</sub> are coefficients representing the hydraulic efficiency of the reservoir connection and the nominal area perpendicular to flow.</li> <li>g is gravity.</li> <li>z<sub>res</sub> and z<sub>node</sub> are the water surface elevations at the reservoir and node (node surface is assessed by means of a reference channel that has no reservoirs attached to it).</li> </ul>"},{"location":"manual/reference/Reservoir/#field-descriptions_1","title":"Field Descriptions","text":"<ul> <li>RES_NAME: Name of the reservoir at which the connection is specified.</li> <li>NODE: Number identifying the node at which the connection is specified.</li> <li>COEF_IN: Coefficient from node to reservoir, greater than zero. If you compare the reservoir equation to the gate or other orifice equation, you will find that the reservoir coefficient actually folds several quantities into one parameter: a flow efficiency (between zero and one) and an area of flow. If you have an observation of the area normal to flow, the coefficient should be some fraction of this aperture.</li> <li>COEF_OUT: Flow direction out of the reservoir.</li> </ul>"},{"location":"manual/reference/Reservoir/#table-info_1","title":"Table Info","text":"<ul> <li>Identifier: RES_NAME, NODE</li> <li>Parent Table: RESERVOIR</li> <li>Parent Identifier: RES_NAME</li> <li>Include Block: GRID</li> </ul> <p>A node may not have more than three reservoir connections and must have at least one ungated channel connection.</p>"},{"location":"manual/reference/Reservoir/#reservoir_vol-table","title":"RESERVOIR_VOL Table","text":""},{"location":"manual/reference/Reservoir/#example_2","title":"Example","text":"<pre><code>RESERVOIR_VOL\nRES_NAME ELEV AREA\nliberty -61.975 0.000\nliberty -32.808 2.478\nliberty -16.404 16.220\nliberty -3.281 272.328\nliberty -1.640 1017.270\nliberty 0.000 1999.522\nliberty 1.640 3031.999\nliberty 3.281 4209.851\nliberty 4.921 4584.028\nliberty 6.562 5190.456\nliberty 8.202 6359.679\nliberty 9.843 6636.050\nliberty 13.123 6731.118\nliberty 16.404 6830.894\nliberty 19.685 6876.916\nliberty 22.966 6890.138\nEND\n</code></pre> <p>Since version 8.2, reservoirs can also have variable area and volume defined as a function of elevation. This table still requires the reservoir to be defined in the RESERVOIR table even though the elevation area specified in the RESERVOIR table will be ignored if it is specified here.</p>"},{"location":"manual/reference/Reservoir/#field-descriptions_2","title":"Field Descriptions","text":"<ul> <li>RES_NAME: Name of the reservoir. This is the identifier of the reservoir and should have been specified in the RESERVOIR table.</li> <li>ELEV: Elevation (ft) of the reservoir at which the area and volume are specified. This elevation is to the datum of the rest of the model (currently NAVD88).</li> <li>AREA: Surface area (in acres) of the reservoir at the specified elevation. The area is interpolated between elevations based on the current elevation of the water level.</li> </ul>"},{"location":"manual/reference/Reservoir/#table-info_2","title":"Table Info","text":"<ul> <li>Identifier: NAME</li> <li>Include Block: GRID</li> </ul>"},{"location":"manual/reference/Reservoir_Concentration/","title":"Reservoir Concentration","text":""},{"location":"manual/reference/Reservoir_Concentration/#overview","title":"Overview","text":"<p>Reservoir concentration represents the concentration of constituents attached to reservoir sources.</p>"},{"location":"manual/reference/Reservoir_Concentration/#tables","title":"Tables","text":""},{"location":"manual/reference/Reservoir_Concentration/#example","title":"Example","text":"<pre><code># Description:\n# BBID EC concentration in Clifton Court Forebay\nRESERVOIR_CONCENTRATION\nNAME RES_NAME VARIABLE FILLIN FILE PATH\ndicu_drain_bbid clifton_court ec last ../../timeseries/dicuwq_3vals_extended.dss /DICU-HIST+RSVR/BBID/DRAIN-EC//1MON/DWR-BDO/\nEND\n</code></pre> <p>The RESERVOIR_CONCENTRATION table attaches concentrations to boundary and source flows defined in QUAL. The table also assigns a time series to the source.</p>"},{"location":"manual/reference/Reservoir_Concentration/#field-descriptions","title":"Field Descriptions","text":"<ul> <li>NAME: Name assigned to the source. An entry here must have the same name as an entry in the BOUNDARY_STAGE, BOUNDARY_FLOW, or SOURCE_FLOW tables. By matching names, you attach concentrations to the flow.</li> <li>NODE_NO: Node number where the flow is applied. This must match the node number given in the original flow table.</li> <li>VARIABLE: Constituent name. If no output is requested for the constituent, it will be ignored.</li> <li>FILLIN: Method for filling in data if the time step of the assigned series is coarser than the time step of the model.</li> <li>FILE: DSS or text file in which data are stored. Use consistent case when referring to the same file. You may also enter the word <code>constant</code> if you would like to assign a constant value to the input (the value will be entered in the next column).</li> <li>PATH: The path within the text or DSS file of the time series data. If you used the <code>constant</code> keyword in the Input File column, enter the value (e.g., <code>4.22</code>) here.</li> </ul>"},{"location":"manual/reference/Reservoir_Concentration/#table-info","title":"Table Info","text":"<ul> <li>Identifier: NAME</li> <li>Include Block: QUAL_TIME_SERIES</li> </ul> <p>Multiple sources and sinks can be assigned to a reservoir. They are often kept separate to assign different concentrations to them.</p>"},{"location":"manual/reference/Reservoir_Initial_Condition/","title":"Reservoir Initial Condition","text":""},{"location":"manual/reference/Reservoir_Initial_Condition/#overview","title":"Overview","text":"<p>HYDRO requires a water surface initial condition at reservoirs. The Reservoir IC view allows the user to specify default initial conditions. The default initial condition is required but will be overridden if a restart file is used.</p>"},{"location":"manual/reference/Reservoir_Initial_Condition/#tables","title":"Tables","text":""},{"location":"manual/reference/Reservoir_Initial_Condition/#example","title":"Example","text":"<pre><code># Description:\n# Initial Condition of Clifton Court Forebay\nRESERVOIR_IC\nRES_NAME STAGE\nclifton_court 5.000\nEND\n</code></pre> <p>The RESERVOIR_IC table specifies initial water surface elevations for reservoirs.</p>"},{"location":"manual/reference/Reservoir_Initial_Condition/#field-descriptions","title":"Field Descriptions","text":"<ul> <li>RES_NAME: Name of the reservoir where the initial condition is to be applied.</li> <li>STAGE: Initial water surface elevation.</li> </ul>"},{"location":"manual/reference/Reservoir_Initial_Condition/#table-info","title":"Table Info","text":"<ul> <li>Identifier: NAME</li> <li>Include Block: INITIAL_CONDITION</li> </ul> <p>Default initial values are replaced if a restart file is used. Consistency should be maintained between initial reservoir-channel stage differences and flows at nodes. If the reservoir is assigned a different initial stage than surrounding channels, the head difference implies a flow described by the reservoir equations. Unless carefully balanced, the initial time step may have a mass imbalance. To avoid this, set the reservoir stage equal to surrounding channels and make the initial flow zero.</p>"},{"location":"manual/reference/Scalar/","title":"Scalar","text":""},{"location":"manual/reference/Scalar/#overview","title":"Overview","text":"<p>Scalars are scalar model variables used to specify model-wide numerical properties and echoed output levels. They are the equivalent of the text input SCALAR section. All of the parameters are interpreted as text and can be replaced by ENVVARS.</p>"},{"location":"manual/reference/Scalar/#tables","title":"Tables","text":""},{"location":"manual/reference/Scalar/#example","title":"Example","text":"<pre><code>SCALAR\nNAME VALUE\nbinary_output false\ncheckdata false\ncont_bad false\ncont_missing true\nEND\n</code></pre> <p>The SCALAR table comprises name-value pairs for scalars. The scalars that are allowed depend on the specific model.</p>"},{"location":"manual/reference/Scalar/#field-descriptions","title":"Field Descriptions","text":"<ul> <li>NAME: Name of the parameter. This is the identifier of the parameter.</li> <li>VALUE: Value assigned to the parameter. These are interpreted by the model first as text (to allow substitution using ENVVARS) and then converted to the correct data type and validated. For boolean (true/false), one letter is sufficient.</li> </ul>"},{"location":"manual/reference/Scalar/#table-info","title":"Table Info","text":"<ul> <li>Identifier: NAME</li> <li>Parent Table: Table is parent</li> <li>Include Block: PARAMETER</li> </ul> <p>Generally, you will work with the standard parameters distributed with DSM2. You always have to provide <code>RUN_START_DATE</code> and <code>RUN_END_DATE</code> as the defaults are deliberately designed to halt the model.</p>"},{"location":"manual/reference/Source_Flow/","title":"Source Flow","text":""},{"location":"manual/reference/Source_Flow/#overview","title":"Overview","text":"<p>Source flows represent inflows and outflows in the interior of the model domain at nodes. An entry here creates a source and assigns a time series of in/outflows to it.</p>"},{"location":"manual/reference/Source_Flow/#tables","title":"Tables","text":""},{"location":"manual/reference/Source_Flow/#example","title":"Example","text":"<pre><code># Description:\n# Historical source flow at Tracy Pump\nSOURCE_FLOW\nNAME NODE SIGN FILLIN FILE PATH\ncvp 181 -1 last ${BNDRYINPUT} /FILL+CHAN/CHDMC004/FLOW-EXPORT//1DAY/${HISTFLOWVERSION}/\nEND\n</code></pre> <p>The node SOURCE_FLOW table defines sources and sinks by giving them names and associating them to a node. The table also assigns a time series to the source.</p>"},{"location":"manual/reference/Source_Flow/#field-descriptions","title":"Field Descriptions","text":"<ul> <li>NAME: Name assigned to the source. This is the identifier of the boundary and is referred to elsewhere in the input system. If you assign water quality, you will use the same name to match concentration to flow.</li> <li>NODE: Node number at which the source is applied.</li> <li>SIGN: Forces the time series to be a source or a sink. Positive values are normally associated with a source, but the data (especially sinks such as agricultural diversions) are sometimes measured in absolute flow. Use <code>1</code> to force the value to be a positive source or <code>-1</code> to interpret values as a sink.</li> <li>FILLIN: Method for filling in data if the time step of the assigned series is coarser than the time step of the model. See fillin types.</li> <li>FILE: DSS or text file in which data are stored. Use consistent case when referring to the same file. You may also enter the word <code>constant</code> if you would like to assign a constant value to the input (the value will be entered in the next column).</li> <li>PATH: The path within the text or DSS file of the time series data. If you used the <code>constant</code> keyword in the Input File column, enter the value (e.g., <code>4.22</code>) here.</li> </ul>"},{"location":"manual/reference/Source_Flow/#table-info","title":"Table Info","text":"<ul> <li>Identifier: NAME</li> </ul> <p>Multiple sources and sinks can be assigned to a node. They are usually kept separate in order to assign different concentrations to them. HYDRO is able to accept sources and sinks at boundary nodes, but this is not good modeling practice. Use them on the interior.</p>"},{"location":"manual/reference/Source_Flow_Reservoir/","title":"Source Flow Reservoir","text":""},{"location":"manual/reference/Source_Flow_Reservoir/#overview","title":"Overview","text":"<p>Reservoir source flows represent mass inflows and outflows in the interior of the model domain at reservoirs. An entry here creates a source and assigns a time series of in/outflows to it.</p>"},{"location":"manual/reference/Source_Flow_Reservoir/#tables","title":"Tables","text":""},{"location":"manual/reference/Source_Flow_Reservoir/#example","title":"Example","text":"<pre><code># Description:\n# Historical source flow at Bank Pump\nSOURCE_FLOW_RESERVOIR\nNAME RES_NAME SIGN FILLIN FILE PATH\nswp clifton_court -1 last ../../timeseries/hist_19902012.dss /FILL+CHAN/CHSWP003/FLOW-EXPORT//1DAY/DWR-DMS-201203/\nEND\n</code></pre> <p>The SOURCE_FLOW_RESERVOIR table defines sources and sinks by giving them names and associating them to a reservoir. The table also assigns a time series to the source.</p>"},{"location":"manual/reference/Source_Flow_Reservoir/#field-descriptions","title":"Field Descriptions","text":"<ul> <li>NAME: Name assigned to the source. This is the identifier of the boundary and is referred to elsewhere in the input system. If you assign water quality, you will use the same name to match concentration to flow.</li> <li>RES_NAME: Name of the reservoir at which the source is applied.</li> <li>SIGN: Forces the time series to be a source or a sink. Positive values are normally associated with a source, but the data (especially sinks such as agricultural diversions) are sometimes measured in absolute flow. Use <code>1</code> to force the value to be a positive source or <code>-1</code> to interpret values as a sink.</li> <li>FILLIN: Method for filling in data if the time step of the assigned series is coarser than the time step of the model. See fillin types.</li> <li>FILE: DSS or text file in which data are stored. Use consistent case when referring to the same file. You may also enter the word <code>constant</code> if you would like to assign a constant value to the input (the value will be entered in the next column).</li> <li>PATH: The path within the text or DSS file of the time series data. If you used the <code>constant</code> keyword in the Input File column, enter the value (e.g., <code>4.22</code>) here.</li> </ul>"},{"location":"manual/reference/Source_Flow_Reservoir/#table-info","title":"Table Info","text":"<ul> <li>Identifier: NAME</li> </ul> <p>Multiple sources and sinks can be assigned to a reservoir. They are usually kept separate in order to assign different concentrations to them.</p>"},{"location":"manual/reference/Source_Tracking/","title":"Source Tracking","text":""},{"location":"manual/reference/Source_Tracking/#overview","title":"Overview","text":"<p>Source Tracking is implemented using QUAL output.</p>"},{"location":"manual/reference/Source_Tracking/#tables","title":"Tables","text":"<ul> <li>OUTPUT_CHANNEL_SOURCE_TRACK</li> <li>OUTPUT_RESERVOIR_SOURCE_TRACK</li> </ul>"},{"location":"manual/reference/Tidefile/","title":"Tidefile","text":""},{"location":"manual/reference/Tidefile/#overview","title":"Overview","text":"<p>The tidefile is the HDF5-formatted binary file used to pass flow and geometry data from HYDRO to QUAL and PTM. The tidefile is specified as output by HYDRO in the IO_FILE table. It is specified as input to QUAL and PTM in the TIDEFILE section. Input tidefiles can be specified only in text.</p> <p>Tidefiles can be stacked if desired, but this is an old feature that is now deprecated. Stacking means that the flow simulation can be divided temporally among several HYDRO runs and then the resulting tidefiles used sequentially in QUAL or PTM.</p>"},{"location":"manual/reference/Tidefile/#tables","title":"Tables","text":""},{"location":"manual/reference/Tidefile/#example","title":"Example","text":"<pre><code>TIDEFILE     \nSTART_DATE END_DATE FILENAME   \nruntime    length   ${HYDROTIDEFILE} # begin run to 20JUL  \nEND \n</code></pre> <p>The following example uses one tidefile with an environmental variable for the file name. This is the most common treatment.</p>"},{"location":"manual/reference/Tidefile/#example_1","title":"Example","text":"<pre><code>TIDEFILE     \nSTART_DATE  END_DATE   FILENAME   \nruntime     20JUL1996  hist1.h5      # beginning of run to 20JUL  \n20JUL1996   24JUL1996  hist2.h5   \nlast        length     hist3.h5      # end of previous to end of run  \n01SEP1996   length     ficticious.h5 # no error: will never be opened   \nEND \n</code></pre> <p>This example uses several tidefiles tiled together to cover a longer period. Please let us know if you need this functionality, as it is a holdover from the old \"repeating tide\" days and will probably be deprecated.</p>"},{"location":"manual/reference/Tidefile/#field-descriptions","title":"Field Descriptions","text":"<ul> <li>START_DATE: When to start using the tidefile. Tidefiles must be listed in temporal order. The START_DATE of the first tidefile must fall on or before the start of the run. The START_DATE of subsequent tidefiles must exactly coincide with the END_DATES of preceding tidefiles. There is no associated \"TIME\" part\u2014tidefiles must be matched on calendar days. If a START_DATE is not given or is listed as \"none\", the timestamp in the tidefile will be used for the start. Special keywords:</li> <li><code>runtime</code>: start time in tidefile</li> <li><code>last</code>: use this tidefile when the previous tidefile ends</li> <li><code>none</code>: use default.</li> <li>END_DATE: When to stop using the tidefile. If not given, the tidefile is used until it ends. The END_DATE of the last tidefile must overlap the runtime of the simulation. Note that this can be tricky because the ending time is the time <code>0000</code> of the END_DATE, so you may need another day. Avoid this problem by specifying run dates with standard times (<code>0000</code> instead of military <code>2400</code>). Special keywords:</li> <li><code>length</code>: use all of the tidefile, up until its end</li> <li><code>none</code>: use default.</li> <li>FILENAME: Name of the file. Use upper/lower case consistently because filenames are case-sensitive.</li> </ul>"},{"location":"manual/reference/Tidefile/#table-info","title":"Table Info","text":"<ul> <li>Identifier: FILENAME</li> </ul> <p>ENVVARs are often used for names of files, DSS paths, and parameters that are varied over a study\u2014the substitution will occur at runtime.</p>"},{"location":"manual/reference/Transfer/","title":"Transfer","text":""},{"location":"manual/reference/Transfer/#overview","title":"Overview","text":"<p>Transfers are direct water connections from a reservoir or node to another reservoir or node. Transfers are instantaneous movements of water (and its constituents and particles) without any detailed description of physics or storage. The Transfer View specifies the connectivity of the transfer. A time series must also be listed in the Transfer Time Series View to specify the flow\u2014the default is zero.</p>"},{"location":"manual/reference/Transfer/#tables","title":"Tables","text":""},{"location":"manual/reference/Transfer/#example","title":"Example","text":"<pre><code># Description:\n# Sample transfer from a reservoir to a node\nTRANSFER \nNAME       FROM_OBJ  FROM_IDENTIFIER TO_OBJ TO_IDENTIFIER \ntransfer_1 reservoir res_1           node   6  \nEND\n</code></pre> <p>The Transfer table defines the name and connectivity of the transfer. The flow is a time series input specified in the TRANSFER_TIME_SERIES table.</p>"},{"location":"manual/reference/Transfer/#field-descriptions","title":"Field Descriptions","text":"<ul> <li>NAME: Name of the transfer. This is the identifier of the transfer used in other GUI views.</li> <li>FROM_OBJ: Type (node or reservoir) of the source object.</li> <li>FROM_IDENTIFIER: Identifier (node number or reservoir name) of the source object.</li> <li>TO_OBJ: Type (node or reservoir) of the destination object.</li> <li>TO_IDENTIFIER: Identifier (node number or reservoir name) of the destination object.</li> </ul> <p>In previous versions of DSM2, Transfers were called \"obj2obj\".</p> <p>To complete the specification of a Transfer, a time series or constant flow must be attached to it in the Transfer Time Series table.</p>"},{"location":"manual/reference/eco_ptm/Fish_Release_Inputs/","title":"Fish Release Inputs","text":""},{"location":"manual/reference/eco_ptm/Fish_Release_Inputs/#overview","title":"Overview","text":"<p>This parameter provides particle release information. It defines where, when, and how many particles will be released in a simulation.</p>"},{"location":"manual/reference/eco_ptm/Fish_Release_Inputs/#example","title":"Example","text":"<pre><code>Fish_Release_Inputs\nNumber_Of_Release_Groups:1\nGroup_1\nNodeID  ChannelID/ReservoirName/Obj2objName   Distance       Station_Name  \n332     412                                   0              Sacramento\nRelease_Date    Release_Time    Particle_Number Release_Style\n03/18/2012      1:00            100             Random\n03/18/2012      1:15            100             Random\n03/18/2012      1:30            100             Random\nEnd_Group_1\nEnd_Fish_Release_Inputs\n</code></pre>"},{"location":"manual/reference/eco_ptm/Fish_Release_Inputs/#field-descriptions","title":"Field Descriptions","text":"<p>Number_Of_Release_Groups</p> <p>Description: The number of defined release groups. Fish within a release group are all released from the same location, but they can be released at different times.</p> <p>Example: <code>Number_Of_Release_Groups: 2</code></p> <p>Group_ <p>Description: Defines the release location, timing, and number of fish within release group x</p> <p>Example keyword: <code>Group_1</code></p> <p>Table: Release location (one entry)</p> <p>NodeID</p> <p>Description: The upstream node of the channel where the fish will be released</p> <p>Example: <code>332</code></p> <p>ChannelID/ReservoirName/Obj2objName</p> <p>Description: The channel where the fish will be released</p> <p>Example: <code>412</code></p> <p>Distance</p> <p>Description: Distance (feet) of the station from the upstream node of the channel where the fish will be released</p> <p>Example: <code>20</code></p> <p>Station_Name</p> <p>Description: Unique name used to identify the release location</p> <p>Example: <code>Sacramento</code></p> <p>Table: List of release times and numbers of fish (multiple entries)</p> <p>Release_Date</p> <p>Description: The date to release the fish. This must be after the run_start_date defined in the ECO-PTM configuration file</p> <p>Example: <code>03/18/2012</code></p> <p>Release_Time</p> <p>Description: The time at which to release the fish</p> <p>Example: <code>1:00</code></p> <p>Particle_Number</p> <p>Description: The number of fish to release at this date and time</p> <p>Example: <code>100</code></p> <p>Release_Style</p> <p>Description: How the fish should be distributed when they are released</p> <p>Allowable entries: <code>RANDOM</code></p> <p>\u26a0\ufe0f Warning: <code>Fish_Release_Inputs</code> is only needed for <code>Particle_Type_Inputs</code> set to <code>salmon_particle</code>.</p>"},{"location":"manual/reference/eco_ptm/Overview/","title":"ECO-PTM Behavior Parameters","text":""},{"location":"manual/reference/eco_ptm/Overview/#overview","title":"Overview","text":"<p>To run ECO-PTM simulations, a set of behavior parameters are required as inputs. This page provides an overview of these parameters.</p>"},{"location":"manual/reference/eco_ptm/Overview/#behavior-parameters","title":"Behavior Parameters","text":"<ul> <li> <p>Particle_Type_Inputs   Particle type to perform the simulation.</p> </li> <li> <p>Time_Zone   Time zone where the simulation is performed.</p> </li> <li> <p>Random_Sequence_Inputs   Random seed selection.</p> </li> <li> <p>Travel_Time_Output   Parameters and options for outputting travel time.</p> </li> <li> <p>Fish_Release_Inputs   Description for where and when to release particles.</p> </li> <li> <p>Swim_Inputs   Parameters for swimming behaviors.</p> </li> <li> <p>Routing_Inputs   Parameters for routing behaviors.</p> </li> <li> <p>Survival_Inputs   Parameters for survival behaviors.</p> </li> <li> <p>Output_Options   Options for outputs.</p> </li> </ul> <p>Note: Many parameters are optional. Please refer to the sub-pages for more details.</p>"},{"location":"manual/reference/eco_ptm/Particle_Type_Inputs/","title":"Particle Type Inputs","text":""},{"location":"manual/reference/eco_ptm/Particle_Type_Inputs/#overview","title":"Overview","text":"<p>This parameter indicates the type of particle being modeled. In the current version of the ECO-PTM, only the following particle models are implemented:  - \"Neutrally buoyant particle\"  - Position-oriented particle  - Salmon particle  </p>"},{"location":"manual/reference/eco_ptm/Particle_Type_Inputs/#example","title":"Example","text":"<pre><code>Particle_Type_Inputs\nsalmon_particle\nEnd_Particle_Type_Inputs\n</code></pre> <p>Allowable entries: neutrally_buoyant_particle, position_oriented_particle, salmon_particle</p> <p>\u26a0\ufe0f Warning: This block is required</p>"},{"location":"manual/reference/eco_ptm/Random_Sequence_Inputs/","title":"Random Sequence Inputs","text":""},{"location":"manual/reference/eco_ptm/Random_Sequence_Inputs/#overview","title":"Overview","text":"<p>This parameter specifies whether to use the default random number seed or generate a new random number seed for each run.</p> <ul> <li>The default seed is typically used for debugging or tuning purposes.</li> <li>Using the default seed ensures that certain aspects of the ECO-PTM use the same sequence of pseudorandom values every time the model is run.</li> <li>This makes the model outputs more repeatable and facilitates run-to-run comparisons.</li> </ul>"},{"location":"manual/reference/eco_ptm/Random_Sequence_Inputs/#example","title":"Example","text":"<pre><code>Random_Sequence_Inputs\nUse_New_Random_Seed: No\nEnd_Random_Sequence_Inputs\n</code></pre>"},{"location":"manual/reference/eco_ptm/Random_Sequence_Inputs/#field-descriptions","title":"Field Descriptions","text":"<p>Allowable entries: Use_New_Random_Seed: YES or Use_New_Random_Seed: NO</p> <p>\u26a0\ufe0f Warning: This block is required</p>"},{"location":"manual/reference/eco_ptm/Route_Inputs/","title":"Route Inputs","text":""},{"location":"manual/reference/eco_ptm/Route_Inputs/#overview","title":"Overview","text":"<p>Parameters related to the configuration and reporting of the routing model.</p>"},{"location":"manual/reference/eco_ptm/Route_Inputs/#example","title":"Example","text":"<pre><code>Route_Inputs\nOutput_Path_Entrainment: \nOutput_Path_Flux: \nSpecial_Behaviors\nChannel_Name_Look_Up: (GS,366),(SACUPGS,422),(SACDOWNGS,423),(DCC,365),(SACUPDCC,421),(SACUPSUT,418),(SACDOWNSUT,419),(SUT,379),(STM,383),(SACDOWNSTM,420)\n# Users can modify calculated entrainment rates. 100 means no change, and 150 means calculated_chance * 1.5.\n# For STM &amp; SUT, if pct &lt; 100, pct is added (e.g., 10 means calculated_chance + 0.1).\nNodeID      Class_Name                      Percent_Increase_Decrease\n343         SalmonGSJRouteBehavior          100\n339         SalmonSutterJRouteBehavior      100\n340         SalmonSTMJRouteBehavior         100\n342         SalmonDCCRouteBehavior          100\nEnd_Special_Behaviors\n\nBarriers\nNumber_of_barriers: 1\nBarrier1\nNodeID          ChannelID/ReservoirName/Obj2objName   Distance\n343             366\nDate        Time    On/Off\n1/1/1989    0:00     0\n12/31/2017  21:20    0\nEnd_Barrier1\nEnd_Barriers\n\nDICU_Filter\nFilter_Efficiency: 0.0\nEnd_DICU_Filter\nEnd_Route_Inputs\n</code></pre>"},{"location":"manual/reference/eco_ptm/Route_Inputs/#field-descriptions","title":"Field Descriptions","text":"<p>Output_Path_Entrainment</p> <p>Description: The path to the entrainment output file that records flows and entrainment probabilities for all particles that pass through the key junctions (Steamboat Slough, Sutter Slough, DCC, and Georgiana Slough). Note that Windows path separators () should be used. To specify paths relative to the working directory from which the ECO-PTM was launched, use a period followed by the relative path. Example: <code>Output_Path_Entrainment: .\\output\\entrainment-01-01-2011.csv</code></p> <p>Special_Behaviors</p> <p>Channel_Name_Look_Up</p> <p>Description: Used to specify a lookup table so the routing model can use channel names in place of channel numbers. The lookup is specified using a comma-separated list of (name, channel number) pairs within parentheses. Example: <code>Channel_Name_Look_Up: (GS,366),(SACUPGS,422),(SACDOWNGS,423),(DCC,365),(SACUPDCC,421),(SACUPSUT,418),(SACDOWNSUT,419),(SUT,379),(STM,383),(SACDOWNSTM,420)</code></p> <p>Table: Specify an amount by which to increase or decrease the calculated entrainment probability at a given node</p> <p>NodeID Description: The node at which to increase or decrease entrainment. Example: <code>343</code></p> <p>Class_Name Description: The class name that the adjustment will be applied to. This must exactly match one of the existing route behavior classes in the ECO-PTM package. Allowable entries (case-sensitive): <code>SalmonGSJRouteBehavior</code>, <code>SalmonSutterJRouteBehavior</code>, <code>SalmonSTMJRouteBehavior</code>, or <code>SalmonDCCRouteBehavior</code></p> <p>Percent_Increase_Decrease Description: A multiplier that adjusts the calculated entrainment probability, as follows: If this number is less than 100, the adjusted probability is equal to calculated(Percent_Increase_Decrease/100). If this number is greater than 100, the meaning depends on the junction. In the Georgiana Slough and DCC junctions, adjusted = calculated(Percent_Increase_Decrease/100). In the Steamboat Slough and Sutter Slough junctions, adjusted = calculated+(Percent_Increase_Decrease/100-1). In other words, a value of 150 would result in a relative increase of 50% at Georgiana Slough and DCC but an absolute increase of 50% at Steamboat Slough and Sutter Slough. Example: <code>150</code></p> <p>Barriers</p> <p>Description: Define barriers and a schedule during which the barriers will be active. Barriers are used to alter the coefficients of the critical streakline model, which was originally derived in order to capture the effects of a bio-acoustic fish fence (BAFF). Currently, the barrier is only used for the Georgiana Slough routing model.</p> <p>Number of barriers</p> <p>Description: The number of barriers that will be defined. Example: <code>Number_of_barriers: 1</code></p> <p>Barrier <p>Example keyword: <code>Barrier1</code></p> <p>Table: Location of the barrier</p> <p>NodeID Description: The upstream node of the channel in which the barrier is located. Example: <code>343</code></p> <p>ChannelID/ReservoirName/Obj2objName Description: The channel within which the barrier is located. Example: <code>366</code></p> <p>Distance Description: Not used.</p> <p>Table: Barrier schedule</p> <p>Date Description: The date at which the barrier will change from its previous state to the new state. The range between the first and last dates in the schedule must encompass the complete ECO-PTM scenario. Example: <code>1/1/1989</code></p> <p>Time Description: The time at which the barrier will change from its previous state to the new state. Example: <code>21:20</code></p> <p>On/Off Description: The new state. 0 = off, 1 = on. Allowable entries: <code>0</code> or <code>1</code></p> <p>DICU_Filter</p> <p>Description: Specify whether particles should be allowed to enter Delta Island Consumptive Use (DICU) boundaries (agricultural diversions).</p> <p>Filter_Efficiency</p> <p>Description: Probability that a particle will enter a DICU boundary if that route is chosen. 0.0 means no particles will enter the DICU and 1.0 means there is no restriction on particles entering an agricultural diversion. Example: <code>Filter_Efficiency: 0.0</code></p> <p>\u26a0\ufe0f Warning: This block is optional</p>"},{"location":"manual/reference/eco_ptm/Survival_Inputs/","title":"Survival Inputs","text":""},{"location":"manual/reference/eco_ptm/Survival_Inputs/#overview","title":"Overview","text":"<p>Parameters of the XT survival model for each user-specified survival group (reach).</p>"},{"location":"manual/reference/eco_ptm/Survival_Inputs/#example","title":"Example","text":"<pre><code>Survival_Inputs\nSurvival_Parameters\nOutput_Path: .\\output\\survival-01-16-2013.csv\nNumber_of_Survival_Calculation_Group: 10\n# group 0 in Russ's report from Sacramento to Freeport\nGroup_1\nName: Sac to Fpt\n# station is defined by (channel number, distance from upstream node)\n# the group number for a start station will be used as the key to search for the calculation group \n# Lambda and X in feet, Omega in feet/second\nStart_Station: (412,0)  \nEnd_Station: (415,0)        \nExchangeable_Start_Station: \nLambda      Omega           X\n1208005.29  0.558198472     65616.8\nEnd_Group_1 \n# group 1 in Russ's report from Freeport to Sutter/Steamboat Sloughs\n...\nEnd_Survival_Parameters \nEnd_Survival_Inputs\n</code></pre>"},{"location":"manual/reference/eco_ptm/Survival_Inputs/#field-descriptions","title":"Field Descriptions","text":"<p>Output_Path</p> <p>Description: the path to the survival output file. Note that Windows path separators () should be used. To specify paths relative to the working directory from which the ECO-PTM was launched, use a period followed by the relative path. Example: Output_Path: .\\output\\survival-01-01-2011.csv</p> <p>Number_of_Survival_Calculation_Group</p> <p>Description: the number of defined groups (reaches) that will be used for survival calculations Example: Number_of_Survival_Calculation_Group: 10</p> <p>Group_ <p>Example keyword: Group_1</p> <p>Name</p> <p>Description: the name of the reach. This is used to label the outputs in the output file. Example: Name: Sac to Fpt</p> <p>Start_Station</p> <p>Description: location of the station, specified using a channel number and a distance (feet) from the upstream node of the channel. Only a single start station can be specified, and the start stations must be unique for each reach. Only a single station per channel is allowed. Example: Start_Station: (412,0)</p> <p>End_Station</p> <p>Description: the location(s) of one or more end stations, specified using a channel number and a distance (feet) from the upstream node of the channel. Multiple end stations can be specified, and end stations may be shared with multiple reaches. The end station must be in a different channel from the start station, and only a single station per channel is allowed. Example: End_Station: (365,0), (423,0), (366,0)</p> <p>Exchangeable_Start_Station</p> <p>Description: to accommodate particles that begin traveling through a reach but then subsequently enter another reach before passing one of the original reach\u2019s end stations, the user can define one or more exchangeable start stations for each reach. If a particle passes one of the exchangeable start stations before passing an end station, its survival will be calculated for the new reach instead of the original reach. However, the travel time clock will not be reset when the exchangeable start station is passed, i.e., the travel time will be calculated from the original reach\u2019s start station. Example: Exchangeable_Start_Station: (379,951), (383,0)</p> <p>Table</p> <p>Description: specify the parameter values for the XT survival model</p> <p>Lambda</p> <p>Description: the mean free path (feet) between predator encounters Example: 1208005.29</p> <p>Omega</p> <p>Description: the random encounter velocity (ft/sec) Example: 0.558198472</p> <p>X</p> <p>Description: the length (feet) of the reach Example: 65616.8</p> <p>\u26a0\ufe0f Warning:  This block is optional</p>"},{"location":"manual/reference/eco_ptm/Swim_Inputs/","title":"Swim Inputs","text":""},{"location":"manual/reference/eco_ptm/Swim_Inputs/#overview","title":"Overview","text":"<p>Parameters that determine the swimming behavior of the fish. Some parameters apply to all locations within the Delta, and others vary by user-defined channel groups.</p>"},{"location":"manual/reference/eco_ptm/Swim_Inputs/#example","title":"Example","text":"<pre><code>Swim_Inputs\nSunrise: 6:31\nSunset: 18:39 \n# STST holding parameters\nSTST_Threshold: -10.0\nTidal_Cycles_to_Calculate_Channel_Direction: 2\nConstant_Confusion_Probability: 1.04\nMaximum_Confusion_Probability: 1.0\nConfusion_Probability_Slope: -0.55\nRandom_Access: True\nAccess_Probability: 0.01\nChannel_Groups\nSwimming_Velocities\n# rearing holding in hours\nGroup Name              Constant_Swimming_Velocity      Standard_Deviation_Particles    Standard_Deviation_Times    Rearing_Holding_Mean    Day_time_not_swim_percent\nAll                     -0.60796                                0.53012                             0.03448                     0.0             0.60081\nRiverine                -0.13495                                0.53012                             0.03448                     0.0             0.50269\nTransitional            -0.27595                                0.53012                             0.03448                     0.0             0.72489 \nEnd_Swimming_Velocities \nChannel_List       \nRiverine\n700,701,702,703,410,412,413,414,415,416,417,\nEnd_Riverine\nTransitional\n323,329,\n330,331,332,333,334,335,336,337,338,339,\n340,341,342,343,344,345,346,347,348,\n350,351,352,353,354,355,356,357,358,359,\n360,361,362,363,364,365,366,367,368,369,\n370,371,372,373,374,375,376,377,378,379,\n380,381,382,383,384,385,386,387,388,389,\n390,391,392,393,394,395,396,397,398,399,\n400,401,402,403,404,405,406,407,408,409,\n418,419,\n420,421,422,423,424,426,427,428,429,430,\n549,\n550,552,553,555,556,557,558,559,\nEnd_Transitional\nEnd_Channel_List\nEnd_Channel_Groups\nEnd_Swim_Inputs\n</code></pre>"},{"location":"manual/reference/eco_ptm/Swim_Inputs/#field-descriptions","title":"Field Descriptions","text":"<p>Sunrise</p> <p>Description: time of sunrise; used to define daytime for the probability of diurnal swimming Example: Sunrise: 6:31</p> <p>Sunset</p> <p>Description: time of sunset: used to define daytime for the probability of diurnal swimming Example: Sunset: 18:39</p> <p>STST_Threshold</p> <p>Description: flow velocity threshold at which selective tidal stream transport (STST) holding will be activated (ft/sec). When flow velocity exceeds this threshold, the fish will hold position. Velocity is defined relative to the downstream direction, so a negative value should be used to specify an upstream flow threshold. Example: STST_Threshold: -10.0</p> <p>Tidal_Cycles_to_Calculate_Channel_Direction</p> <p>Description: the number of tidal cycles over which the net flow direction should be calculated when determining the apparent direction of flow in a channel Example: Tidal_Cycles_to_Calculate_Channel_Direction: 2</p> <p>Constant_Confusion_Probability</p> <p>Description: the half-saturation point of the logistic function defining the probability of confusion as a function of the signal to noise ratio of the flow. The half-saturation point is the point at which the function reaches half of its maximum value. Changing this number has the effect of shifting the function left and right. Example: Constant_Confusion_Probability: 1.04</p> <p>Maximum_Confusion_Probability</p> <p>Description: the maximum value of the logistic function defining the probability of confusion as a function of the signal to noise ratio of the flow Example: Maximum_Confusion_Probability: 1.0</p> <p>Confusion_Probability_Slope</p> <p>Description: the growth rate, or steepness, of the logistic function defining the probability of confusion as a function of the signal to noise ratio of the flow Example: Confusion_Probability_Slope: -0.55</p> <p>Random_Access</p> <p>Description: enable or disable random assessment of confusion. Setting this value to FALSE disables the probability of confusion feature. Allowable entries: Random_Access: TRUE or Random_Access: FALSE</p> <p>Access_Probability</p> <p>Description: probability of assessing confusion in a given 15-minute PTM time step Example: Access_Probability: 0.01</p> <p>Channel_Groups</p> <p>Description: specifies user-defined channel groups and swimming parameters that vary by channel group. The channel group name \u201cALL\u201d is a special name that applies to all channels that are not associated with another user-defined channel group.</p> <p>Swimming_Velocities</p> <p>Table: swimming parameters that vary by channel group</p> <p>Group_Name</p> <p>Description: the user-specified name of the channel group Example: Riverine</p> <p>Constant_Swimming_Velocity</p> <p>Description: mean of the distribution from which each particle\u2019s mean swimming velocity (ft/sec) will be drawn from in the channel group. Each channel group has a distribution of mean velocities associated with it. Each particle draws from this distribution to obtain its own unique mean velocity in the channels associated with this group. The actual velocity of the particle in a given time step is then drawn from a velocity distribution defined by the particle\u2019s mean velocity and the Standard_Deviation_Times parameter. Example: -0.60796</p> <p>Standard_Deviation_Particles</p> <p>Description: standard deviation of the distribution from which each particle\u2019s mean swimming velocity (ft/sec) will be drawn from in the channel group Example: 0.53012</p> <p>Standard_Deviation_Times</p> <p>Description: standard deviation of the distribution from which the particle\u2019s swimming speed will be drawn from in each time step Example: 0.03448</p> <p>Rearing_Holding_Mean</p> <p>Description: mean holding time, in hours. The actual holding time for each particle and channel group will be drawn from an exponential distribution with a mean equal to Rearing_Holding_Mean. Example: 24.0</p> <p>Day_time_not_swim_percent</p> <p>Description: the probability that the fish will be inactive during any given time step during the daytime. A value of 0.0 means that the fish will never be inactive (hold) during the day. A value of 1.0 means that fish will always hold during the daytime hours. Example: 0.25</p> <p>Channel_List</p> <p>Description: list of channels associated with each channel group specified in the Swimming_Velocities table. Each channel group will be specified using a separate block. Channels that are not included in a user-defined channel group are automatically included in the ALL group; it is not necessary to explicitly specify the channels in the ALL group.</p> <p> <p>Description: list of channels associated with a channel group Example keyword: Riverine Example entry: 410,412,413,414,415,416,417,</p> <p>STST_Threshold, Constant_Confusion_Probability, Confusion_Probability_Slope, Constant_Swimming_Velocity, Standard_Deviation_Particles, Standard_Deviation_Times, Rearing_Holding_Mean, and Day_time_not_swim_percent are all calibrated from field tag data. They should not be changed unless a new calibration is needed.</p> <p>\u26a0\ufe0f Warning: This block is required</p>"},{"location":"manual/reference/eco_ptm/Time_Zone/","title":"Time Zone","text":""},{"location":"manual/reference/eco_ptm/Time_Zone/#overview","title":"Overview","text":"<p>The <code>Time_Zone</code> parameter defines the time zone where the simulation is performed.</p>"},{"location":"manual/reference/eco_ptm/Time_Zone/#example","title":"Example","text":"<pre><code>Time_Zone\nPST\nEnd_Time_Zone\n</code></pre>"},{"location":"manual/reference/eco_ptm/Time_Zone/#allowable-entries","title":"Allowable entries","text":"<p>Allowable entries: PST (default), MST, CST, or EST</p>"},{"location":"manual/reference/eco_ptm/Time_Zone/#requirement","title":"Requirement","text":"<p>\u26a0\ufe0f Warning: This parameter is required.</p>"},{"location":"manual/reference/eco_ptm/Travel_Time_Output/","title":"Travel Time Output","text":"<p>Owned by: Xiao Wang Last updated: May 19, 2021  </p>"},{"location":"manual/reference/eco_ptm/Travel_Time_Output/#overview","title":"Overview","text":"<p>The <code>Travel_Time_Output</code> parameter provides options for users who need to output travel time information from a specified start location to an end location.  </p> <ul> <li>Users provide a path to the travel time output file.  </li> <li>Note that Windows path separators (<code>\\</code>) should be used.  </li> <li>To specify paths relative to the working directory from which the ECO-PTM was launched, use a period followed by the relative path.</li> </ul>"},{"location":"manual/reference/eco_ptm/Travel_Time_Output/#example","title":"Example","text":"<pre><code>Travel_Time_Output\nOutput_Path: .\\output\\travel_time_in_min-03-18-2012.csv \nNodeID          ChannelID/ReservoirName/Obj2objName   Distance          Station_Name\n339             379                                     951             Sutter\nEnd_Travel_Time_Output\n</code></pre>"},{"location":"manual/reference/eco_ptm/Travel_Time_Output/#field-descriptions","title":"Field Descriptions","text":"<p>Table</p> <p>List of downstream locations, or stations, at which to record the travel time.</p> <p>NodeID</p> <p>Description: Upstream node of the channel containing the station Example: 339</p> <p>ChannelID/ReservoirName/Obj2objName</p> <p>Description: Channel within which the station is located Example: 379</p> <p>Distance</p> <p>Description: Distance (feet) of the station from the upstream node of the channel Example: 951</p> <p>Station_Name</p> <p>Description: Unique name used to identify the station in the travel time output file. These stations will be listed under the \u201cDetect_Sta\u201d header in the output file.</p> <p>\u26a0\ufe0f Warning: Only one travel time output location can be specified</p>"},{"location":"reference/Background_Material_and_References/","title":"Background Material and References","text":"<p>DSM2 Hydro is based on the FourPt computer program written by Lew DeLong in USGS. The original documentation for FourPt is available here. DSM2 adds improvements and enhancements to the FourPt model, including an improved input and output system.</p> <p>DSM2 Qual is roughly based on QUAL-2E  and the Branched Langrangian Transport Model (BLTM) written by Harvey Jacobson of USGS.\u00a0</p>"},{"location":"reference/Background_Material_and_References/#download-dsm2-versions","title":"Download DSM2 Versions","text":"<p>DSM2 has had many versions over the past 30 years. Some of the recent ones are available here</p> <p>Downloads of DSM2 Versions</p>"},{"location":"reference/Background_Material_and_References/#references","title":"References:","text":"<ul> <li>QUAL2E Documentation - Basis for QUAL Nonconservative Constituent Kinetics.pdf</li> <li>D1641rev.pdf</li> <li>EC_chloride_bromide_05_29_01.pdf</li> <li>Delta D1641 Water Quality Standards Full Reference.pdf</li> <li>BLTMenhancements-USGSWRI97_4050.pdf</li> <li>Four Point memo from USGS-basis for DSM2 HYDRO.pdf</li> </ul>"},{"location":"reference/Background_Slide_Material/","title":"Background Slide Material","text":"<p>Download All</p>"},{"location":"reference/Background_Slide_Material/#attachments","title":"Attachments:","text":"<p> IIA.05.Banks pumping.kts.ppt DSM2 reservoirs and gates.ppt DSM2 Delta Applications Intro.ppt DSM2 boundary conditions.ppt DSM2 analysis tools.ppt DeltaTutorial-Planning_PermanentBarriers.doc Tutorial maps.ppt Reservoir and Gate equations.ppt </p>"},{"location":"reference/CALSIM_-_DSM2_Integration/","title":"CALSIM - DSM2 Integration","text":""},{"location":"reference/CALSIM_-_DSM2_Integration/#background","title":"Background","text":"<p>CALSIM is a water operations simulation model. It meets demands using reservoir release operations and other operational criteria. A crucial operational criterion is meeting the salinity and X2 standards in the Delta.</p> <p>CALSIM relies on DSM2 simulation of water quality standards. However DSM2 is computationally expensive to run in repeated scenarios needed by CALSIM. CALSIM relies on a linear programming approach and needs flow salinity relationships to estimate the flow needed to meet a particular water quality standard. Furthermore CALSIM is a monthly model and needs to make assumptions pertaining to that limitation.\u00a0</p>"},{"location":"reference/CALSIM_-_DSM2_Integration/#artificial-neural-networks-anns","title":"Artificial Neural Networks (ANNs)","text":"<p>To make it computationally feasible, the flow salinity relationships are derived from DSM2 simulations with perturbations of inputs that are of concern to CALSIM. These flow relationship information is used as training data for Artificial Neural Networks (ANNs); more specifically Feed-forward Neural Networks (FNNs).\u00a0 These ANNs then are surrogate models for DSM2 and are supposed to represent the impact of operations on X2 and salinity standards.</p>"},{"location":"reference/CALSIM_-_DSM2_Integration/#full-circle-analysis","title":"Full circle analysis","text":"<p>To verify the results derived from having a surrogate DSM2 (ANN) model in CALSIM, the CALSIM flows and gate conditions are converted into daily inputs (with assumptions for monthly to daily) for DSM2 and the output salinity is checked against the X2 or salinity standards in CALSIM. This is called a \"full circle analysis\".\u00a0 Typically these have been done for a select period of 16 years but can be extended to the entire period of 82 years of simulation if desired.</p>"},{"location":"reference/CALSIM_-_DSM2_Integration/#dsm2-boundary-conditions","title":"DSM2 boundary conditions","text":"<p>DSM2 needs flow and stage boundary conditions, i.e. the inputs at the edges of the domain that would drive the simulation.</p> <ol> <li>Flow boundaries: CALSIM operates the reservoirs upstream of the     Delta and as a result the flow conditions are established by CALSIM     simulations, though on a monthly time step resolution.</li> <li>Gate positions:\u00a0 CALSIM operates these to satisfy regulations and     other constraints.\u00a0</li> <li>Stage boundary: The only one is the ocean boundary at Martinez that     is derived from astronomical stage at San Francisco with regression     using historical data to transfer to Martinez (Planning tide     generator)</li> <li>Martinez EC boundary: This is derived from a flow salinity     relationship based on G model and stage boundaries (Planning     Martinez EC generator)</li> <li>Vernalis EC boundary: Derived from flow regression equations.</li> <li>Consumptive Use: These are represented in DSM2 at 258 nodes, CALSIM     does not directly simulate these, however they are provided as input     to CALSIM based on consumptive use models</li> <li>Agricultural Drain EC: These are the most uncertain of the boundary     conditions and are represented in DSM2 as annually repeating values.</li> <li>Waste water treatment plants ??</li> </ol>"},{"location":"reference/CALSIM_-_DSM2_Integration/#implementation","title":"Implementation","text":"<p>These boundary conditions are explicitly mapped in this document between the CALSIM and DSM2 schematics.\u00a0Schematics and Boundaries</p>"},{"location":"reference/CALSIM_-_DSM2_Integration/#resolving-monthly-daily-conversions","title":"Resolving Monthly - Daily conversions","text":"<p>CALSIM is a monthly time step model and DSM2 runs on 15 min or lower time steps. The input data for CALSIM is monthly averaged i.e. a single value for the entire month. DSM2 typically takes daily input values and is also capable of hourly or sub hourly resolved values.\u00a0 This mismatch has to be resolved when doing this integration.</p> <p>For daily to monthly conversions, it is simply a monthly averaging technique. For certain quantities, such as gate positions, a count of values may be computed ?</p> <p>For monthly to daily conversions, there is huge impedance. This means a lot of information that is lost has to be either estimated or left as the same value repeated over the days of the month. This is usually the case for the flows, except that for stability reasons ( hydrodynamic models ) the transition days between months employ a volume conserving spline to smooth the transition.\u00a0</p> <ul> <li>Discuss daily variation issue here</li> </ul>"},{"location":"reference/CALSIM_-_DSM2_Integration/#version-control","title":"Version Control","text":"<p>CALSIM and DSM2 have different versions, evolving at different rates for different needs. As a result is important to manage these versions and the mappings between them.\u00a0Draft_CALSIMII_DCU_Modification_081809</p> <ul> <li>What if CALSIM schematic changes? \u00a0Implication for the integration     above?</li> </ul>"},{"location":"reference/CALSIM_-_DSM2_Integration/#notes","title":"Notes","text":"<p>Martinez stage has been adjusted a little bit on 24DEC1967 to overcome a dry-up breakdown at channel 201. The correction reside in a timeseries\u00a0${DSM2}\\timeseries\\Planning_Tide_82years.dss. Planning study users should add it to replace the regular timeseries.</p>"},{"location":"reference/CALSIM_-_DSM2_Integration/#attachments","title":"Attachments:","text":"<p> bat_prep.png (image/png)  </p>"},{"location":"reference/Checklists/","title":"Checklists","text":"<p>Checklists are a great way to codify quality checks to avoid obvious and typical errors for a task.\u00a0</p> <p>Historical simulations</p> <p>Bob Suits put together a checklist used for historical simulation updates.\u00a0</p> <p>Historical Simulation Checklist</p>"},{"location":"reference/Checklists/#planning-simulations","title":"Planning simulations","text":"<ul> <li>Need a similar checklist for planning runs\u00a0Yu (Joey) Zhou ?\u00a0</li> </ul>"},{"location":"reference/Checklists/#attachments","title":"Attachments:","text":"<p> verifying_historical_simulation_101620.pdf (application/pdf)  </p>"},{"location":"reference/DSM2_Cloud_Setup/","title":"DSM2 Cloud Setup","text":"<p>DSM2 has been compiled on Linux and tested against the Windows results. The output from hydro and qual is similar thought not exact (floating point level differences)</p> <p>AWS (Amazon Web Services) Cloud services have been used to run DSM2 using AWS Linux AMI (Amazon Machine Image). This requires a user to start linux VM and then download and run DSM2 on that VM.</p> <p>Docker install on AWS Linux</p> <p>A serverless approach to this would be that the user submits a batch job consisting of a specification of what container (Docker) to be used and a zip file with the inputs. The batch job is then run on a suitable machine and the resulting output file is zipped and uploaded to S3 (AWS Simple Storage System)</p> <p>The serverless approach allows for submission of multiple concurrent jobs that provide the ability to do many parallel runs at the same time. The charges are on persecond basis making efficient use of computing resources.</p> <p>The use of the cloud to run a batch DSM2 PTM is here:\u00a0How to Run a DCP PTM Batch Job on AWS</p>"},{"location":"reference/Historical_Simulation_Checklist/","title":"Historical Simulation Checklist","text":""},{"location":"reference/Historical_Simulation_Checklist/#verifying-an-extension-of-the-historical-simulation","title":"Verifying an Extension of the Historical Simulation","text":"<p>Bob Suits 10/16/2020</p>"},{"location":"reference/Historical_Simulation_Checklist/#verify-input-hydrology","title":"Verify Input Hydrology","text":""},{"location":"reference/Historical_Simulation_Checklist/#check-boundary-conditions","title":"Check Boundary Conditions","text":"<ul> <li>Sacramento River inflow</li> <li>San Joaquin River Inflow</li> <li>Sacramento River + Yolo Bypass Inflow</li> <li>Banks pumping</li> <li>Jones pumping</li> </ul>"},{"location":"reference/Historical_Simulation_Checklist/#get-observed-data","title":"Get Observed Data","text":"<p>Preferably get the Sacramento and San Joaquin River inflows and Banks and Jones pumping from DAYFLOW. If DAYFLOW isn\u2019t complete, get remainder daily average flow from CDEC. It needs to be independent of the DSM2 set-up.</p> <p>Get reported flow at SRV (Rio Vista) and generate daily average flow.</p>"},{"location":"reference/Historical_Simulation_Checklist/#generate-daily-average-flow-from-dsm2-simulation-at","title":"Generate daily average flow from DSM2 simulation at:","text":"<p>VCU, ORI, OH4, OBD, GLC, RSAC101, RSAN115, RSAC155</p>"},{"location":"reference/Historical_Simulation_Checklist/#compare-daily-average-observed-flows-to-dsm2-simulated-flows-at-boundaries","title":"Compare daily average observed flows to DSM2-simulated flows at boundaries","text":"Flow CDEC Stations Operating Agency Simulated Sac River Inflow FPT USGS RSAC155 SJR Inflow RSAN112 SRV RSAC101 Banks Pumping VCU + ORI \u2013 OH4 Jones Pumping OBD + GLC \u2013 ORI Banks + Jones VCU + OBD + GLC \u2013 OH4"},{"location":"reference/Historical_Simulation_Checklist/#verify-timing-of-installation-and-removal-of-temporary-barriers-and-operation-of-montezuma-control-structure-and-delta-cross-channel-gates","title":"Verify timing of installation and removal of temporary barriers and operation of Montezuma Control Structure and Delta Cross Channel Gates","text":"<ol> <li> <p>Create a dss file with the observed and simulated 15-minute data.     Compare observed and simulated stages just upstream and downstream     of each barrier site. This would already have been done with     observed data in establishing the timings by looking at observed     stages. Now repeat the analysis in order to confirm that you got the     operation timing correct.</p> Barrier Stations to use to check \u00a0Middle River \u200bMUP and (MAB or MTB) Grant Line Canal GCT and GLE Old River OBD and (OAD or ODM) Old River at Head OH1 and SJL </li> <li> <p>Compare internal daily average flows affected by gate operations</p> Observed Simulated DLC DLC Delta Cross Channel NSL SLMZU025 Montezuma Slough at National Steel </li> <li> <p>Check key internal flows for overall circulation of Delta waters</p> Observed Simulated GSS GSS TRN TRN OBI OBI (ROLD024) MDM \u00a0Subtract RMID015-145 from RMID015-144 OH4 OH4 (ROLD034) VCU VCU OH1 OH1 OLD OLD GLE GLE </li> </ol>"},{"location":"reference/Historical_Simulation_Checklist/#verifying-ec","title":"Verifying EC","text":"<p>Get OCO\u2019s monthly updated EC estimates at: Banks, Jones, OH4, OBI to compare to Delta Modeling Section\u2019s historical simulation and reported EC.</p> Observed\u00a0 Simulated BANKS BANKS JONES JONES OBI ROLD024 OH4 ROLD034 <p>Compare observed EC to simulated EC at other key locations</p> Observed\u00a0 Simulated ANH\u00a0 ANH (RSAN007) EMM\u00a0 EMM (RSAC092) JER\u00a0 RSAN018 MDM\u00a0 RMID015 VCU\u00a0 CHVCT000 OH1\u00a0 ROLD074 OLD\u00a0 ROLD059"},{"location":"reference/Historical_Simulation_Checklist/#attachments","title":"Attachments:","text":"<p> worddavdba6ec7d80d8e6d0f248fa3e9c1a9f2c.png (image/png) worddav1c660f167bf01e13f8c74d8d923445b8.png (image/png) worddavc1eb73f1cc8b69c78afeeda15ca65f9f.png (image/png) worddavffb24ad6008690b469796d092cb3e822.png (image/png) worddav7e51b7bfbf042cd6018456316774c8ac.png (image/png) worddav46b8e2022c959a0ec4979c9f7a8a9b76.png (image/png) worddav096303e2931d512411acda589b39c3cb.png (image/png) worddavac6c9b93520fd08679d0ccfdc60b7b66.png (image/png) worddav417450fd7fccb9d246170ebb7f24d040.png (image/png) worddava898e0b2ae4581b97e766bf78287b5c1.png (image/png) worddav68b2cbedb7a858f3c9b7f1a1e3ab74ac.png (image/png) worddav5b3d61166388b0d6b2f355755006cda2.png (image/png) worddav8c7ed9ee05ac17e5e96e011f3f2618cd.png (image/png) worddav1767755d061f0eab1f3ca3f72ac8cef2.png (image/png) worddavf80c76565c2ad18b4d77a1e3e7d95206.png (image/png) worddavcbc13ea92d122ff1199746fedcb5d095.png (image/png) worddavbec1a14c56fb496c1411aa3f37418f8f.png (image/png) worddav7b88059002b010dabef7c4b1535bb124.png (image/png) worddav5bb8f683823e3bf73c38767313d66e71.png (image/png) worddavc305fa1a938290814fd2248549ba4430.png (image/png) worddavdfbad699bc5178a523225eb4db224138.png (image/png) worddav4ea5b554fecf90d7309db37a565d3a27.png (image/png) worddav97cef626ad442220331e25e5e8e8107f.png (image/png) worddave19a27a3183a9cd911129f634e5785e1.png (image/png) worddavcafee77d96b6996a2ba1b263d685acf2.png (image/png) worddav586bb64e084e848f881c6b85898711d9.png (image/png) worddav06a87c364207da504d16cfe87f616b71.png (image/png) worddavfed7fe74c73acf86c9270728c405a08c.png (image/png) worddav1b9bd5a7d0e6c5fe479a6a2f41178b3d.png (image/png) worddav252bc8a83f17f3aaf5122fc24dc34a7b.png (image/png) worddavaefc8f4547e26bc30a4e1e2e6d9649e4.png (image/png) worddav43dc9470b95f7adadead62075f065c27.png (image/png) worddav44b58de538e094cc6f56fa252e0b3775.png (image/png) worddavf81ccfffae04537644dbc328afa65327.png (image/png) worddavfb16076aa13e127fd8eb1d3b049dec6e.png (image/png) worddav9a6b20dc0a73de6bf9dab0456647125f.png (image/png) worddav230f2ffdb48f91fc3c31cb101e946de0.png (image/png) worddave2ddaa02d082a0f4038f1aa1cae93a47.png (image/png) worddav41ab2bbaa193e682b05098057d919df2.png (image/png) worddavcb34498bae50e99b02ddf21594a946c0.png (image/png) worddav248f48ce932b6488517e2109896fdb56.png (image/png) worddave25679296b7bc483b7fb1156d722e8b6.png (image/png) worddav44d59b8b5a711866ad8dde94f0d42d27.png (image/png) worddav660ebda068b8ef166732e0aef200cc6e.png (image/png)</p>"},{"location":"reference/Organizing_a_Study/","title":"Organizing a Study","text":""},{"location":"reference/Organizing_a_Study/#overview","title":"Overview","text":"<p>The DSM2 installation directory is as follows:</p> <pre><code>/dsm2\n  /bin            # where the executable files are\n  /common_input   # a repository of common input files\n  /studies\n     [empty]      # DO YOUR WORK HERE (if inside the distribution)\n  /study_templates    # ...and NOT here\n     /historical\n     /ocap_sdip\n        config_ocap_sdip.inp\n        hydro.inp\n        qual_ec.inp\n        /timeseries\n  /timeseries     # Archivable time series (historical\n                  # and series used as basis for preprocessing)\n  /tutorials\n</code></pre>"},{"location":"reference/Organizing_a_Study/#explaining-the-directory-structure","title":"Explaining the directory structure","text":""},{"location":"reference/Organizing_a_Study/#bin","title":"bin","text":"<p>\"Bin\" stands for \"binary\" and refers to the executables. When you installed dsm2, your path variable got set to the new distribution.</p>"},{"location":"reference/Organizing_a_Study/#common_input","title":"common_input","text":"<p>The common input directory is a repository of files that are used in the standard templates. These files start with the name of the (parent) object, then the \"layer\" name then the version date. The templates refer to this directory often. You do not have to maintain these links, but please do not edit the files... the\u00a0Layering\u00a0system should help with this.</p>"},{"location":"reference/Organizing_a_Study/#study_templates","title":"study_templates","text":"<p>This directory houses samples for historical and planning runs. They represent our latest setup as of the distribution. As the templates are updated they may point to newer files in\u00a0common_input.</p>"},{"location":"reference/Organizing_a_Study/#studies","title":"studies","text":"<p>A study is an associated group of simulations, which might involve any combination of DSM2 modules. Often the study compares several different alternatives. Many DSM2 modelers prefer to house different alternatives in different folders, but there are good reasons to house them in one study folder and just use different configuration files. To get started you will typically copy one of the study_template sub-directories to the /study folder. Don't change the ones in study_templates!</p>"},{"location":"reference/Organizing_a_Study/#timeseries","title":"timeseries","text":"<p>The\u00a0timeseries\u00a0directory contains the timeseries you will need in the regular course of working with DSM2. Since the data that are most reusable are historical and DICU, that is most of what you will find here. We don't recommend putting study-specific files (e.g. CALSIM output) in this directory.</p>"},{"location":"reference/Organizing_a_Study/#tutorials","title":"tutorials","text":"<p>The\u00a0tutorials\u00a0directory is a workspace for using the tutorials. It is a lot like /studies in the sense that you will copy templates here.</p>"},{"location":"reference/Planning_Simulation_Checklist/","title":"Planning Simulation Checklist","text":""},{"location":"reference/Planning_Simulation_Checklist/#model-preparation","title":"Model Preparation","text":"<p>Get Input Data from Calsim</p> <ul> <li>Rename Calsim output as dv.dss and put in timeseries\\CALSIM\\</li> <li>Preprocess Calsim output to DSM2 inputs</li> </ul> <p>Check Boundary Conditions</p> <ul> <li>Sacramento River inflow</li> <li>San Joaquin River Inflow</li> <li>Sacramento River + Yolo Bypass Inflow</li> <li>Banks pumping</li> <li>Jones pumping</li> </ul> <p>Martinez Stage</p> <ul> <li>Planning stage</li> <li>Sea Level Rise stage ()</li> </ul> <p>Martinez EC</p> <ul> <li>Martinez EC generator</li> </ul> <p>Consumptive Usage</p> <ul> <li>DCD planning (which is also input for Calsim3)</li> </ul> <p>Gate Operation</p> <ul> <li>DCC</li> <li>Montezuma Gate</li> <li>Clifton Court Forebay Gate</li> </ul>"},{"location":"reference/Planning_Simulation_Checklist/#model-run","title":"Model Run","text":"<p>Binary Versions</p> <ul> <li>suggest using relative path and .bat</li> </ul> <p>Running Time Window</p> <ul> <li>16-year: 1975/10 - 1991/9</li> <li>94-year:\u00a01975/10 - 1991/9</li> <li>start a few months earlier for warming-up</li> </ul>"},{"location":"reference/Planning_Simulation_Checklist/#postprocess-visualization-usage","title":"Postprocess, Visualization, Usage","text":"<p>Notebook</p> <p>Water Quality Standard</p>"},{"location":"reference/Post_processing_and_Visualization/","title":"Post processing and Visualization","text":"<p>DSM2 writes out model information in HEC-DSS format and HDF5 format. Output requested from the model is written to HEC-DSS while model input and state is recorded in HDF5 files.</p> <p>Vista is the standard tool for accessing both these kinds of information for DSS. Vscript is the associated scripting tool which leverages the Python language with the HEC-DSS and HDF5 java libraries.</p>"},{"location":"reference/Presentations/","title":"Presentations","text":""},{"location":"reference/Presentations/#introduction","title":"Introduction","text":""},{"location":"reference/Presentations/#installation","title":"Installation","text":""},{"location":"reference/Presentations/#attachments","title":"Attachments:","text":"<ul> <li>Cool Tips.ppt</li> <li>PTM_2_LowPumpingTempBar.avi</li> <li>p0_template.pptx</li> <li>or2_sdip_oprules.ppt</li> <li>or2_sdip_op.ppt</li> <li>or1_op_rule.ppt</li> <li>DSM2V8 agenda 900 class.docx</li> <li>DSM2V8 agenda 830 class.docx</li> <li>DSM2_class_list_900.docx</li> <li>DSM2_class_list_830.docx</li> <li>DSM2 Version 8 Class Syllabus.docx</li> <li>d2_ptm.pptx</li> <li>d1b_nonconservative.pptx</li> <li>d1_historical.pptx</li> <li>d0_delta_applications.ppt</li> <li>b6_op_rule.ppt</li> <li>b5_source_tracking.ppt</li> <li>b4_simulation_data.pptx</li> <li>b3_input_system_layering.pptx</li> <li>b2_reservoir_gate_transfer.ppt</li> <li>b1_channel.pptx</li> <li>a4_dsm2_version8.ppt</li> <li>a3_dsm2_user_group.pptx</li> <li>a2_installation.pptx</li> <li>a1_introduction.pptx</li> <li>d4_move_archive_batch.ppt</li> <li>d3_planning.ppt</li> </ul>"},{"location":"reference/RKI_Referenced_Output/","title":"RKI Referenced Output","text":"<p>River Kilometer Index is a way to index locations along a river by measuring the distance in kilometers from the downstream end of the river, e.g. RSAC054 is 54 km from the Golden gate bridge, the most downstream discernible reach of the Sacramento River</p> <p>In light of current GIS information this practice should be superseded by exact latitude,longitude coordinates, however it is resilient in its use due to legacy reasons. It is also useful in terms of physical processes which are related to the distance along the river rather than coordinates in a general space</p> <p>The table below comes from DSM2 v6 (last referenced by J. Anderson)\u00a0</p> NAME CHAN DIST COMMON_NAME BYOLO040 399 0 # Yolo Bypass\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CFTRN000 172 727 # Turner Cut\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CHCCC006 247 0 # Contra Costa Pumping Plant / Rock Slough\u00a0\u00a0\u00a0\u00a0\u00a0 CHDMC004 216 2000 # DMC\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CHDMC006 216 0 # DMC @ Tracy Pumping Plant\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CHGRL005 211 1585 # Grant Line Canal (West Position)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CHGRL009 207 36 # Grant Line Canal (East Position)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CHGRL012 204 1672 # Grant Line Canal @ Head\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CHSAC030 392 23614 # Sacto. Ship Channel\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CHSAC031 392 20661 # Sacto. Ship Channel\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CHSWP003 82 length # Clifton Court Forebay (gates)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CHVCT000 229 1328 # Victoria Canal\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CHWST000 232 3084 # Clifton Court Forebay Entrance\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CRGRV002 455 0 # Green Valley Creek\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CRSUS004 457 0 # Suisun Creek @ Cordelia Rd.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 LSHL001 281 113 # Sac. @ Lake Sherman\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 LSHL003 299 5145 # SJR @ Mayberry Cut\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RCAL009 21 0 # Calaveras River at Stockton\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RCSM075 549 2501 # Cosumnes River\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RFAL008 276 5648 # FALSE River @ Webb Tract\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RMID005 156 140 # Middle River\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RMID007 248 665 # Middle River\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RMID015_144 144 838 # Middle River\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RMID015_145 145 2114 # Middle River (same as #144)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RMID023 135 719 # Middle River @ Borden Hwy\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RMID027 133 3641 # Middle River @ Tracy Blvd\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RMID040 126 3951 # Middle River @ Mowery Bridge\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RMID041 125 1700 # Middle River @ Old River\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RMKL005 374 5030 # North Fork Moke. River (Georgiana Sl.)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RMKL019 357 694 # North Fork Moke. River\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RMKL027 334 350 # Moke. River @ Thornton\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RMKL032 550 2617 # Moke. River near Thornton\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RMKL070 550 0 # Moke. River @ Woodbridge\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ROLD014 117 0 # Old River @ Holland Cut\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ROLD024 106 2718 # Old River @ Bacon Island (near CCC)\u00a0\u00a0\u00a0\u00a0\u00a0 ROLD034 90 3021 # Old River near Byron\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ROLD040 82 2609 # Old River @ Clifton Court Ferry\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ROLD046 80 1431 # Old River\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ROLD047 79 2766 # Old River\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ROLD059 71 3116 # Old River @ Tracy Road\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ROLD074 54 735 # Old River @ Head\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAC054 441 length # Martinez (MRZ)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAC056 441 3119 # Martinez at Benicia Bridge\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAC064 452 190 # Port Chicago\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAC075 437 11108 # Mallard Island (MAL)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAC077 437 1870 # Pittsburg (PTB)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAC081 436 5733 # Collinsville\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAC084 435 9662 # Sac River near Sherman Lake\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAC092 434 435 # Emmaton\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAC101 430 9684 # Rio Vista (RIV)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAC123 423 1358 # Sac near Georgiana Slough\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAC128 421 8585 # Sac above DCC\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAC139 418 4814 # Sac @ Green's Landing\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAC140 418 0 # Sac @ Snodgrass Sl.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAC142 417 5496 # Sac @ Hood\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAC155 414 11921 # Sac @ Freeport\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAN002 53 4276 # Mouth SJR\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAN003 285 1700 # SJR @ Sherman Lake\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAN007 52 366 # SJR @ Antioch\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAN008 52 0 # Lone Tree Way @ Hwy. 4 near Antioch\u00a0\u00a0\u00a0\u00a0\u00a0 RSAN014 49 9570 # Blind Point\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAN018 83 4213 # Jersey Point\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAN024 47 8246 # SJR @ Bradford Isl.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAN032 349 9672 # San Andreas Landing, NOTE: RSAN032 water comes from Moke. R, hence the model location is on Moke. MM, 2000.09.06\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAN037 42 286 # SJR @\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAN040 38 3526 # SJR\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAN043 319 8571 # SJR @ Venice Isl.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAN046 31 5628 # SJR between Turner &amp; Columbia Cut\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAN052 24 2643 # SJR @ Rindge Pump\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAN058 20 2520 # SJR @ Stockton Ship Channel\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAN063 14 3281 # SJR @ Stockton\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAN072 10 9400 # SJR @ Brandt Bridge\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAN087 6 3930 # SJR @ Mossdale\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSAN112 17 4744 # SRJ @ Vernalis\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSMKL008 344 7088 # South Fork Moke @ Staten Island\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 RSMKL024 337 971 # South Fork Moke @ New Hope Bridge\u00a0\u00a0\u00a0\u00a0\u00a0 SLBAR002 406 0 # Barker Slough / North Bay Aqueduct\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLCBN001 477 4000 # Chadbourne Sl. (Hollywood Club)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLCBN002 477 0 # Chadbourne Sl. (Sunrise Club)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLCCH016 402 0 # Cache Slough\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLCRD000 471 7216 # Cordelia Sl. (Miramonte)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLCRD003 474 3754 # Cordelia Sl. (Cygnus)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLCRD006 469 4776 # Cordelia Sl. (Ibis)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLCRD009 468 11200 # Cordelia Sl. (Garibaldi)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLDUT007 274 7351 # Dutch Sl.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLDUT009 273 4026 # Dutch Sl.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLFHN002 479 2640 # Frank Horan Sl.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLGYR003 501 0 # Goodyear Sl. (Morrow Island)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLGYR008 473 1955 # Goodyear Sl. (Ghost Fleet)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLHIL002 486 4615 # Hill Sl.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLIND005 239 0 # East CC Pumping Plant (Discovery Bay)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLMAY002 283 1611 # Mayberry Sl.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLMID001 289 5441 # Middle Sl. @ Winters Island\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLML001 443 4599 # Mallard Sl. (CCWD)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLMZU003 523 0 # Montezuma Sl. @ Hunter Cut\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLMZU011 517 7662 # Montezuma Sl. @ Beldon's Landing\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLMZU025 513 13883 # Montezuma Sl. @ National Steel\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLMZU029 513 1346 # Montezuma Sl. @ Roaring River\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLMZU032 511 1677 # Montezuma Sl.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLNY002 288 2005 # New York Sl.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLPPR000 268 4735 # Piper Sl. @ Bethel Tract\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLPPR003 269 8333 # Piper Sl. @ Bethel Isl.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLRAR000 529 1250 # Roaring River\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLRAR009 532 7375 # Roaring River @ Sprig\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLRCK005 247 221 # Rock Slough (CCC)\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLSBT011 385 2273 # Steamboat Sl.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLSUS012 461 9982 # Suisun Sl. @ Volanti\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLTMP000 194 1946 # Tom Paine Sl. Intake\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLTMP017 185 0 # Tom Pain Sl.\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 SLTRM004 310 540 # Three Mile Sl. @ SJR"},{"location":"reference/Scripts_and_input_files/","title":"Scripts and input files","text":""},{"location":"reference/Scripts_and_input_files/#folder-structure","title":"Folder Structure","text":"<p>Most preprocess scripts are originally located at ${DSM2}/scripts/, while it also needs DSM2 config file and input timeseries (from CalSIM) to run.</p> <p>Fig. Sample batch preprocess from CalSIM II to DSM2</p> <p> </p> <p>Some\u00a0key functions\u00a0are explained below:</p> <ul> <li>config file usually defines the time window, CALSIMNAME,     DSM2MODIFIER, which need to be consistent with the scripts and     timeseries files.</li> <li>planning_boundary_flow contains method 'smooth_flow' to 'tension     spline' boundaries Sac and Vernalis from monthly to daily-interval     data.</li> <li>DICU are directly transferred as monthly-interval data.</li> <li>prep_ec generate Martinez EC from its astro-planning stage and NDO     (modified from G-model and has a newly calibrated version). Refer to     Martinez EC     Generator\u00a0for     its introduction and updates.</li> </ul>"},{"location":"reference/Scripts_and_input_files/#to-run","title":"To run","text":"<p>type prepro*.bat config*.inp at the scenario path in command window</p> <p>*usually we start preprocess earlier (like 1 month) than hydro + qual</p>"},{"location":"reference/Scripts_and_input_files/#cwf-preprocess-version-in-dsm2-v806","title":"CWF preprocess version in DSM2 v806","text":"<p>CH2M helped creating an updated version for project 'California Water Fix' (CWF), whose existing condition (EST)\u00a0 and No Action Alternative (NAA) are widely used as templates for DSM2 planning study.</p> Directory Structure Timeseries Files CalSim Files <p>Figure. Sample directory of CWF scenario for DSM2 planning modeling</p> <p>Compared to the original scripts package, CWF version has</p> <ul> <li>a sub-folder 'scripts' under scenario. It contains updated scripts     to replace those under ${DSM2}/scripts/.</li> <li>a sub-folder 'input' under scenario. It contains updated hydro and     qual grids.</li> <li>a sub-folder 'timeseries' under scenario. It contains updated DSM2     input files (*daily.dss for boundaries, *.dss for Martinez stage,     DICU, oprule, QUAL, *qa.dss for QAQC)</li> <li>the above input files are generated from\u00a0its 'CALSIM' subfolder,     with *DV.dss (CalSIM outputs) and *SV.dss (CalSIM inputs) required     by\u00a0the current CalSIM II preprocess (CalSIM 3 plans to keep only     *DV.dss)</li> </ul> <p></p> <p>Figure. CWF version's batch preprocess from CalSIM II to DSM2</p> <p>More details in the scripts:</p> <ul> <li>planning_boundary_flow contains method 'smooth_flow' to 'tension     spline' boundaries Sac and Vernalis from monthly to daily-interval     data.</li> <li>CWF dailymapping processes all boundaries and source flows to     daily-interval data.</li> <li>prep_ec has updated changes with sea level rise adjustment</li> <li>Vernalis Adaptive Management Plan (VAMP) of the original scripts is     not active\u00a0anymore</li> <li>Source flows and intakes are added/modified</li> <li>Method 'daily mapping' to process all boundaries/source inputs to     daily-interval data</li> </ul>"},{"location":"reference/Scripts_and_input_files/#attachments","title":"Attachments:","text":"<ul> <li>bat_prep_cwf.png (image/png)</li> <li>CWFtimeseries.JPG (image/jpeg)</li> <li>CWFcalsimfiles.JPG (image/jpeg)</li> <li>CWFdir.JPG (image/jpeg)</li> <li>bat_prep.png (image/png)</li> <li>prep_doc_bst.py (application/octet-stream)</li> <li>planning_ec_mtz_bst.py (application/octet-stream)</li> <li>planning_boundary_flow_bst.py (application/octet-stream)</li> <li>extend_calsim_outputs_bst.py (application/octet-stream)</li> <li>expand_seasonal_bst.py (application/octet-stream)</li> <li>dailymapping_051010.py (application/octet-stream)</li> <li>prep_ec_bst.py (application/octet-stream)</li> <li>prepro_BST_Existing.bat (application/octet-stream)</li> </ul>"},{"location":"reference/Supporting_Tools_and_QAQC/","title":"Supporting Tools and QA/QC","text":"<p>All preprocess scripts are written with DSM2-Vista. Use the most updated version available at DSM2 Vista GitHub. Ensure relevant environment variables are updated.</p>"},{"location":"reference/Supporting_Tools_and_QAQC/#recommended-practices","title":"Recommended Practices","text":"<ol> <li>Review and Compare CALSIM Outputs</li> <li>Use HEC-DSS and DSM2-Vista.</li> <li> <p>Ensure consistency in pathnames, time windows, etc., between comparison scenarios.</p> </li> <li> <p>WRIMS Report Tool</p> </li> <li>Useful for comparing CALSIM outputs (e.g., DSM2 inputs in <code>timeseries\\CALSIM\\\\DV.dss</code>).</li> <li>WRIMS Tool Details.</li> </ol> <p> </p> <ol> <li>Compare DSS Tool</li> <li>Use the Compare DSS Tool in DSM2-Vista.</li> <li>Scripts: <code>${vista}\\bin\\compare_dss.bat</code> or <code>${vista}\\bin\\compare_dss_files.bat</code>.</li> </ol> <p></p> <p>* One good practice is to locate changes first (maybe the big ones), then use compare_dss.bat to specify and illustrate them.</p> <p></p> <ol> <li>Net Delta Flow (NDO) Check</li> <li>Compare NDO = inflows - outflows - CU for accuracy.</li> </ol>"},{"location":"reference/Supporting_Tools_and_QAQC/#attachments","title":"Attachments","text":"<ul> <li>compareDSSfiles.JPG</li> <li>compareDSS.JPG</li> <li>wrimsReport2.JPG</li> <li>wrimsReport1.JPG</li> <li>wrimsReport.png</li> </ul>"},{"location":"reference/Versions/","title":"Versions","text":"<p>If the grid/arc is changed (moved/split/merged) at any boundaries of the Delta, CALSIM \u2192 DSM2 preprocess scripts (and DSM2 configuration) should change, i.e., a different version. Some typical keywords are usually used as part of the scenario version name:</p> <ul> <li>Existing (EST): Represents the current Delta condition (could change over time).</li> <li>No Action Alternative (NAA): Future scenarios without major grid changes.</li> <li>Proposed Alternative (PA): Future scenarios with major grid changes (e.g., construction).</li> <li>Level of Development (LOD): Represents land use information (e.g., 2005, 2030). Since censuses are infrequent, different scenarios may use the same LOD.</li> <li>Sea Level Rise Projection (SLR): Represents climate change scenarios (e.g., 15cm, 45cm).</li> </ul> <p>Refer to California Water Fix settings of CALSIM and DSM2 (page 70, Table B-8) for examples: Final EIR-EIS Appendix.</p> <p>Operational changes or constraints usually only affect CALSIM results, not DSM2 settings. Thus, DSM2 preprocess and configuration often remain unchanged. A practical routine is to use the same file name/modifier/path for various scenarios within one version category (modify folder name or use unique names for post-processing).</p>"},{"location":"reference/Versions/#commonly-used-versions","title":"Commonly Used Versions","text":"<ul> <li>Original scripts package in <code>DSM2/scripts</code> or <code>vista/scripts/dsm2</code>.</li> <li>CH2M prepared CWF-related EST, NAA, PA, combining LOD and SLR.</li> <li>SWP Delivery Capability Report (DCR), Water Storage Investment Program (WSIP).</li> <li>Widely used versions in DSM2 v806, with updates to DSM2 v812.</li> <li>Recent practice: CALSIM3 to DSM2 in SWP Fingerprinting study (details).</li> </ul>"},{"location":"tutorials/An_Introduction_to_DSM2_Tutorials/","title":"An Introduction to DSM2 Tutorials","text":""},{"location":"tutorials/An_Introduction_to_DSM2_Tutorials/#dsm2-website","title":"DSM2 Website","text":"<p>Official Website</p>"},{"location":"tutorials/An_Introduction_to_DSM2_Tutorials/#documentation","title":"Documentation","text":"<p>If DSM2 is installed on your computer, click on the START menu and select: Programs \u2192 DSM2_v8 \u2192 DSM2_documentation</p>"},{"location":"tutorials/An_Introduction_to_DSM2_Tutorials/#introduction","title":"Introduction","text":"<p>Welcome to the Delta Simulation Model 2 (DSM2) Version 8 tutorial.</p> <p>The tutorial is divided into two sets of lessons: 1. Basic DSM2 Skills: Using simplified channels. 2. Advanced DSM2 Skills: Using the model application to the Sacramento-San Joaquin Delta.  </p> <p>The input files for these tutorials are located in the following directories: - Basic Tutorials: <code>tutorial\\simple</code> - Delta Tutorials: <code>tutorial\\historical</code> </p>"},{"location":"tutorials/An_Introduction_to_DSM2_Tutorials/#basic-tutorials","title":"Basic Tutorials","text":"<p>The goal of the Basic Tutorials (Tutorials 1-6, see Figure 1) is to familiarize you with the DSM2 input system and fundamental modeling capabilities. This six-part tutorial builds a model of a simple channel system, with each part building in complexity from its predecessor.  </p> <p>Recommendation: Complete the tutorials in order for the best learning experience. However, each tutorial is self-contained and can be completed independently.</p> <p></p>"},{"location":"tutorials/An_Introduction_to_DSM2_Tutorials/#delta-tutorials","title":"Delta Tutorials","text":"<p>The goal of the Delta Tutorials (Tutorials 1-5, see Figure 2) is to familiarize you with Delta-specific DSM2 applications and tasks.  </p> <p>In addition, a DSM2 Overview document is provided, which describes the DSM2 modules (HYDRO, QUAL, and PTM) and their typical modes of application (historical, real-time, and planning).  </p> <p></p>"},{"location":"tutorials/An_Introduction_to_DSM2_Tutorials/#dsm2-home-directory","title":"DSM2 Home Directory","text":"<p>In working through the tutorials, the directory where you installed DSM2 will be referred to as {DSM2_home}.  </p> <p>For example, if you accepted the default installation directory, {DSM2_home} would be: <code>d:\\delta\\dsm2</code> (there may also be a version number in the directory name).</p>"},{"location":"tutorials/An_Introduction_to_DSM2_Tutorials/#tutorials-overview","title":"Tutorials Overview","text":""},{"location":"tutorials/An_Introduction_to_DSM2_Tutorials/#tutorial-1-channels","title":"Tutorial 1: Channels","text":"<p>Set up the channel grid, add parameters, set boundary conditions, and list output locations.</p>"},{"location":"tutorials/An_Introduction_to_DSM2_Tutorials/#tutorial-2-reservoir-gate-transfer","title":"Tutorial 2: Reservoir Gate Transfer","text":"<p>Add reservoirs, gates, and transfers to the simple channel system.</p>"},{"location":"tutorials/An_Introduction_to_DSM2_Tutorials/#tutorial-3-layering","title":"Tutorial 3: Layering","text":"<p>Learn how to organize data in multiple files using DSM2's data management system. Layers allow input items to be grouped logically and enable changes to be incorporated into old simulations without altering archived items.</p>"},{"location":"tutorials/An_Introduction_to_DSM2_Tutorials/#tutorial-4-time-varying-data-timevar","title":"Tutorial 4: Time-Varying Data (Timevar)","text":"<p>Add time-varying information to the model. This section demonstrates how to use DSS files for boundary conditions and gate timings, replacing constant values used in earlier sections.</p>"},{"location":"tutorials/An_Introduction_to_DSM2_Tutorials/#tutorial-5-advanced-output-options","title":"Tutorial 5: Advanced Output Options","text":"<p>Covers advanced output options, including: 1. Modifications to the text input file (<code>hydro.inp</code>). 2. Use of groups and source tracking in QUAL.</p>"},{"location":"tutorials/An_Introduction_to_DSM2_Tutorials/#tutorial-6-operating-rules-oprule","title":"Tutorial 6: Operating Rules (Oprule)","text":"<p>Learn how to use Operating Rule Language (ORL) statements to set gate operations. ORL allows gates to operate dynamically based on conditions (e.g., automatically closing a gate when salinity reaches a threshold).</p>"},{"location":"tutorials/An_Introduction_to_DSM2_Tutorials/#icons-used-in-tutorials","title":"Icons Used in Tutorials","text":"<ul> <li> <p>   Indicates a DSM2 \"gotcha\" moment where extra care is necessary.  </p> </li> <li> <p>   Indicates a question to test your new DSM2 knowledge.</p> </li> </ul>"},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_1_Historical_Simulation/","title":"DSM2 Bay-Delta Tutorial 1: Historical Simulation","text":""},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_1_Historical_Simulation/#purpose","title":"Purpose:","text":"<p>This tutorial will demonstrate how to launch a basic run of the historical HYDRO and QUAL simulations.You will also get practice using the study templates that are distributed with DSM2, see how the configuration file is used, make some changes in the output and learn about the post-processing \"transfer\" script for averaging your output.</p> <p>Except as part of a re-calibration, it is rare to make big changes in the historical simulation. More commonly, you will want to add a few output locations or scalars. Large scale policy or physical changes are usually analyzed within a Planning simulation framework, covered in a later tutorial.</p>"},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_1_Historical_Simulation/#hydro-and-qual","title":"HYDRO and QUAL","text":"<ol> <li>Copy the historical template:<ol> <li>In windows, copy the     folder\u00a0\\{DSM2_home}\\study_template\\historical\u00a0to the tutorial     directory, after creating\u00a0\\{DSM2_home}\\tutorials\\historical.     If there is already a historical folder, just copy the contents.</li> <li>Open\u00a0historical_hydro.inp\u00a0and\u00a0historical_qual_ec.inp.\u00a0Note     the CONFIGURATION sections of both reference a     file\u00a0configuration_historical.inp. By containing variables     such as run dates in this file, you can more easily synchronize     the models.</li> <li>Examine the\u00a0common_input\u00a0directory. By looking     at\u00a0historical_hydro.inp, configuration_historical.inp\u00a0and the     other main input files, you will see that many of the included     files for the models are in the directory ${DSM2INPUTDIR}. In     this distribution, this variable points     to\u00a0/dsm2/common_input\u00a0\u2013 a repository in which all the     distributed DSM2 input files are housed. Later, you may want to     copy the input files locally and repoint ${DSM2INPUTDIR} to this     local directory. In fact, there are tools to help with this.     Regardless of whether you copy them, please resist changing the     files directly \u2013 it is much easier to diagnose problems if you     make your changes in the main file (historical_hydro.inp,     historical_qual_ec.inp\u2026) or in a new file of your own making.</li> </ol> </li> <li>Modify the Run Times in the Configuration File:</li> </ol> <p>In the configuration file, set the runtime definitions as follows.</p>"},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_1_Historical_Simulation/#runtime","title":"runtime","text":"<p>START_DATE 01JUL1996 START_TIME 0000 QUAL_START_DATE 02JUL1996 PTM_START_DATE ${QUAL_START_DATE} END_DATE 01SEP1996 END_TIME 0000\u00a0</p> <ol> <li>Note the Output Step in HYDRO:</li> </ol> <p>If you look in the channel output files (e.g.\u00a0output_channel_std_hydro_rki_20090715.inp), you will find that the time step of the output is itself an ENVVAR definition called ${FINE_OUT}. This is usually defined as 15 minutes in configuration file. Although DSM2 v8 will perform daily averages, it is recommended that you use the finer output and aggregate as a postprocessing step (we will cover this shortly).</p> <ol> <li>Add some Output</li> </ol> <p>In historical_hydro.inp, add a block containing an extra flow output for Old River at Head. Notice that the name in this case is a \"practical\" name. Although you may sometimes add input with names like \"ch56_0\", such a name is redundant with the other information in the line, is difficult for non-modelers to understand and causes confusion if the grid numbering changes.  </p> <p>OUTPUT_CHANNEL NAME CHAN_NO DISTANCE VARIABLE INTERVAL PERIOD_OP FILE\u00a0 oldr_head 56 0 flow ${FINE_OUT} inst ${HYDROOUTDSSFILE}\u00a0 END\u00a0</p> <ol> <li> <p>Run HYDRO and QUAL:</p> <ol> <li>In Windows Explorer, navigate to the directory,     _\\{DSM2_home}\\tutorial_</li> <li>Right-click on the\u00a0historical\u00a0directory, and select,\u00a0Open     Command Window Here.</li> <li>In the command window, type:\u00a0hydro historical_hydro.inp</li> <li>Wait for HYDRO to complete its runs.</li> <li>Now type:\u00a0qual historical_qual_ec.inp</li> </ol> </li> <li> <p>Aggregate the Output</p> </li> </ol> <p>Above we recommended that you use post-processing to aggregate your output. Let's see how this works. At a command prompt in the ${study}/output directory, type: &gt; transfer -\u2013help This command should give you the options for the \"transfer.py\" script that will help you aggregate your output.\u00a0 For instance, if you want to create a daily average of all your flow output, type (this is all one line):\u00a0 &gt;transfer --out=postpro.dss --selection=///FLOW////\u00a0 --transform=period_ave --interval=1DAY historical.dss\u00a0 As another example, you may want to take a Godin average of all the stage output and put it in the same file:\u00a0 &gt;transfer --out=postpro.dss --selection=///STAGE////\u00a0 --transform=godin historical.dss\u00a0 You can similarly do monthly averages by making the interval 1MON and you can \"slice\" in time by specifying a time window (the syntax is given by the help command: &gt; transfer -\u2013help\u00a0</p> <ol> <li> <p>Running QUAL with Volumetric fingerpringting:</p> <ol> <li>In the command window, type:\u00a0qual historical_qual_vol.inp.</li> <li>Open the qual echo file qual_vol_echo_historical.inp in the     output subfolder.</li> <li>Open the results file in the output subfolder, and examine the     results.</li> </ol> </li> <li> <p>Running QUAL with Nonconservative Constituents fingerpringting:</p> <ol> <li>In Windows Explorer, navigate to the     directory,\u00a0\\{DSM2_home}\\study_template_     _historical_qual_do\\ Conduct a similar study as EC and VOL.</li> <li>Notice that the running time period is 1996-2000, since Stockton     effluent is not using 'constant'\u00a0but detailed     timeseries:\u00a0effluentflow96-2000.dss</li> </ol> </li> </ol> <p>ENVVAR NAME VALUE\u00a0 STOCKTON_FLOW_FILE ${TSINPUTDIR}/effluentflow96-2000.dss # needed for DO runs, if not available use constant END</p> <ol> <li> <ol> <li>Open the results file in the output subfolder, and examine the         results.</li> </ol> </li> </ol>"},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_1_Historical_Simulation/#particle-tracking-modeling-ptm","title":"Particle Tracking Modeling (PTM)","text":"<ol> <li> <p>Run PTM in Delta Grid under Historical Condition</p> <ol> <li> <p>In Windows Explorer, navigate to the directory, _  </p> <p>Unknown macro: {DSM2_home}{_}tutorial\\ in the command window, type:\u00a0ptm historical_ptm.inp. *If necessary, reduce the running time period by modifying\u00a0END_DATE\u00a0in\u00a0configuration_historical.inp.     2.  Open the ptm echo file ptm_echo_historical.inp in the output subfolder and examine the contents.     3.  Open the ptmout.dss file in the output subfolder, and examine the results. Do a little mass balance to see if the particle fluxes add up.</p> </li> </ol> </li> <li> <p>Repeat with Particle Filter on Channel Turned on:</p> </li> </ol> <p>Set particle filter at Head of Old River\u00a0</p> <ol> <li> <ol> <li>In historical_ptm.inp, create the table for particle filter,         with constant closing operation.</li> </ol> </li> </ol> <p>PARTICLE_FILTER NAME NODE AT_WB FILLIN FILE PATH Filter_HOR 8 chan:54 last constant 0 END\u00a0</p> <ol> <li> <ol> <li>Add the related output, like</li> </ol> </li> </ol> <p>PARTICLE_FLUX_OUTPUT NAME FROM_WB TO_WB INTERVAL FILE SJR-OLD chan:7 chan:54 15min ${PTMOUTPUTFILE} END\u00a0</p> <ol> <li> <ol> <li>Open the ptmout.dss file in the output subfolder, and examine         the results</li> </ol> </li> <li> <p>Repeat with Particle Filter on Reservoir Turned on:</p> </li> </ol> <p>With particle filter installed at Clifton Court Forebay (this is a special version of filter dealing with source flows directly connecting to reservoir)\u00a0</p> <ol> <li> <ol> <li>In historical_ptm.inp, create the table for particle filter,         with time-varying operation control, specified in DSS file.</li> </ol> </li> </ol> <p>PARTICLE_RES_FILTER NAME RES_NAME AT_WB FILLIN FILE PATH clfc_div_bbid clifton_court qext:dicu_div_bbid last ./filterOp.dss /HIST+FILTER/CLFC_DIV/FILTER_OP//IR-DECADE/DWR-BDO/ END\u00a0</p> <ol> <li> <ol> <li>Add the related output, like</li> </ol> </li> </ol> <p>PARTICLE_FLUX_OUTPUT NAME FROM_WB TO_WB INTERVAL FILE SWP-AG res:clifton_court group:bbid 15min ${PTMOUTPUTFILE}\u00a0 END\u00a0</p> <ol> <li> <ol> <li>Open the ptmout.dss file in the output subfolder, and examine         the results</li> </ol> </li> <li> <p>Repeat with Particle Filter on Source Flow Turned on:</p> </li> </ol> <p>Agriculture source flow (diversions and seepages) could be required to restrict particles from entering in simulations. It is one application for particle filter.\u00a0</p> <ol> <li> <ol> <li>In Windows Explorer, navigate to the directory,         \\{DSM2_home}\\tutorial\\ Open the         file\u00a0delta_dicu_filter_closed.txt. Copy the content into         historical_ptm.inp</li> </ol> </li> <li> <ol> <li>Open the ptmout.dss file in the output subfolder, and examine         the results</li> </ol> </li> </ol>"},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_1_Historical_Simulation/#making-animation-of-particle-tracking-modeling-ptm","title":"Making animation of Particle Tracking Modeling (PTM)","text":"<ol> <li>Modify the PTM input file to make text output and to turn on the     dispersion parameters:<ol> <li>In Windows Explorer, copy the folder\u00a0ptm_animate\u00a0(with     subfolders) from\u00a0\\{DSM2_home}\\study_templates\\ptm_animate</li> </ol> </li> </ol> <p>to the study directory, creating: \\{DSM2_home}\\tutorials\\historical\\ptm_animate</p> <ol> <li> <ol> <li>With the PTM, it is useful to be able to switch easily between         text and dss output formats \u2013 note that the animator requires         text files. The\u00a0configuration_historical.inp\u00a0file is         structured so that we can swap the environmental         variable\u00a0PTMOUTPUTFILE. We are going to         point\u00a0PTMOUTPUTFILE\u00a0to txt format so we can use the animator.         1.  1.  Locate the\u00a0PTMOUTPUTFILE\u00a0at the end of the file, and                 modify as:</li> </ol> </li> </ol> <p>PTMOUTPUTFILE ptmout.txt</p> <ol> <li> <ol> <li>Open the file,\u00a0historical_ptm.inp.         1.  Locate the SCALARS section. Check all of the dispersion             parameters to be\u00a0t.</li> </ol> </li> </ol> <p>ptm_ivert\u00a0t\u00a0# Use Vertical velocity profile ptm_itrans\u00a0t\u00a0# Use Transverse velocity profile ptm_iey\u00a0t\u00a0# Use transverse mixing ptm_iez\u00a0t\u00a0# Use vertical mixing</p> <ol> <li> <ol> <li> <ol> <li>Make sure the\u00a0anim_db.bin\u00a0line is turned on (this is             usually commented out to save much running time)</li> </ol> </li> </ol> </li> </ol> <p>ptm anim out 15min ${DSM2OUTPUTDIR}/anim_db.bin\u00a0  </p> <ol> <li>Run PTM:<ol> <li>In the command window, type:\u00a0ptm historical_ptm.inp.</li> <li>In Windows Explorer:<ol> <li>Navigate to the directory,</li> </ol> </li> </ol> </li> </ol> <p>\\{DSM2_home}\\tutorials\\historical\\output</p> <ol> <li> <ol> <li> <ol> <li>Examine the output in the\u00a0ptmout.txt\u00a0file.         2.  Copy the files,\u00a0anim_db.bin\u00a0and\u00a0ptmout.txt.         3.  Navigate to the directory,</li> </ol> </li> </ol> </li> </ol> <p>\\{DSM2_home}\\tutorials\\historical\\ptm-animate\\dual\\left_panel</p> <ol> <li> <ol> <li> <ol> <li>Paste the files in the\u00a0left_panel\u00a0directory.</li> </ol> </li> </ol> </li> <li> <p>Repeat with Dispersions Parameters Turned Off:</p> <ol> <li>In Windows Explorer, navigate to the directory,     _\\{DSM2_home}\\tutorials\\historical_</li> <li>Open the file,\u00a0historical_ptm.inp.<ol> <li>Locate the SCALARS section.</li> <li>Change all of the dispersion parameters from\u00a0t\u00a0to\u00a0f.</li> </ol> </li> </ol> </li> </ol> <p>ptm_ivert\u00a0f\u00a0# Use Vertical velocity profile ptm_itrans\u00a0f\u00a0# Use Transverse velocity profile ptm_iey\u00a0f\u00a0# Use transverse mixing ptm_iez\u00a0f\u00a0# Use vertical mixing</p> <ol> <li> <ol> <li>In the command window, type:\u00a0ptm historical_ptm.inp.</li> <li>In Windows Explorer:<ol> <li>Navigate to the directory,</li> </ol> </li> </ol> </li> </ol> <p>\\{DSM2_home}\\tutorials\\historical\\output</p> <ol> <li> <ol> <li> <ol> <li>Copy the files,\u00a0anim_db.bin\u00a0and\u00a0ptmout.txt.         2.  Navigate to the directory,</li> </ol> </li> </ol> </li> </ol> <p>\\{DSM2_home}\\tutorials\\historical\\ptm-animate\\dual\\right_panel</p> <ol> <li> <ol> <li> <ol> <li>Paste the files in the\u00a0right_panel\u00a0directory.         2.  Navigate to the directory,</li> </ol> </li> </ol> </li> </ol> <p>\\{DSM2_home}\\tutorials\\historical\\ptm-animate</p> <ol> <li> <ol> <li> <ol> <li>Double-click on\u00a0dual.bat\u00a0to open the animator.         2.  Press start to start the animator and use the controls to             adjust the speed.</li> </ol> </li> </ol> </li> <li> <p>Modifying the Animator Display:</p> <ol> <li>The\u00a0left_panel\u00a0and\u00a0right_panel\u00a0directories contain files     needed for operation:<ol> <li>Modify the data path names:\u00a0fluxInfoDB.data\u00a0stores file     and path information for the PTM output (the flux output in     the text file is labeled with DSS-like path names). The     listings in this file will be turned into the small flux bar     graphs you see in the animator. The integer you see above     the file name is an internal node ID, which is how you     assign locations in the animator (also     see\u00a0network.dat\u00a0below). Also, an output file of the PTM     version 8 contains a minor version number. So the user may     need to modify the data path names in     the\u00a0fluxInfoDB.data\u00a0according to corresponding path names     in an output file,\u00a0ptmout.txt\u00a0in this example.</li> <li>labelsDB.data\u00a0stores label information. You list labels     and their location (using nodes, see\u00a0network.dat\u00a0below)</li> <li>network.dat\u00a0stores internal\u00a0x-\u00a0and\u00a0y-locations for     nodes and channels. Pseudo-nodes are used for labels and     other annotations as noted above. Please note that the nodes     that are used in\u00a0network.dat\u00a0are internal node numbers,     not external. (This makes the file very hard to edit, a     point that will probably be addressed in the future). If you     want a mapping of external-to-internal numbers, look at your     echoed hydro output file (*.out or *.hof).</li> </ol> </li> <li>Examine these files and the labels in them. Change the labels to     something creative and reopen the animator.</li> </ol> </li> </ol>"},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_2_Source_Tracking_Fingerprinting_/","title":"DSM2 Bay-Delta Tutorial 2: Source Tracking (Fingerprinting)","text":""},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_2_Source_Tracking_Fingerprinting_/#purpose-the-purpose-of-this-tutorial-is-to-use-the-source-tracking-capabilities-of-the-model-to-create-a-fingerprinting-study-we-will-set-up-both-volumetric-and-concentration-based-fingerprinting-and-visualize-the-results","title":"Purpose: The purpose of this tutorial is to use the source tracking capabilities of the model to create a fingerprinting study. We will set up both volumetric and concentration-based fingerprinting and visualize the results.","text":"<ol> <li> <p>Reopen the historical tutorial</p> <ol> <li>In windows, navigate to \\{DSM2_home}\\tutorial\\historical.     (folders and files are copied as described in the Delta tutorial     1)</li> </ol> </li> <li> <p>Create a model for source tracking:</p> </li> </ol> <p>In the background, source tracking imposes a computational cost on QUAL that is the same as one additional constituent per source. For this reason, it is useful to comment out source tracking as a standard course of running DSM2. But when you desire source tracking, you can uncomment it as follows:</p> <ol> <li> <ol> <li>In historical_qual_ec.inp, locate the GROUPS include section.</li> <li>Uncomment the group definitions for source tracking (delete the     # sign at the start of the line). You may wish to review the     referenced file to see how the groups are identified.</li> <li>Similarly uncomment the two fingerprinting files \u2013 the ones that     have \"source_track\" in their names.</li> </ol> </li> <li> <p>Define volumetric inputs</p> <ol> <li>Create the QUAL volumetric input file. Copy     historical_qual_ec.inp and rename as     historical_qual_vol.inp.</li> <li>Modify the concentration blocks. Go through each of the node and     reservoir concentration files for QUAL ec. Modify the     constituent (variable) to unit, value (FILE) to constant,     (PATH) to 100. This step is conceptually simple, but will     produce a large file \u2013 feel free to break it into several files     if you prefer. If you are using Notepad++, you may want to use     its column delete/copying features (press alt while you make     your selection).</li> <li>Compare what you produced to the existing files in common_input     that have \"volumetric\" in their names (node and reservoir     concentration). Are they the same input? How could you test this     using the echoed output?</li> </ol> </li> <li> <p>Define the fingerprinting output</p> <ol> <li>Specify Clifton Court concentration output for each of the     source groups defined in the previous step, for both     constituents: ec and unit, in block     OUTPUT_RESERVOIR_SOURCE_TRACK. The name should be clifton_court,     the concentration (variable) should be ec or volume and the     interval should be 1day. Avoid redundancy or use of the source     in the output name: i.e. use \"clifton_court\" for the name, not     \"clifton_ag\" or \"clifton_ec\" . Because the source information is     recorded in the F part of output dss file.</li> <li>Similar specification could be defined for channel source track     in block OUTPUT_CHANNEL_SOURCE_TRACK. Pick any channel you are     interested and do the definition.</li> </ol> </li> <li> <p>Run HYDRO and QUAL for One Year</p> <ol> <li>Using historical_hydro.inp, historical_qual_ec.inp, historical_qual_vol.inp as the launch files, run HYDRO and     QUAL for one year in 2002. Start QUAL a day later to avoid mass     conservation errors in the first hour. Make sure the init_conc     variable (in SCALAR block) is set to zero so that there will be     no initial condition contribution for any variables (note: for a     volumetric fingerprint, it may be useful to make this     concentration 100 if you want to include initial conditions in     the fingerprint analysis).</li> <li>Open the output file (historical.dss), and examine the     results.</li> </ol> </li> <li> <p>Process the output</p> <ol> <li>Use VISTA or HEC-DSSVUE to open up the output file. Copy     May-September concentrations source track of Clifton Court for     each location. Paste the output into a new sheet in the Excel     provided called excel_fingerprint.xls, which you can use as a     reference. Use the \"stacked area plot\" in Excel (one of the     standard Excel plot types) to plot up the fingerprint results.</li> </ol> </li> </ol>"},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_3_Planning_Simulation/","title":"DSM2 Bay-Delta Tutorial 3: Planning Simulation","text":""},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_3_Planning_Simulation/#purpose-the-goal-of-this-tutorial-is-to-learn-to-preprocess-and-launch-a-bay-delta-planning-simulation-using-calsim-output-as-the-basis-for-flow-inputs","title":"Purpose: The goal of this tutorial is to learn to preprocess and launch a Bay-Delta planning simulation using CalSim Output as the basis for flow inputs.","text":""},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_3_Planning_Simulation/#the-calsim-study-we-will-use-is-the-ocap_2005a01a_ewa2_71_novamp_dvdss-provided-in-the-tutorialsdata-directory-we-will-prepare-and-launch-the-run-using-both-temporary-barriers-and-permanent-barriers-configurations-sdip-south-delta-improvements-program","title":"The CalSim study we will use is the ocap_2005A01A_EWA2_71_novamp_DV.dss provided in the tutorials/data directory. We will prepare and launch the run using both temporary barriers and permanent barriers configurations (SDIP: South Delta Improvements Program).","text":""},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_3_Planning_Simulation/#preparation","title":"Preparation","text":"<p>We will begin by creating a study space to house the planning study.</p> <ol> <li> <p>Copy the study template:</p> <ol> <li>In windows, navigate to \\{DSM2_home}\\study_templates. Copy     the ocap_sdip template to \\{DSM2_home}\\tutorial\\ocap_sdip.     Copy the ocap_temp_barrier template to     \\{DSM2_home}\\tutorial\\ocap_temp_barrier</li> <li>In each new study folder, create a directory called \"output\" if     there is not such a folder there already.</li> <li>Copy the file ocap_2005A01A_EWA2_71_novamp_DV.dss from     \\{DSM2_home}\\timeseries to     \\{DSM2_home}\\tutorial\\data\\calsim. Note that we just put this     file in timeseries as a sample \u2013 in practice CalSim output will     be exterior to the DSM2 distribution (or will be in the study     folder).</li> </ol> </li> <li> <p>Preprocess for sdip and temp_barriers:</p> <ol> <li>Navigate to the ocap_sdip study directory and open     config_sdip_ocap_71.inp.</li> <li>Make sure that the run dates are set to the full 1974-1991     sixteen year planning period. It is a good idea to preprocess     the full period even if you want to run a subset of these dates.</li> <li>Set the DSM2MODIFIER to ocap_sdip_tutorial.</li> <li>Make sure that the DICU version in the configuration file is     2005, representing a 2005 level of development.</li> <li>Makes sure the STAGE_VERSION in the configuration file is     PLANNING-2-SL.\u00a0\u00a0</li> <li>Make sure the configuration file is pointing to the right data,     which means using the right directory, file and DSS path to find     the CalSim results. In this case, set:<ol> <li>CALSIMNAME to ocap_2005A01A_EWA2_71_novamp_DV (CalSim output     file without the \".dss\" extension)</li> <li>CALSIMSTUDY_ORIGINAL to 2005A01A</li> <li>~~ CALSIMDIR to ../data/calsim ~~</li> </ol> </li> <li>Save your data</li> <li>Launch the preprocessing system. Obtain a command prompt and     type:</li> </ol> </li> </ol> <p>&gt; prepro config_sdip_ocap_71.inp</p> <ol> <li> <ol> <li>Repeat the steps above for the temporary barriers directory and         the configuration file config_ocap_temp_barriers.inp. Make         sure that the dates span the full 1974-1991 period and repeat         the checks (d) and (e) for the temporary barrier configuration         file.</li> <li>Set the DSM2MODIFIER to ocap_temp_barrier_tutorial.</li> <li>Launch the preprocessor with the command:</li> </ol> </li> </ol> <p>&gt; prepro config_ocap_temp_barriers.inp</p> <ol> <li>Run DSM2:<ol> <li>In Windows Explorer, navigate to the directory,     \\{DSM2_home}\\tutorial\\ocap_sdip</li> <li>Open the launch files hydro.inp and qual_ec.inp.</li> <li>Set the dates to a shorter period, 1974-1976, so that the run     will take reasonable time for the tutorial. Note that we always     preprocess the full period even when we attempt to shorten the     run.</li> <li>Run the sdip simulation, for hydro and qual by typing:</li> </ol> </li> </ol> <p>&gt; hydro hydro.inp,~~ocap_sdip_tutorial.dss  ~~ &gt; qual qual_ec.inp</p> <ol> <li> <ol> <li>Uncomment and Repeat these steps (a-c) and run hydro and qual         for the temporary barrier simulation.</li> </ol> </li> <li> <p>Examine the output:</p> <ol> <li>The temporary barriers and permanent barriers protect water     levels in the South Delta in very different ways. Compare the     output at ROLD059, Old River at Tracy Blvd for your two runs to     see the differences.</li> </ol> </li> </ol>"},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_4_Batch_Preprocessing/","title":"DSM2 Bay-Delta Tutorial 4: Batch Preprocessing","text":""},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_4_Batch_Preprocessing/#purpose-this-tutorial-will-demonstrate-how-to-preprocess-a-number-of-calsim-output-files-each-of-which-represents-a-different-alternative-we-will-look-at-three-alternatives-but-the-techniques-apply-to-large-numbers-of-alternatives-just-as-well-in-the-process-of-this-tutorial-you-should-become-more-familiar-with-how-dsm2-and-calsim-label-their-simulations-and-scenarios-and-a-learn-a-little-bit-about-batch-files","title":"Purpose: This tutorial will demonstrate how to preprocess a number of CalSim output files, each of which represents a different alternative \u2013 we will look at three alternatives, but the techniques apply to large numbers of alternatives just as well. In the process of this tutorial, you should become more familiar with how DSM2 and CalSim label their simulations and scenarios and a learn a little bit about batch files","text":""},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_4_Batch_Preprocessing/#calsim-files-a-typical-situation-with-planning-studies-is-that-the-input-scenarios-are-represented-by-different-calsim-output-files-sometimes-these-files-reside-in-a-directory-structure-that-follows-a-pattern-for-instance-the-first-two-alternatives-might-look-like-this","title":"CalSim Files: A typical situation with planning studies is that the input scenarios are represented by different CalSim output files. Sometimes these files reside in a directory structure that follows a pattern, for instance the first two alternatives might look like this:","text":""},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_4_Batch_Preprocessing/#ccalsim","title":"C:/calsim","text":"<p>/altname1 /dss /d1641 2020d09edvsa.dss /altname2 /dss /d1641 2020d09edvsa.dss Note that this scheme CalSim uses directory structure to differentiate its output \u2013 the files and pathnames are identical. Another system you may encounter is one where the CalSim files themselves are named after the scenario:</p>"},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_4_Batch_Preprocessing/#ccalsim_1","title":"C:/calsim","text":"<p>/altname1_2020d09edvsa.dss /altname2_2020d09edvsa.dss</p>"},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_4_Batch_Preprocessing/#preprocessor-requirements","title":"Preprocessor requirements:","text":"<p>The DSM2 preprocessing scheme requires three pieces of information for each scenario:</p> <ol> <li>The DSM2 name we want to give the scenario (will become     DSM2MODIFIER).</li> <li>The directory in which the CalSim output is found (will become     CALSIMDIR)</li> <li>The name of the CalSim file (minus the .dss part \u2013 will become     CALSIMNAME)</li> </ol> <p>So for the first example above DSM2MODIFIER=altname1 CALSIMNAME=2020d09edvsa CALSIMDIR= c:/calsim/altname1/dss/d1641 How you will get this information into the preprocessing system depends on approach. We will look at two, but if you are an experienced script writer you will immediately see lots of possibilities.</p>"},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_4_Batch_Preprocessing/#two-approaches-for-batch-jobs","title":"Two approaches for batch jobs:","text":"<p>For larger studies, you have some choices as to how to set things up. We'll look at a few that may help you get started, while experienced script writers are likely to come up with numerous interesting variations. These exercises will guide you in setting up modest batch processing and familiarize you a bit more with the concept of environmental variables at the command line and in windows \"batch\" scripts (files with a *.bat extension that list commands for the command line).</p> <ol> <li>You can create configuration files for each alternative, e.g.     config_alt1.inp, config_alt2.inp. In each configuration file you     hard-wire the information that is required is hardwired for that     scenario. This method record of each scenario for people who inherit     your study. It is a good choice when the number of alternatives is     small. It is also a good choice when things other than CalSim vary     between alternatives.</li> <li>Alternatively, you can create a single configuration file that     points the three scenario-related variables to generic values. Then     you use a batch_prepro.bat script to loop through the scenarios.     When the number of simulations is very large (say 100 climate change     scenarios) and the only difference in the inputs is CalSim, this     method is efficient.</li> </ol> <p>Now let's go through the exercises and check out the details.</p>"},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_4_Batch_Preprocessing/#method-1-using-separate-configuration-files","title":"Method 1: Using separate configuration files:","text":"<ol> <li>Create the configuration files:<ol> <li>In windows, navigate to \\{DSM2_home}\\tutorial\\ocap_sdip. The     alternatives we are using have generic sounding names, but they     are compatible with OCAP assumptions.</li> <li>Copy the configuration file config_sdip_ocap_71.inp to     config_alt1.inp</li> <li>Make sure the study dates cover the full 1974-1991 period for     planning runs. It is usually a good idea to preprocess the whole     period, even if you are going to do run dsm2 on a subset of the     simulation period.</li> <li>Replace the three variables indicated below. The three lines may     not be next to one another.</li> </ol> </li> </ol> <p>\\&lt;file config_alt1.inp&gt; ENVVAR NAME VALUE [other definitions\u2026] # NOTE: LINES SHOWN MAY NOT</p> <ul> <li> <ol> <li>BE TOGETHER*         CALSIMNAME 2005a01edv # File name, minus .dss         DSM2MODIFIER alt1 # DSM2 name for alternative         CALSIMDIR ..data/calsim/alt1 # CalSim output directory         END</li> </ol> </li> <li> <ol> <li>Copy the file config_alt1.inp to config_alt2.inp. Repeat         step (d) using alt2 as the DSM2MODIFIER.</li> <li>Prepare hydro.inp and qual.inp to handle a generic     configuration file by making the name of the configuration file     at the top of each an ENVVAR. We will be providing this from the     command line or batch file \u2013 as an operating system     environmental variable.</li> </ol> </li> </ul> <p>\\&lt;file hydro.inp&gt; CONFIGURATION ${CONFIGFILE} # Changed END \u2026 [other data]  </p> <ol> <li> <ol> <li>Prepare a batch file for preprocessing. It will have one line         per alternative. Notice the \"call\" statement \u2013 this is the best         way to call a succession of other batch files (prepro is itself         a batch file called prepro.bat).</li> </ol> </li> </ol> <p>\\&lt;file study_prepro.bat&gt; call prepro config_alt1.inp call prepro config_alt2.inp  </p> <ol> <li> <ol> <li>At the command prompt, launch the preprocessing by typing:</li> </ol> </li> </ol> <p>&gt; study_prepro.bat</p> <ol> <li> <ol> <li>Now create a batch file that launches QUAL and HYDRO for every         alternative in the study. For each alternative, you must set the         environment variable CONFIGFILE, then launch the models.</li> </ol> </li> </ol> <p>\\&lt;file study.bat&gt; SET CONFIGFILE=config_alt1.inp hydro hydro.inp qual qual_ec.inp SET CONFIGFILE=config_alt2.inp hydro hydro.inp qual qual_ec.inp</p> <ol> <li> <ol> <li>Launch the study batch file by typing at the command prompt:</li> </ol> </li> </ol> <p>&gt; study.bat</p>"},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_4_Batch_Preprocessing/#method-2-batch-file-that-loops","title":"Method 2: Batch file that loops","text":"<ol> <li>*Create a generic configuration file:*<ol> <li>In the looping method, we are going to describe the alternatives     in a text file and loop through the text file. First we need a     configuration file that is generic. Let's begin by copying     config_sdip_ocap_71.inp one more time to a file called     config_study.inp. Change the 3 variables (DSM2MODIFIER,     CALSIMNAME and CALSIMDIR) as follows.</li> </ol> </li> </ol> <p>\\&lt;file config_study.inp&gt; ENVVAR NAME VALUE [other definitions\u2026] CALSIMNAME ${BATCH_CALSIMNAME} # File name, minus . DSM2MODIFIER ${BATCH_DSM2MODIFIER} CALSIMDIR ${BATCH_CALSIMDIR} # CalSim output directory dss</p> <ol> <li> <p>DSM2 name for alternative [other definitions\u2026] END</p> </li> <li> <p>Create the scenarios.txt file</p> <ol> <li>In the study folder, create a file called scenarios.txt</li> <li>On each line of the file, put the scenario name (DSM2MODIFIER),     directory (CALSIMDIR) and file name (CALSIMNAME) minus the     \".dss\" extension.</li> </ol> </li> </ol> <p>\\&lt;file scenarios.txt&gt; alt1,../data/calsim/alt1,2005a01edv alt2,../data/calsim/alt2,2005a01edv</p> <ol> <li>Launch batch_prepro.bat<ol> <li>In the study directory, obtain a command prompt and type:</li> </ol> </li> </ol> <p>&gt; batch_prepro config_study.inp scenarios.txt</p> <ol> <li> <ol> <li>Note: if the batch_prepro script fails for a particular scenario         after running others successfully, first fix the problem and         eliminate the failed (half-processed) scenario. Then avoid         re-running the successful scenarios by adding the \"resume\" tag,         for example:</li> </ol> </li> </ol> <p>&gt; batch_prepro config_study.inp scenarios.txt resume If you type this command now, batch_prepro.bat will harmlessly do nothing.</p> <ol> <li>Examine and use the preprocessing products<ol> <li>The preprocessing product is a HEC-DSS file for each scenario in     the local time series directory. You should have one file per     scenario.</li> <li>If you are doing this tutorial on your own, you may choose to     launch dsm2 on each alternative. To do this, change the     configuration file in hydro.inp and qual_ec.inp to the     generic one:</li> </ol> </li> </ol> <p>\\&lt;file hydro.inp&gt; CONFIGURATION config_study.inp END</p> <ol> <li> <ol> <li>Use batch_run.bat with the same syntax as you did for         batch_prepro::</li> </ol> </li> </ol> <p>&gt; batch_run config_study.inp Note that you may need to modify this script if you use it for something other than qual_ec. We may not be able to run the simulations in class because of the time required \u2013 but if you have extra time, change the dates to a one year (1991) and try it out.</p>"},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_5_Suisun_Marsh_Operating_Rules/","title":"DSM2 Bay-Delta Tutorial 5: Suisun Marsh Operating Rules","text":""},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_5_Suisun_Marsh_Operating_Rules/#purpose-the-objective-of-this-tutorial-is-to-learn-about-the-suisun-marsh-salinity-control-gate-and-practice-tuning-an-operating-rule","title":"Purpose: The objective of this tutorial is to learn about the Suisun Marsh Salinity Control Gate and practice tuning an operating rule.","text":""},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_5_Suisun_Marsh_Operating_Rules/#background","title":"Background:","text":""},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_5_Suisun_Marsh_Operating_Rules/#the-suisun-marsh-salinity-control-gates-smscg-were-completed-and-began-operating-in-october-1988-the-first-year-of-operation-was-used-to-test-the-gates-and-official-operation-began-in-november-1989-the-facility-consists-of-a-boat-lock-a-series-of-three-radial-gates-and-flashboards-the-smscg-control-salinity-by-restricting-the-flow-of-higher-salinity-water-from-grizzly-bay-into-montezuma-slough-during-incoming-tides-and-retaining-lower-salinity-sacramento-river-water-from-the-previous-ebb-tide-operation-of-the-smscg-in-this-fashion-lowers-salinity-in-suisun-marsh-channels-and-results-in-a-net-movement-of-water-from-east-to-west-when-delta-outflow-is-low-to-moderate-and-the-smscg-are-not-operating-net-movement-of-water-is-from-west-to-east-resulting-in-higher-salinity-water-in-montezuma-slough","title":"The Suisun Marsh Salinity Control Gates (SMSCG) were completed and began operating in October 1988. The first year of operation was used to test the gates, and official operation began in November 1989. The facility consists of a boat lock, a series of three radial gates, and flashboards. The SMSCG control salinity by restricting the flow of higher salinity water from Grizzly Bay into Montezuma Slough during incoming tides and retaining lower salinity Sacramento River water from the previous ebb tide. Operation of the SMSCG in this fashion lowers salinity in Suisun Marsh channels and results in a net movement of water from east to west. When Delta outflow is low to moderate and the SMSCG are not operating, net movement of water is from west to east, resulting in higher salinity water in Montezuma Slough.","text":""},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_5_Suisun_Marsh_Operating_Rules/#the-smscg-usually-begin-operating-in-early-october-and-depending-on-salinity-conditions-may-continue-operating-through-the-end-of-the-control-season-in-may-when-the-channel-water-salinity-decreases-sufficiently-below-the-salinity-standards-or-at-the-end-of-the-control-season-the-flashboards-are-removed-and-the-smscg-raised-to-allow-unrestricted-movement-through-montezuma-slough-details-of-annual-smscg-operations-can-be-found-in-summary-of-salinity-conditions-in-suisun-marsh-during-water-years-19841992-dwr-1994b-or-the-suisun-marsh-monitoring-program-data-summary-produced-annually-by-dwrs-environmental-services-office","title":"The SMSCG usually begin operating in early October and, depending on salinity conditions, may continue operating through the end of the control season in May. When the channel water salinity decreases sufficiently below the salinity standards, or at the end of the control season, the flashboards are removed and the SMSCG raised to allow unrestricted movement through Montezuma Slough. Details of annual SMSCG operations can be found in Summary of Salinity Conditions in Suisun Marsh During Water Years 1984\u20131992 (DWR 1994b), or the Suisun Marsh Monitoring Program Data Summary produced annually by DWR's Environmental Services Office.","text":""},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_5_Suisun_Marsh_Operating_Rules/#the-tidal-operation-of-the-gate-should-open-the-gate-when-a-water-level-drop-of-03-ft-exists-across-the-gate-upstream-to-downstream-and-to-close-the-gate-when-velocity-is-less-than-01-impending-flood-tide-the-boat-lock-is-held-open-whenever-the-radial-gates-are-operated-tidally-the-flashboard-is-typically-in-place-when-the-gates-are-operated-and-removed-when-the-gate-is-fully-open-note-that-in-the-historical-record-these-relationships-do-not-always-hold-there-have-been-numerous-operating-experiments","title":"The tidal operation of the gate should open the gate when a water level drop of 0.3 ft exists across the gate (upstream to downstream) and to close the gate when velocity is less than 0.1 (impending flood tide). The boat lock is held open whenever the radial gates are operated tidally. The flashboard is typically in place when the gates are operated and removed when the gate is fully open. Note that in the historical record these relationships do not always hold \u2013 there have been numerous operating experiments.","text":""},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_5_Suisun_Marsh_Operating_Rules/#we-will-use-martinez-ec-as-a-surrogate-to-determine-when-ec-compliance-is-an-issue-and-the-gates-need-to-be-operated-tidally","title":"We will use Martinez EC as a surrogate to determine when EC compliance is an issue and the gates need to be operated tidally.","text":""},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_5_Suisun_Marsh_Operating_Rules/#a-simplified-version-of-the-marsh-standards-is-given-below-the-units-are-millisiemens-per-square-cm-which-are-a-thousand-times-the-microsiemens-used-in-dsm2-modeling-there-is-a-water-year-dependence-in-the-full-set-of-standards-the-levels-given-in-the-table-apply-to-1974-1976-but-not-to-1977-which-is-a-deficiency-year-in-this-tutorial-we-will-consider-only-the-site-s-42-suisun-slough-volanti-slough-the-rki-for-this-location-is-slsus012-and-the-location-is-channel-494-distance-4681ft","title":"A simplified version of the Marsh standards is given below. The units are millisiemens per square cm, which are a thousand times the microsiemens used in DSM2 modeling. There is a water year dependence in the full set of standards \u2013 the levels given in the table apply to 1974-1976 but not to 1977 which is a \"deficiency year\". In this tutorial, we will consider only the site S-42, Suisun Slough @ Volanti Slough. The RKI for this location is SLSUS012, and the location is channel 494 distance 4681ft.","text":"<p>D-1641 STANDARD</p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p>OCT</p> <p>NOV</p> <p>DEC</p> <p>JAN</p> <p>FEB-MAR</p> <p>APR-MAY</p> <p> </p> <p> </p> <p>Eastern</p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p>C-2</p> <p>19.0</p> <p>15.5</p> <p>15.5</p> <p>12.5</p> <p>8.0</p> <p>11.0</p> <p> </p> <p> </p> <p>S-64</p> <p>19.0</p> <p>15.5</p> <p>15.5</p> <p>12.5</p> <p>8.0</p> <p>11.0</p> <p> </p> <p> </p> <p>S-49</p> <p>19.0</p> <p>15.5</p> <p>15.5</p> <p>12.5</p> <p>8.0</p> <p>11.0</p> <p> </p> <p> </p> <p>Western</p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p>S-42</p> <p>19.0</p> <p>16.5</p> <p>15.5</p> <p>12.5</p> <p>8.0</p> <p>11.0</p> <p> </p> <p> </p> <p>S-21</p> <p>19.0</p> <p>16.5</p> <p>15.5</p> <p>12.5</p> <p>8.0</p> <p>11.0</p> <p> </p> <p> </p> <p>S-35</p> <p>N/A*</p> <p>N/A*</p> <p>N/A*</p> <p>N/A*</p> <p>N/A*</p> <p>N/A*</p> <p> </p> <p> </p> <p>S-97</p> <p>N/A*</p> <p>N/A*</p> <p>N/A*</p> <p>N/A*</p> <p>N/A*</p> <p>N/A*</p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p>*In a good faith effort, DWR will consider S35 and S97 monitoring stations</p> <p>*In a good faith effort, DWR will consider S35 and S97 monitoring stations</p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p>when deciding gate operations.</p> <p>when deciding gate operations.</p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p> </p> <p></p>"},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_5_Suisun_Marsh_Operating_Rules/#steps","title":"Steps:","text":"<ol> <li>Copy the study and configuration files:<ol> <li>In windows, navigate to     \\{DSM2_home}\\study_templates\\ocap_sdip.</li> <li>Copy the planning study to the \\{DSM2_home}\\tutorials     directory.</li> <li>Rename config_sdip_ocap_71.inp as config_suisun.inp.</li> <li>In the configuration file, make sure the study dates cover the     full 1974-1991 period for planning runs. It is usually a good     idea to preprocess the full period of the inputs, even if you     are going to do run dsm2 on a subset of the simulation period.</li> <li>Set DSM2MODIFIER to suisun.</li> <li>Run prepro on the file config_suisun.inp:</li> </ol> </li> </ol> <p>&gt; prepro config_suisun.inp</p> <ol> <li>Examine and correct the Suisun Marsh operating rule.</li> </ol> <p>The Montezuma Slough velocity close rule in oprule_montezuma_planning_gate.inp is based on flow (note this file name will have a version date appended to it. The rule requires correction to be based on velocity.</p> <ol> <li> <ol> <li>Add a file representing a \"correction layer\" to the operating         rules called oprule_revised_montezuma.inp.</li> <li>Correct the velclose part of the rule to be based on channel     velocity. You can look up the correct variable name in the     Operating Rule Guide in the html help system.</li> <li>Note the Martinez EC path used in the operating rule to     determine whether the gate needs to be operated tidally. Open     the suisun.dss input file and tidally or daily average this     path. Then substitute the tidally averaged version of EC in the     operating rule by overriding the time series definition in the     Operation Time Series table.</li> <li>Note that the threshold for operating the gate is in the     configuration file: MSCS_EC_THRESH 20000</li> </ol> </li> <li> <p>Run DSM2:</p> <ol> <li>In the configuration file, set the run dates to 1974 \u2013 1977.</li> <li>Add the output you will need to examine the S42 site using the     information given in the introduction and the techniques you     have learned from the other tutorials.</li> <li>Point the CONFIGURATION include file in hydro.inp to     config_suisun.inp.</li> </ol> </li> </ol> <p>d. Launch HYDRO with the command: &gt;hydro hydro.inp</p> <ol> <li>Examine the output.<ol> <li>Compare EC output to the standard presented in the introduction.     Is the gate over operating or underoperating?</li> <li>How can you further enhance the operating rule? Discuss the     boatlock and flashboards.</li> </ol> </li> </ol>"},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_7_Clifton_Court_Diurnal_Pumping/","title":"DSM2 Bay-Delta Tutorial 7: Clifton Court Diurnal Pumping","text":""},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_7_Clifton_Court_Diurnal_Pumping/#purpose-the-goal-of-this-tutorial-is-to-learn-how-to-implement-a-diurnal-pumping-quota-for-banks-pumping-state-water-project-in-the-process-you-will-learn-how-to-track-totals-using-the-accumulate-function","title":"Purpose: The goal of this tutorial is to learn how to implement a diurnal pumping quota for Banks pumping (State Water Project). In the process you will learn how to track totals using the ACCUMULATE function.","text":""},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_7_Clifton_Court_Diurnal_Pumping/#background-the-banks-pumping-facility-is-often-operated-on-a-diurnal-schedule-emphasizing-pumping-during-off-peak-electricity-hours-an-example-of-summer-electricity-prices-for-the-year-2005-is-shown-in-figure-1","title":"Background: The Banks pumping facility is often operated on a diurnal schedule, emphasizing pumping during off-peak electricity hours. An example of summer electricity prices for the year 2005 is shown in Figure 1:","text":"<p> Figure 1: Example wholesale electricity prices in July 2005 (CWEMF, KT Shum)</p>"},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_7_Clifton_Court_Diurnal_Pumping/#an-idealized-schedule-from-the-point-of-view-of-electricity-would-be-to-pump-the-maximum-possible-amount-late-at-night-until-the-daily-pumping-needs-are-satisfied-actual-hourly-variations-in-pumping-are-shown-in-figure-2-numerous-other-factors-eg-ensuring-minimum-stage-requirements-in-the-forebay-can-affect-instantaneous-maximum-pumping-which-is-why-we-might-consider-an-operating-rule-instead-of-a-simple-time-series-to-model-diurnal-pumping","title":"An idealized schedule from the point of view of electricity would be to pump the maximum possible amount late at night until the daily pumping needs are satisfied. Actual hourly variations in pumping are shown in Figure 2. Numerous other factors (e.g. ensuring minimum stage requirements in the Forebay) can affect instantaneous maximum pumping, which is why we might consider an operating rule instead of a simple time series to model diurnal pumping.","text":"<p>In this tutorial we will emulate the ideal pumping schedule by tracking the amount pumped since midnight and quitting once we have pumped a total that satisfies the daily average requested by CALSIM. We will use the ACCUMULATE function to track the total. In a later step we will attenuate pumping to avoid drawing Clifton Court Forebay below -2ft NGVD. Figure 2: Diurnal Variation in Pumping, July-August 2004 (CWEMF, KT Shum)</p>"},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_7_Clifton_Court_Diurnal_Pumping/#the-planning-study-we-will-use-for-this-tutorial-is-ocap_sdip-provided-in-the-study_templates-directory-the-choice-between-temporary-and-permanent-barriers-is-not-central-to-the-material-though-the-sdip-project-did-propose-higher-pumping","title":"The planning study we will use for this tutorial is ocap_sdip *provided in the study_templates directory. The choice between temporary and permanent barriers is not central to the material, though the SDIP project did propose higher pumping.*","text":""},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_7_Clifton_Court_Diurnal_Pumping/#preparation","title":"Preparation","text":"<p>We will begin by creating a study space to house the planning study.</p> <ol> <li> <p>Copy the study template:</p> <ol> <li>In windows, navigate to \\{DSM2_home}\\study_templates. Copy     and rename the ocap_sdip template to     \\{DSM2_home}\\tutorial\\ocap_sdip_diurnal_swp.</li> <li>If you have not already done so for a previous tutorial, copy     the file ocap_2005A01A_EWA2_71_novamp_DV.dss (CALSIM output     file used for planning runs) from \\{DSM2_home}\\timeseries to     \\{DSM2_home}\\tutorial\\data\\calsim. Note that we just put this     file in timeseries as a sample \u2013 in practice CalSim output will     be exterior to the DSM2 distribution (or should go in the study     folder).</li> </ol> </li> <li> <p>Preprocess for sdip barriers:</p> <ol> <li>Rename config_sdip_ocap.inp to     config_sdip_ocap_diurnal_ccfb.inp and open the file.</li> <li>Make sure that the run dates are set to the full 1974-1991     (01OCT1974 0000 \u2013 01OCT1991 0000) sixteen year planning period.     It is a good idea to preprocess the full period even if you want     to run a subset of these dates.</li> <li>Set the DSM2MODIFIER to diurnal_pumping.</li> <li>Make sure that the DICU version in the configuration file is     2005, representing a future (2005) level of development.</li> <li>Makes sure the STAGE_VERSION in the configuration file is     PLANNING-2-SL.</li> <li>Make sure the configuration file is pointing to the right     directory, file and DSS path to find the CalSim results. In this     case, set:<ol> <li>CALSIMNAME to ocap_2005A01A_EWA2_71_novamp_DV (CalSim output     file without the \".dss\" extension)</li> <li>CALSIMSTUDY_ORIGINAL to 2005A01A</li> <li>CALSIMDIR to ../data/calsim</li> </ol> </li> <li>Save your data</li> <li>Launch the preprocessing system. Obtain a command prompt and     type:</li> </ol> </li> </ol> <p>&gt; prepro config_sdip_ocap_diurnal_ccfb.inp</p> <ol> <li> <p>Add output for Clifton Court Forebay:</p> <ol> <li>In hydro.inp, add output that will allow you to more directly     track the operations. Create an OUTPUT_RESERVOIR table. Create a     15min instantaneous output request with clfct or     clifton_court as the name, clifton_court as the reservoir,     none as the connecting node and flow-source as the variable.     The flow-source output will give the total source and sink     inflow to Clifton Court \u2013 it will differ from SWP pumping only     by a small amount (due to Byron-Bethany Irrigation District).</li> </ol> </li> <li> <p>Run DSM2:</p> <ol> <li>In the configuration file, set the dates 01JAN1975 to 25JAN1975     so that the run will take a short time. These dates will     generate the features we want for the tutorial, including a     period of low stage at Clifton Court Forebay under diurnal     operation. Note that we always preprocess the full period even     when we shorten the run.</li> <li>Open hydro.inp file and change the included configuration file     to config_sdip_ocap_diurnal_ccfb.inp and save it.</li> <li>Run the sdip simulation for HYDRO by typing:</li> </ol> </li> </ol> <p>&gt; hydro hydro.inp</p> <ol> <li>Examine the output:</li> </ol> <p>Once you have run HYDRO, open the file and look at the flow-source output for Clifton Court. This variable represents exports out of Clifton Court Forebay, which are dominated by State Water Project pumping..</p>"},{"location":"tutorials/DSM2_Bay-Delta_Tutorial_7_Clifton_Court_Diurnal_Pumping/#diurnal-operating-rule","title":"Diurnal Operating Rule","text":"<p>1. Create the diurnal rule with no Forebay stage protection:</p> <ol> <li> <ol> <li>Create a file called oprule_diurnal_swp.inp. Create empty         OPERATING_RULE and OPRULE_EXPRESSION tables. Alternatively, do         this by copying, renaming and clearing the contents of another         operating rule input file.</li> <li>Create an expression to accumulate daily State Water Project     (SWP) pumping since midnight:<ol> <li>Name: daily_total_swp</li> <li>Definition: \"ACCUMULATE(ext_flow(name=swp)*DT,0.0,HOUR==0)\"</li> </ol> </li> </ol> </li> </ol> <p>This reads \"accumulate swp, starting at zero, resetting when the hour of the day is zero\". We multiply by DT to get a volume (which makes the rule time step independent and allows comparison to a daily target). The time series reference comes from elsewhere in the input and is the daily average pumping rate. It is perfectly acceptable to use time series that are defined elsewhere in the DSM2 input without redefining it in the OPRULE_TIME_SERIES table \u2013 the latter is just there to allow you to define any additional time series you might need.</p> <ol> <li> <ol> <li>Create an expression to quantify the da.ily target. Note that we         are multiplying an average daily flow in cubic feet per second         by the number of seconds in the day to obtain a volume.         1.  Name: daily_target_swp         2.  Definition: ts(name=swp)*(60*60*24)</li> <li>Create an expression that defines maximum physical SWP pumping     as a magnitude:<ol> <li>Name: max_swp_pumping</li> <li>Definition: 9000.0</li> </ol> </li> <li>Now, in the OPERATING_RULE table create a rule that pumps the     maximum until the daily total is reached:<ol> <li>Name: swp_diurnal</li> <li>Action: \"SET ext_flow(name=swp) TO     IFELSE(abs(daily_total_swp) &gt; abs(daily_target_swp), 0.0,     -max_swp_pumping)\". Note the quotes and the minus sign: SWP     is really a sink, not a source.</li> <li>Trigger: Use STARTUP or TRUE for the trigger (the two do the     same thing, and trigger exactly once at the beginning of the     run). The rule will be in use unless it is displaced by     another operating rule.</li> </ol> </li> <li>In hydro.inp, add the new oprule_diurnal_swp.inp file at the     bottom of the OPERATIONS include block..</li> <li>Run HYDRO on the simulation. Examine the output for HYDRO,     including Clifton Court reservoir water levels, flow through the     gates to node 72 and the \"flow-source\" output for the reservoir     (which will differ from SWP pumping by a small amount due to     Byron-Bethany Irrigation District). Are you getting the     fully-on-fully-off pumping pattern you expect? Could the same     schedule be prepared off-line in advance using a 15-min time     series for SWP pumping? Does Clifton Court water surface go     below the \"warning\" level of -2.0ft NGVD needed to maintain flow     in the fish facilities?</li> <li>Now create an expression identifying a low stage condition:<ol> <li>Name: ccfb_stage_low</li> <li>Definition: res_stage(res=clifton_court) \\&lt; -2.0</li> </ol> </li> <li>Change the trigger for swp_diurnal to \"NOT ccfb_stage_low\",     including the quotes.</li> <li>Create an expression that describes inflow into Clifton Court     from the outside channel:<ol> <li>Name: ccfb_inflow</li> <li>Definition: res_flow(res=clifton_court,node=72)</li> </ol> </li> <li>Create a new operating rule that covers the critical case:<ol> <li>Name: swp_low_stage</li> <li>Action:</li> </ol> </li> </ol> </li> </ol> <p>\"SET ext_flow(name=swp) TO -min2(abs(ccfb_inflow),max_swp_pumping)\" This rule sets exports equal to the inflow to Clifton Court, which allows some pumping to continue as long as it does not further draw down Clifton Court. A simple alternative would be just to set exports to zero.</p> <ol> <li> <ol> <li> <ol> <li>Trigger: ccfb_stage_low</li> </ol> </li> </ol> </li> </ol> <p>Note the minus sign, again because SWP exports are a sink rather than a source. The absolute sign is there to make sure the minimum function is not operating on any big transient negative flows.</p> <ol> <li> <ol> <li>Rerun HYDRO. Are you getting the results you expected? Does         Clifton Court stage go below -2.0? Are you still pumping         according to the expected pattern? Could you implement this         policy with a time series controlling SWP instead of an         operating rule?</li> </ol> </li> </ol>"},{"location":"tutorials/Delta_Tutorial_8_-_Temperature_Simulation/","title":"Delta Tutorial 8 - Temperature Simulation","text":"<p>DSM2 can be used to simulate water temperature and transport of this property. It also is influenced with suspended particles and bio matter in the water and is provided as a module in DSM2.\u00a0</p> <ul> <li>Hari Could you help outline the steps for a tutorial in temperature     simulation ?</li> </ul>"},{"location":"tutorials/Delta_Tutorial_8_-_Temperature_Simulation/#step-by-step-guide","title":"Step-by-step guide","text":""},{"location":"tutorials/Delta_Tutorial_8_-_Temperature_Simulation/#related-articles","title":"Related articles","text":"<ul> <li> <p>Page:</p> <p>Data Requirement</p> </li> <li> <p>Page:</p> <p>Delta Tutorial 8 - Temperature Simulation</p> </li> </ul>"},{"location":"tutorials/Delta_Tutorial_9_-_DO_Simulation/","title":"Delta Tutorial 9 - DO Simulation","text":"<p>DSM2 can be used to simulate dissolved oxygen levels in the water. This tutorial shows how to setup the input, run and retrieve the output from the model simulation.\u00a0</p> <ul> <li>HariCould you help me outline the steps involved in doing a DO     simulation ?</li> </ul>"},{"location":"tutorials/Delta_Tutorial_9_-_DO_Simulation/#step-by-step-guide","title":"Step-by-step guide","text":""},{"location":"tutorials/Delta_Tutorial_9_-_DO_Simulation/#related-articles","title":"Related articles","text":"<ul> <li> <p>Page:</p> <p>Delta Tutorial 9 - DO Simulation</p> </li> </ul>"},{"location":"tutorials/Tutorial_1_Channels/","title":"Tutorial 1: Channels","text":"<p>Task Run DSM2 for a steady boundary condition flow and salinity (EC-electrical conductivity) simulation for a simple straight channel grid.  </p> <p>Skills Gained</p> <ul> <li>Get started with DSM2</li> <li>Creating channels</li> <li>Establishing initial and boundary conditions  </li> </ul> <p>The purpose of this tutorial is twofold: to get a start with the DSM2 model and to get practice setting up channels. We will set up a simple channel-only grid with simple constant boundary conditions and run both HYDRO and QUAL. We will look at two formats for entering cross-section geometry (the new DSM2 single file format and CSDP [Cross Section Development Program] format) and familiarize ourselves with the echo output file that gives you a single-file complete record of all the input data used in a DSM2 module.  </p> <p>For the tutorial, the channels have the following configuration and specifications:  </p> <p> </p> <p>Figure 1 - Simple channel configuration and specifications. </p> <p>Note that there are two cross-section geometries labeled A and B, which will be specified later in this tutorial. In all the channels except Channel 5, the cross-sections have been assigned at the midpoint of the channel. In Channel 5, the cross-sections are assigned at fractions 0.2 and 0.8 of the length of the channel measured from the upstream end. The DSM2 grid map includes arrows pointing from upstream to downstream, indicating the positive direction of flow.</p>"},{"location":"tutorials/Tutorial_1_Channels/#overview-of-dsm2-channel-cross-sections","title":"Overview of DSM2 Channel Cross Sections","text":"<p>DSM2 assumes a piecewise linear cross-sectional bathymetry. Width, area, and wetted perimeter are tabulated according to elevation. Each elevation lists the data (width) or cumulative data (wetted perimeter and area) below the given elevation. Anything above the top elevation is extrapolated using a slope given by a global scalar called levee_slope.  </p> <p> Figure 2: Piecewise linear bathymetry</p> <p>For instance, for a cross-section halfway downstream in a fictitious channel 123, the five layers of a cross-section with elevations given by Figure 2 might be tabulated as follows:</p> <pre><code>XSECT_LAYER\nCHAN_NO DIST ELEV AREA WIDTH WET_PERIM\n123 0.5 -14.6 0.0 0.0 0.0\n123 0.5 -9.2 216.0 80.0 102.5\n123 0.5 -4.0 736.0 120.0 111.0\n123 0.5 9.5 2410.0 160.0 142.3\n123 0.5 12.0 3028.5 162.0 148.0\n</code></pre> <p>The above table is in the single-file DSM2 cross-section format. An analogous table is produced by the Cross Section Development Program (CSDP). We will practice using both in the tutorial. The parameter levee_slope is seldom changed from its standard value of 0.33.</p> <p>The following steps will instruct you on how to create the channels, give them very simple boundary conditions, and run the model.</p> <ol> <li> <p>Open the hydro input file and add parameters:</p> <ol> <li>Use a text editor like Notepad++, Textpad, or Emacs.</li> <li>Navigate to the <code>${DSM2_home}\\tutorial\\simple\\t1_channels</code> directory.</li> <li>Open the <code>hydro.inp</code> file.</li> </ol> </li> <li> <p>In HYDRO, add the Scalar Runtime information:</p> <ol> <li>Locate the <code>SCALAR</code> table in <code>hydro.inp</code>.</li> <li> <p>Add the following scalars:</p> <pre><code>SCALAR\nNAME VALUE\nrun_start_date 01JAN1992\nrun_end_date 01MAR1992\nrun_start_time 0000\nrun_end_time 0000\ntemp_dir c:/temp\ntitle \"TUTORIAL SIMULATION ${DSM2MODIFIER}\"\nwarn_unchecked false\nEND\n</code></pre> </li> <li> <p>Ensure <code>temp_dir</code> points to a location with ample disk space.</p> </li> </ol> </li> <li> <p>In HYDRO, add Channel information:</p> <ul> <li>Add a <code>CHANNEL</code> table with channel details.</li> <li>Add an <code>XSECT_LAYER</code> table for cross-section geometry.</li> </ul> <p>Example for Channel 1:</p> </li> </ol> <pre><code>CHANNEL\nCHAN_NO LENGTH MANNING DISPERSION UPNODE DOWNNODE\n1 10000 0.035 10 1 2\nEND\n\nXSECT_LAYER\nCHAN_NO DIST ELEV AREA WIDTH WET_PERIM\n1 0.5 -24.0 0.0 40.0 40.0\n1 0.5 0.0 960.0 80.0 102.5\n1 0.5 20.0 2640.0 160.0 192.0\nEND\n</code></pre> <p>Repeat for other channels as needed.</p> <ol> <li>In HYDRO, set the Boundary information:</li> <li>Add <code>BOUNDARY_FLOW</code> and <code>BOUNDARY_STAGE</code> tables for upstream and downstream boundaries.</li> </ol> <p>Example:</p> <p>```    BOUNDARY_FLOW    NAME NODE SIGN FILLIN FILE PATH    upstream_flow 1 1 last constant 200.    END</p> <p>BOUNDARY_STAGE    NAME NODE FILLIN FILE PATH    downstream_stage 7 last constant 0.0    END    ```</p> <ol> <li>In HYDRO, set the Initial Conditions:</li> <li>Add a <code>CHANNEL_IC</code> table to set initial stage and flow for each channel.</li> </ol> <p>Example:</p> <p><code>CHANNEL_IC    CHAN_NO DISTANCE STAGE FLOW    1 0 0.0 0.0    1 length 0.0 0.0    END</code></p> <ol> <li>In HYDRO, specify the Output Locations:</li> <li>Add an <code>OUTPUT_CHANNEL</code> table to request output data.</li> </ol> <p>Example:</p> <p><code>OUTPUT_CHANNEL    NAME CHAN_NO DISTANCE VARIABLE INTERVAL PERIOD_OP FILE    bnd_1 1 0 flow 15min inst output.dss    END</code></p> <ol> <li>In QUAL, add the Scalar Runtime information:</li> <li>Add runtime scalars to <code>qual.inp</code>.</li> </ol> <p>Example:</p> <p><code>SCALAR    NAME VALUE    run_start_date 02JAN1992    run_end_date 01MAR1992    run_start_time 0000    run_end_time 0000    temp_dir c:/temp    END</code></p> <ol> <li>In QUAL, set the Boundary Concentration information:</li> <li>Add a <code>NODE_CONCENTRATION</code> table for upstream and downstream boundaries.</li> </ol> <p>Example:</p> <p><code>NODE_CONCENTRATION    NAME NODE_NO VARIABLE FILLIN FILE PATH    upstream_flow 1 ec last constant 200.    downstream_stage 7 ec last constant 30000.    END</code></p> <ol> <li>In QUAL, specify Output Locations:</li> <li>Add an <code>OUTPUT_CHANNEL</code> table for concentration data.</li> </ol> <p>Example:</p> <p><code>OUTPUT_CHANNEL    NAME CHAN_NO DISTANCE VARIABLE INTERVAL PERIOD_OP FILE    bnd_1 1 0 ec 15min inst ${QUALOUTDSSFILE}    END</code></p> <ol> <li> <p>Run HYDRO and QUAL:</p> <ul> <li>Run <code>hydro hydro.inp</code> and <code>qual qual.inp</code> from the command line.</li> <li>Verify the output in <code>output.dss</code>.</li> </ul> </li> <li> <p>CSDP style cross-sections:</p> <ul> <li>Use <code>hydro_csdp.inp</code> to reference external cross-section files.</li> </ul> <p>Example:</p> <p><code>XSECT CHAN_NO DIST FILE 1 0.5 xsect_a.txt END</code></p> </li> <li> <p>Rerun HYDRO and compare cross-section formats.</p> </li> <li> <p>Run HYDRO using echoed input.</p> </li> </ol> <p>Brain Teasers 1. What is the actual delta-x between computational points for each of the subreaches (channels 1-6)? 2. Why is the requested dx the minimum spatial step for each reach? Wouldn't you want to impose a maximum on how big dx can be? 3. Change the bottom elevation of one of the cross-sections by lowering it 5ft. How does this affect the simulation?</p>"},{"location":"tutorials/Tutorial_2_Reservoirs_Gates_Transfers/","title":"Tutorial 2: Reservoirs, Gates, Transfers","text":""},{"location":"tutorials/Tutorial_2_Reservoirs_Gates_Transfers/#task","title":"Task","text":"<p>Add reservoirs, gates, and object-to-object flow transfers to the simple channel grid created in Tutorial 1.</p>"},{"location":"tutorials/Tutorial_2_Reservoirs_Gates_Transfers/#skills-gained","title":"Skills Gained","text":"<ul> <li>Understanding how reservoirs and gates are represented in DSM2.</li> <li>Learning how to transfer flow from one reservoir or node to another reservoir or node in DSM2.</li> </ul> <p>The purpose of this tutorial is to learn how to add reservoirs, gates, and flow transfers to the simple channel-only grid created in Tutorial 1 (Figure 1). The grid we are going to create has the following configuration and specifications. The channel portion is identical to the simple channel model from Tutorial 1. Note that each tutorial is self-contained, so it is not necessary to complete Tutorial 1 before starting this tutorial.</p> <p> Figure 1 - Simple channel with a new reservoir, gate, and flow transfer.</p> <p>The following steps will instruct you on how to create these new features and add them to the simple channel system.</p>"},{"location":"tutorials/Tutorial_2_Reservoirs_Gates_Transfers/#dsm2-definitions","title":"DSM2 Definitions","text":""},{"location":"tutorials/Tutorial_2_Reservoirs_Gates_Transfers/#reservoir","title":"Reservoir","text":"<p>In DSM2, reservoirs are open bodies of water that store flow and are connected to nodes by means of an energy-based equation. This means that flow moves between the reservoir and its connected node or channel whenever there is an energy imbalance (e.g., stage difference). Reservoirs are considered instantly well-mixed.</p> <p>The <code>RESERVOIR</code> table specifies the identity and physical properties of the reservoir. Connections to nodes are specified in the <code>RESERVOIR_CONNECTION</code> table. If it is desired to regulate flow between a reservoir and its connected node or channel, a gate device is used.</p> <p>In DSM2 applications for the Delta, reservoirs are used for actual reservoirs such as Clifton Court Forebay and for open water bodies such as flooded islands.</p>"},{"location":"tutorials/Tutorial_2_Reservoirs_Gates_Transfers/#gate","title":"Gate","text":"<p>In DSM2, gates are sites that present a barrier or control on flow. A gate may have an arbitrary number of associated hydraulic devices (pipes and weirs), each of which may be operated independently to control flow.</p> <p>In DSM2 applications for the Delta, gates are used to represent the Delta Cross Channel, the Montezuma Slough Salinity Control Gates, and permanent or temporary barriers.</p>"},{"location":"tutorials/Tutorial_2_Reservoirs_Gates_Transfers/#object-to-object-flow-transfer","title":"Object-to-Object Flow Transfer","text":"<p>Transfers are direct water connections from a reservoir or node to another reservoir or node. Transfers are instantaneous movements of water (and its constituents and particles) without any detailed description of physics or storage. The <code>TRANSFER</code> table specifies the connectivity of the transfer.</p> <p>In DSM2 applications for the Delta, object-to-object transfers have been used to represent proposed peripheral canal withdrawal and outflow locations.</p>"},{"location":"tutorials/Tutorial_2_Reservoirs_Gates_Transfers/#steps-to-add-reservoirs-gates-and-transfers","title":"Steps to Add Reservoirs, Gates, and Transfers","text":""},{"location":"tutorials/Tutorial_2_Reservoirs_Gates_Transfers/#1-create-the-reservoir","title":"1. Create the Reservoir","text":"<ol> <li>Navigate to the directory: <code>${DSM2_home}\\tutorial\\simple\\t2_reservoir_gate_transfer</code>.</li> <li>Open <code>hydro.inp</code>. At the bottom of the file, add the skeleton for the <code>RESERVOIR</code> table:</li> </ol> <pre><code>RESERVOIR\nNAME AREA BOT_ELEV\nEND\n</code></pre> <ol> <li>Enter the following values:</li> <li>Name: <code>res_1</code></li> <li>Area (million sq ft): <code>40</code></li> <li> <p>Bottom elevation (ft): <code>-24</code></p> </li> <li> <p>Add the <code>RESERVOIR_CONNECTION</code> table for the reservoir connections:</p> </li> </ol> <pre><code>RESERVOIR_CONNECTION\nRES_NAME NODE COEF_IN COEF_OUT\nres_1 3 200 200\nres_1 4 200 200\nEND\n</code></pre> <ol> <li>Save the file.</li> </ol>"},{"location":"tutorials/Tutorial_2_Reservoirs_Gates_Transfers/#2-add-initial-conditions-for-the-reservoir","title":"2. Add Initial Conditions for the Reservoir","text":"<ol> <li>Create the <code>RESERVOIR_IC</code> table to set the initial stage:</li> </ol> <pre><code>RESERVOIR_IC\nRES_NAME STAGE\nres_1 0.0\nEND\n</code></pre> <ol> <li>Save the file.</li> </ol>"},{"location":"tutorials/Tutorial_2_Reservoirs_Gates_Transfers/#3-create-the-gate","title":"3. Create the Gate","text":"<ol> <li>Add the skeleton for the <code>GATE</code> table:</li> </ol> <pre><code>GATE\nNAME FROM_OBJ FROM_IDENTIFIER TO_NODE\nEND\n</code></pre> <ol> <li>Enter the following values:</li> <li>Name: <code>gate_1</code></li> <li>From object: <code>channel</code></li> <li>From identifier: <code>2</code> (refers to Channel 2)</li> <li> <p>To node: <code>2</code> (refers to Node 2)</p> </li> <li> <p>Add the <code>GATE_WEIR_DEVICE</code> table for the weir:</p> </li> </ol> <pre><code>GATE_WEIR_DEVICE\nGATE_NAME DEVICE NDUPLICATE WIDTH ELEV HEIGHT CF_FROM_NODE CF_TO_NODE DEFAULT_OP\ngate_1 weir 2 20 2 9999.0 0.8 0.8 gate_open\nEND\n</code></pre> <ol> <li>Add the <code>GATE_PIPE_DEVICE</code> table for the pipe:</li> </ol> <pre><code>GATE_PIPE_DEVICE\nGATE_NAME DEVICE NDUPLICATE RADIUS ELEV CF_FROM_NODE CF_TO_NODE DEFAULT_OP\ngate_1 pipe 2 2 2 0.8 0.8 gate_open\nEND\n</code></pre> <ol> <li>Save the file.</li> </ol>"},{"location":"tutorials/Tutorial_2_Reservoirs_Gates_Transfers/#4-create-the-transfer","title":"4. Create the Transfer","text":"<ol> <li>Add the skeleton for the <code>TRANSFER</code> table:</li> </ol> <pre><code>TRANSFER\nNAME FROM_OBJ FROM_IDENTIFIER TO_OBJ TO_IDENTIFIER\nEND\n</code></pre> <ol> <li>Enter the following values:</li> <li>Name: <code>transfer_1</code></li> <li>From object: <code>reservoir</code></li> <li>From identifier: <code>res_1</code></li> <li>To object: <code>node</code></li> <li> <p>To identifier: <code>6</code></p> </li> <li> <p>Save the file.</p> </li> </ol>"},{"location":"tutorials/Tutorial_2_Reservoirs_Gates_Transfers/#5-add-the-transfer-flow-time-series","title":"5. Add the Transfer Flow Time Series","text":"<ol> <li>Add the <code>INPUT_TRANSFER_FLOW</code> table to assign a flow to the transfer:</li> </ol> <pre><code>INPUT_TRANSFER_FLOW\nTRANSFER_NAME FILLIN FILE PATH\ntransfer_1 last constant 40\nEND\n</code></pre> <ol> <li>Save the file.</li> </ol>"},{"location":"tutorials/Tutorial_2_Reservoirs_Gates_Transfers/#running-hydro-and-qual","title":"Running HYDRO and QUAL","text":"<ol> <li>Navigate to the directory: <code>${DSM2_home}\\tutorial\\simple\\t2_reservoir_gate_transfer</code>.</li> <li>Open a command window in the directory.</li> <li>Run the following commands:</li> </ol> <pre><code>hydro hydro.inp\nqual qual.inp\n</code></pre> <ol> <li>Open the <code>output.dss</code> file in the directory and examine the results.</li> </ol>"},{"location":"tutorials/Tutorial_2_Reservoirs_Gates_Transfers/#brain-teasers","title":"Brain Teasers","text":"<ol> <li>Compare the equation for inflow from a node to a reservoir through a gate with the reservoir connection equation. What terms in the gate equation does the reservoir coefficient lump together?</li> <li>Clifton Court Forebay has five duplicate radial gates with a crest elevation of -10.1 ft and a width of 20 ft.</li> <li>If water is at 0 ft and the gates are open, what is the area exposed to flow?</li> <li>If the weirs are perfectly efficient (coefficient = 1.0), what would be the equivalent \"lumped\" reservoir coefficient?</li> <li>Was the DSM2v6 calibrated reservoir coefficient of 1800 physical? Why might it have been acceptable given the model assumptions?</li> </ol>"},{"location":"tutorials/Tutorial_3_Layering/","title":"Tutorial 3: Layering","text":""},{"location":"tutorials/Tutorial_3_Layering/#task","title":"Task","text":"<ul> <li>Separate DSM2 input data into multiple input files.</li> <li>Use layers in DSM2 to group related items.</li> </ul>"},{"location":"tutorials/Tutorial_3_Layering/#skills-gained","title":"Skills Gained","text":"<p>Learn how to use layering in DSM2 to add, change, and delete features in a DSM2 simulation, for example, including a new reservoir in a simulation.</p> <p>The purpose of this tutorial is to demonstrate the use of layering to structure your project. Layers are part of the DSM2 data management system. They allow input items to be grouped in logical bundles and allow changes to be brought into an old simulation without erasing or altering archived items. At the same time, we will neaten up our input by dividing it into several files that are \"included\" from a fairly sparse primary file. The layering concept will be demonstrated by adding a \"dummy\" reservoir connected to nodes 5 and 6 (Figure 1) that will be \"turned on\" or \"turned off\" in a simulation. We will also use <code>DSM2MODIFIER</code> to differentiate between alternative simulations.</p> <p> Figure 1: Simple channel with a reservoir, gate, flow transfer, and dummy reservoir.</p>"},{"location":"tutorials/Tutorial_3_Layering/#1-convert-the-previous-hydroinp-grid-items-to-external-files","title":"1. Convert the Previous <code>hydro.inp</code> GRID Items to External Files","text":"<p>To use layers, the input tables must be gathered into individual input files.</p> <p>Key Points about Layering: - Each file represents a layer. - Information in the launch file (<code>hydro.inp</code> or <code>qual.inp</code>) supersedes all other input information. - For include blocks, files read later replace files read earlier. If the same type of input information exists in more than one file, the last information read will overwrite the previously read values. - Overriding values is based on an identifier (e.g., <code>NAME</code> or <code>NAME</code> and <code>VARIABLE</code>\u2014identifiers are listed in table reference documentation). - Parent and child tables (e.g., <code>CHANNEL</code> and <code>XSECT</code>) must be grouped in the same file. - If a parent item is overridden, all the child items associated with the overridden parent item are ignored.</p>"},{"location":"tutorials/Tutorial_3_Layering/#steps","title":"Steps:","text":"<ol> <li>Navigate to the <code>t3_layering</code> directory.</li> <li>Create a new file called <code>grid_tutorial_base.inp</code>.</li> <li>Open <code>hydro.inp</code> and:</li> <li>Locate the <code>CHANNEL</code> and <code>XSECT_LAYER</code> tables. Cut them and paste them into <code>grid_tutorial_base.inp</code>.</li> <li>Locate the <code>RESERVOIR</code> and <code>RESERVOIR_CONNECTION</code> tables. Cut them and paste them into <code>grid_tutorial_base.inp</code>. Leave the <code>RESERVOIR_IC</code> table in <code>hydro.inp</code>.</li> <li>Move the <code>TRANSFER</code> and <code>GATE</code> information to <code>grid_tutorial_base.inp</code>. Ensure the <code>GATE</code> child tables are also moved. Leave the <code>INPUT_TRANSFER_FLOW</code> table in <code>hydro.inp</code>.</li> <li>Ensure the data tables listed above have been removed from <code>hydro.inp</code>.</li> <li>Add the following lines to <code>hydro.inp</code> to include the external file:</li> </ol> <pre><code>GRID\ngrid_tutorial_base.inp\nEND\n</code></pre> <p>Note: Ensure there is a carriage return at the end of each <code>.inp</code> file.</p>"},{"location":"tutorials/Tutorial_3_Layering/#2-running-hydro-and-qual-with-grid-information-in-separate-files","title":"2. Running HYDRO and QUAL with Grid Information in Separate Files","text":"<p>This simulation will serve as the base case for comparison with other simulations. Use <code>DSM2MODIFIER</code> to differentiate between the various simulations. <code>DSM2MODIFIER</code> is a special <code>ENVVAR</code> definition automatically used by DSM2 to mark output (the F-Part of the DSS Path).</p>"},{"location":"tutorials/Tutorial_3_Layering/#steps_1","title":"Steps:","text":"<ol> <li>In the <code>ENVVAR</code> section of <code>hydro.inp</code> and <code>qual.inp</code>, change <code>DSM2MODIFIER</code> to <code>layers_base</code> and save the files.</li> <li>Navigate to the <code>t3_layering</code> directory.</li> <li>Open a command window in the directory.</li> <li>Run the following commands:</li> </ol> <pre><code>hydro hydro.inp\nqual qual.inp\n</code></pre> <ol> <li>Open the <code>output.dss</code> file in the <code>t3_layering</code> directory and examine the results.</li> </ol>"},{"location":"tutorials/Tutorial_3_Layering/#3-creating-a-new-reservoir","title":"3. Creating a New Reservoir","text":"<p>To add a feature, such as a new reservoir, follow these steps:</p>"},{"location":"tutorials/Tutorial_3_Layering/#steps_2","title":"Steps:","text":"<ol> <li>In <code>grid_tutorial_base.inp</code>, add the following data for the new reservoir below the data for <code>res_1</code>:</li> </ol> <pre><code>RESERVOIR\nNAME AREA BOT_ELEV\ndummy_res 60.0 -30.0\nEND\n\nRESERVOIR_CONNECTION\nRES_NAME NODE COEF_IN COEF_OUT\ndummy_res 5 220.0 220.0\ndummy_res 6 220.0 220.0\nEND\n</code></pre> <ol> <li>Save the file.</li> </ol>"},{"location":"tutorials/Tutorial_3_Layering/#4-running-hydro-and-qual-with-the-new-reservoir","title":"4. Running HYDRO and QUAL with the New Reservoir","text":"<p>This simulation is the first alternative, which adds a reservoir. Use <code>DSM2MODIFIER</code> to differentiate this simulation from the base simulation.</p>"},{"location":"tutorials/Tutorial_3_Layering/#steps_3","title":"Steps:","text":"<ol> <li>In the <code>ENVVAR</code> section of <code>hydro.inp</code> and <code>qual.inp</code>, change <code>DSM2MODIFIER</code> to <code>layers_dummyres</code> and save the files.</li> <li>Navigate to the <code>t3_layering</code> directory.</li> <li>Open a command window in the directory.</li> <li>Run the following commands:</li> </ol> <pre><code>hydro hydro.inp\nqual qual.inp\n</code></pre> <ol> <li>Compare the <code>layers_base.out</code> and <code>layers_dummyres.out</code> echoed input files to ensure the dummy reservoir was included in the simulation. Open the <code>output.dss</code> file and compare results from the base run and the new <code>dummyres</code> simulation.</li> </ol>"},{"location":"tutorials/Tutorial_3_Layering/#5-disabling-a-reservoir-using-a-revision-layer","title":"5. Disabling a Reservoir Using a Revision Layer","text":"<p>To disable the dummy reservoir, use a revision layer.</p>"},{"location":"tutorials/Tutorial_3_Layering/#steps_4","title":"Steps:","text":"<ol> <li>Create a file called <code>grid_tutorial_revision.inp</code>. Add this file to the <code>GRID</code> include-file section in <code>hydro.inp</code>:</li> </ol> <pre><code>GRID\ngrid_tutorial_base.inp\ngrid_tutorial_revision.inp\nEND\n</code></pre> <ol> <li>Copy the <code>RESERVOIR</code> and <code>RESERVOIR_CONNECTION</code> tables for <code>dummy_res</code> from <code>grid_tutorial_base.inp</code> to <code>grid_tutorial_revision.inp</code>. Add a caret (<code>^</code>) before the reservoir name in the parent table:</li> </ol> <pre><code>RESERVOIR\nNAME AREA BOT_ELEV\n^dummy_res 60.0 -30.0\nEND\n\nRESERVOIR_CONNECTION\nRES_NAME NODE COEF_IN COEF_OUT\ndummy_res 5 220.0 220.0\ndummy_res 6 220.0 220.0\nEND\n</code></pre> <ol> <li>Save the file.</li> </ol>"},{"location":"tutorials/Tutorial_3_Layering/#6-running-hydro-and-qual-disabling-the-new-reservoir","title":"6. Running HYDRO and QUAL Disabling the New Reservoir","text":""},{"location":"tutorials/Tutorial_3_Layering/#steps_5","title":"Steps:","text":"<ol> <li>In the <code>ENVVAR</code> section of <code>hydro.inp</code> and <code>qual.inp</code>, change <code>DSM2MODIFIER</code> to <code>layers_nodummyres</code> and save the files.</li> <li>Navigate to the <code>t3_layering</code> directory.</li> <li>Open a command window in the directory.</li> <li>Run the following commands:</li> </ol> <pre><code>hydro hydro.inp\nqual qual.inp\n</code></pre> <ol> <li>Compare the <code>layers_base.out</code>, <code>layers_dummyres.out</code>, and <code>layers_nodummyres.out</code> echoed input files and the <code>output.dss</code> file. Are the results the same for the base simulation and the no-dummy-reservoir simulation?</li> </ol>"},{"location":"tutorials/Tutorial_3_Layering/#7-changing-the-properties-of-a-reservoir","title":"7. Changing the Properties of a Reservoir","text":"<p>To change the properties of <code>res_1</code>, such as increasing its area:</p>"},{"location":"tutorials/Tutorial_3_Layering/#steps_6","title":"Steps:","text":"<ol> <li>In <code>grid_tutorial_revision.inp</code>, update the <code>RESERVOIR</code> table:</li> </ol> <pre><code>RESERVOIR\nNAME AREA BOT_ELEV\nres_1 50.0 -24.0\n^dummy_res 60.0 -30.0\nEND\n\nRESERVOIR_CONNECTION\nRES_NAME NODE COEF_IN COEF_OUT\nres_1 3 200.0 200.0\nres_1 4 200.0 200.0\ndummy_res 5 220.0 220.0\ndummy_res 6 220.0 220.0\nEND\n</code></pre> <ol> <li>Save the file.</li> </ol>"},{"location":"tutorials/Tutorial_3_Layering/#8-running-hydro-and-qual-with-increased-area-for-res_1","title":"8. Running HYDRO and QUAL with Increased Area for <code>res_1</code>","text":""},{"location":"tutorials/Tutorial_3_Layering/#steps_7","title":"Steps:","text":"<ol> <li>In the <code>ENVVAR</code> section of <code>hydro.inp</code> and <code>qual.inp</code>, change <code>DSM2MODIFIER</code> to <code>layers_larger_res1</code> and save the files.</li> <li>Navigate to the <code>t3_layering</code> directory.</li> <li>Open a command window in the directory.</li> <li>Run the following commands:</li> </ol> <pre><code>hydro hydro.inp\nqual qual.inp\n</code></pre> <ol> <li>Compare the output to earlier simulations.</li> </ol>"},{"location":"tutorials/Tutorial_3_Layering/#9-changing-the-name-of-channel-2004","title":"9. Changing the Name of Channel 2004","text":"<p>To replace the channel number of Channel 2004 with Channel 4:</p>"},{"location":"tutorials/Tutorial_3_Layering/#steps_8","title":"Steps:","text":"<ol> <li>In <code>grid_tutorial_revision.inp</code>, copy the <code>CHANNEL</code> and <code>XSECT_LAYER</code> data for Channel 2004. Update the channel number to 4 and disable Channel 2004 using a caret (<code>^</code>):</li> </ol> <pre><code>CHANNEL\nCHAN_NO LENGTH MANNING DISPERSION UPNODE DOWNNODE\n4 15000 0.035 0.3 4 5\n^2004 15000 0.035 0.3 4 5\nEND\n\nXSECT_LAYER\nCHAN_NO DIST ELEV AREA WIDTH WET_PERIM\n4 0.5 -24.0 0.0 40.0 40.0\n4 0.5 0.0 960.0 80.0 91.22\n4 0.5 20.0 2640.0 160.0 133.6\n2004 0.5 -24.0 0.0 40.0 40.0\n2004 0.5 0.0 960.0 80.0 91.22\n2004 0.5 20.0 2640.0 160.0 133.6\nEND\n</code></pre> <ol> <li>Save the file.</li> </ol>"},{"location":"tutorials/Tutorial_3_Layering/#10-add-initial-conditions-for-channel-4","title":"10. Add Initial Conditions for Channel 4","text":""},{"location":"tutorials/Tutorial_3_Layering/#steps_9","title":"Steps:","text":"<ol> <li>Create a file called <code>channel_ic_revision.inp</code> and add the following:</li> </ol> <pre><code>CHANNEL_IC\nCHAN_NO DISTANCE STAGE FLOW\n4 0 0.0 0.0\n4 length 0.0 0.0\nEND\n</code></pre> <ol> <li>Update <code>hydro.inp</code> to include the file:</li> </ol> <pre><code>INITIAL_CONDITION\nchannel_ic_revision.inp\nEND\n</code></pre> <ol> <li>Save the files.</li> </ol>"},{"location":"tutorials/Tutorial_3_Layering/#11-running-hydro-and-qual-with-channel-4","title":"11. Running HYDRO and QUAL with Channel 4","text":""},{"location":"tutorials/Tutorial_3_Layering/#steps_10","title":"Steps:","text":"<ol> <li>In the <code>ENVVAR</code> section of <code>hydro.inp</code> and <code>qual.inp</code>, change <code>DSM2MODIFIER</code> to <code>layers_ch2004_to_ch4</code> and save the files.</li> <li>Navigate to the <code>t3_layering</code> directory.</li> <li>Open a command window in the directory.</li> <li>Run the following commands:</li> </ol> <pre><code>hydro hydro.inp\nqual qual.inp\n</code></pre> <ol> <li>Examine the results in the <code>output.dss</code> file and the echoed input file.</li> </ol>"},{"location":"tutorials/Tutorial_3_Layering/#brain-teaser","title":"Brain Teaser","text":"<p>For the same change in elevation between the reservoir and connecting node, which reservoir would have a higher flow, <code>res_1</code> or <code>dummy_res</code>?</p> <p>Table 1: Include Blocks for DSM2 Input Files</p> Include Block Sections CONFIGURATION ENVVAR, SCALAR GRID CHANNEL, XSECT (child), XSECT_LAYER (child), RESERVOIR, RESERVOIR_CONNECTION (child), GATE, GATE_WEIR_DEVICE (child), GATE_PIPE_DEVICE (child), TRANSFER GROUPS GROUP, GROUP_MEMBER (child) HYDRO_TIME_SERIES INPUT_TRANSFER_FLOW, INPUT_GATE, BOUNDARY_STAGE, BOUNDARY_FLOW, SOURCE_FLOW, SOURCE_FLOW_RESERVOIR INITIAL_CONDITION CHANNEL_IC, RESERVOIR_IC OPERATION OPERATING_RULE, OPRULE_EXPRESSION, OPRULE_TIME_SERIES OUTPUT_TIME_SERIES OUTPUT_CHANNEL, OUTPUT_RESERVOIR, OUTPUT_CHANNEL_SOURCE_TRACK, OUTPUT_RESERVOIR_SOURCE_TRACK, OUTPUT_GATE PARTICLE PARTICLE_INSERTION, PARTICLE_FLUX_OUTPUT, PARTICLE_GROUP_OUTPUT QUAL_SPATIAL RATE_COEFFICIENT QUAL_TIME_SERIES INPUT_CLIMATE, NODE_CONCENTRATION, RESERVOIR_CONCENTRATION"},{"location":"tutorials/Tutorial_3_Layering/#learning-more","title":"Learning More","text":"<p>Overriding is easy to understand. The main things you will need to keep in mind are:</p> <ol> <li>Understanding how child table replacement works:</li> <li>You can't replace the child element without replacing the parent.</li> <li>The children of an overridden parent element are never used.</li> <li>What is the unique identifier for each row in a table? In most cases, this is the first field and it is usually a name or a map number (it is a label rather than a piece of hard data). In some cases (e.g., output), the unique identifier may be two fields such as <code>NAME</code> and <code>VARIABLE</code> for output. Overriding only occurs when the identifier for the row is duplicated. This information is available in the table reference documentation in the \"documentation\" folder.</li> <li>Which data can be included in which blocks. For instance, <code>GRID</code> can contain <code>CHANNEL</code>, <code>GATE</code>, <code>RESERVOIR</code>, and <code>TRANSFER</code> data. This information is given in Table 1.</li> </ol>"},{"location":"tutorials/Tutorial_3_Layering/#brain-teaser_1","title":"Brain Teaser","text":"<p>For the same change in elevation between the reservoir and connecting node, which reservoir would have a higher flow, <code>res_1</code> or <code>dummy_res</code>?</p>"},{"location":"tutorials/Tutorial_4_Time_Varying_Data/","title":"Tutorial 4: Time-Varying Data","text":""},{"location":"tutorials/Tutorial_4_Time_Varying_Data/#task","title":"Task","text":"<p>Convert the boundary conditions and gate operations from constants to time-varying input data.</p>"},{"location":"tutorials/Tutorial_4_Time_Varying_Data/#skills-gained","title":"Skills Gained","text":"<ul> <li>Learn about HEC-DSS as a time series data storage system.</li> <li>Learn how HEC-DSS path names are used to reference time series in DSM2 input files.</li> </ul> <p>The purpose of this tutorial is to incorporate time-varying information into the model. In the previous sections, all boundary conditions and gate timings were set as constant, and no input files were needed. In this section, the model is set to read time-varying information stored in HEC-DSS files.</p> <p> The U.S. Army Corps of Engineers' Hydrologic Engineering Center Data Storage System, or HEC-DSS, is a database system designed to efficiently store and retrieve scientific data that is typically sequential. Such data types include, but are not limited to, time series data, curve data, spatial-oriented gridded data, and others. The system was designed to make it easy for users and application programs to retrieve and store data.</p> <p>Data in HEC-DSS format can be viewed using special software including VISTA (DWR) or HEC-DSSVue. Each time series is described in the database using DSS Pathnames. For DSM2, the pathnames are typically used as follows:</p> <p></p> <ul> <li>A-Part: Data Source  </li> <li>B-Part: Location  </li> <li>C-Part: Variable  </li> <li>D-Part: Date range  </li> <li>E-Part: Data frequency  </li> <li>F-Part: Description (e.g., CalSim run identifier).</li> </ul> <p>For more information, see the HEC-DSS website.</p>"},{"location":"tutorials/Tutorial_4_Time_Varying_Data/#steps-to-incorporate-time-varying-data","title":"Steps to Incorporate Time-Varying Data","text":""},{"location":"tutorials/Tutorial_4_Time_Varying_Data/#1-change-the-transfer-flows-to-hec-dss-input","title":"1. Change the Transfer Flows to HEC-DSS Input","text":"<ol> <li>Create a new file called <code>input_hydro_ts_tutorial.inp</code>.</li> <li>In the new file, create the <code>INPUT_TRANSFER_FLOW</code> table:</li> </ol> <pre><code>INPUT_TRANSFER_FLOW\nTRANSFER_NAME FILLIN FILE PATH\ntransfer_1 linear ${TUTORIALINPUT} /TUTORIAL/TRANSFER/FLOW//15MIN/CONSTANT/\nEND\n</code></pre> <p>Note: HEC-DSS pathnames use forward slashes: <code>/A-Part/B-Part/C-Part/D-Part/E-Part/F-Part/</code>. In the example above, the A-Part is <code>TUTORIAL</code>, the B-Part is <code>TRANSFER</code>, etc.</p> <ol> <li>Open <code>hydro.inp</code> and add the following <code>ENVVAR</code> definition:</li> </ol> <pre><code>ENVVAR\nNAME VALUE\nHYDROOUTDSSFILE output.dss\nDSM2MODIFIER timevar_1\nTUTORIALINPUT ../timeseries/tutorial.dss\nEND\n</code></pre> <ol> <li>Update the <code>HYDRO_TIME_SERIES</code> block in <code>hydro.inp</code> to include the new file:</li> </ol> <pre><code>HYDRO_TIME_SERIES\ninput_boundary_hydro_tutorial.inp\ninput_transfer_flow_tutorial.inp\ninput_hydro_ts_tutorial.inp\nEND\n</code></pre> <ol> <li>Save the files.</li> <li>Open <code>qual.inp</code> and set <code>DSM2MODIFIER</code> to <code>timevar_1</code> to match <code>hydro.inp</code>.</li> </ol>"},{"location":"tutorials/Tutorial_4_Time_Varying_Data/#2-running-hydro-and-qual","title":"2. Running HYDRO and QUAL","text":"<ol> <li>Navigate to the directory: <code>${DSM2_home}\\tutorial\\simple\\t4_timevar</code>.</li> <li>Open a command window in the directory.</li> <li>Run the following commands:</li> </ol> <pre><code>hydro hydro.inp\nqual qual.inp\n</code></pre> <ol> <li>Open the <code>output.dss</code> file in the <code>t4_timevar</code> directory and verify the results.</li> </ol>"},{"location":"tutorials/Tutorial_4_Time_Varying_Data/#3-adjust-dsm2modifier-for-a-variant-scenario","title":"3. Adjust DSM2MODIFIER for a Variant Scenario","text":"<ol> <li>Open <code>hydro.inp</code> and change <code>DSM2MODIFIER</code> to <code>timevar_2</code> in the <code>ENVVAR</code> section.</li> <li>Open <code>qual.inp</code> and make the same change to <code>DSM2MODIFIER</code>.</li> </ol>"},{"location":"tutorials/Tutorial_4_Time_Varying_Data/#4-add-source-information-into-hydro","title":"4. Add Source Information into HYDRO","text":"<ol> <li>In <code>input_hydro_ts_tutorial.inp</code>, add the <code>SOURCE_FLOW</code> table:</li> </ol> <pre><code>SOURCE_FLOW\nNAME NODE SIGN FILLIN FILE PATH\nsource1 5 1 linear ${TUTORIALINPUT} /TUTORIAL/SOURCE/FLOW//15MIN/CONSTANT/\nEND\n</code></pre> <ol> <li>Save the file.</li> </ol>"},{"location":"tutorials/Tutorial_4_Time_Varying_Data/#5-add-corresponding-source-information-into-qual","title":"5. Add Corresponding Source Information into QUAL","text":"<ol> <li>Create a new file called <code>input_qual_ts_tutorial.inp</code>.</li> <li>Add the <code>NODE_CONCENTRATION</code> table:</li> </ol> <pre><code>NODE_CONCENTRATION\nNAME NODE_NO VARIABLE FILLIN FILE PATH\nsource1 5 ec last ${TUTORIALINPUT} /TUTORIAL/SOURCE/EC//15MIN/CONSTANT/\nEND\n</code></pre> <ol> <li>Add the <code>TUTORIALINPUT</code> definition to <code>qual.inp</code>:</li> </ol> <pre><code>ENVVAR\nNAME VALUE\nTUTORIALINPUT ../timeseries/tutorial.dss\nEND\n</code></pre> <ol> <li>Update the <code>QUAL_TIME_SERIES</code> block in <code>qual.inp</code>:</li> </ol> <pre><code>QUAL_TIME_SERIES\ninput_node_conc_tutorial.inp\ninput_qual_ts_tutorial.inp\nEND\n</code></pre> <ol> <li>Save the files.</li> </ol>"},{"location":"tutorials/Tutorial_4_Time_Varying_Data/#6-add-time-varying-tide-information-for-downstream-boundary-in-hydro","title":"6. Add Time-Varying Tide Information for Downstream Boundary in HYDRO","text":"<ol> <li>In <code>input_hydro_ts_tutorial.inp</code>, add the <code>BOUNDARY_STAGE</code> table:</li> </ol> <pre><code>BOUNDARY_STAGE\nNAME NODE FILLIN FILE PATH\ndownstream_stage 7 linear ${TUTORIALINPUT} /TUTORIAL/DOWNSTREAM/STAGE//15MIN/REALISTIC/\nEND\n</code></pre>"},{"location":"tutorials/Tutorial_4_Time_Varying_Data/#7-add-downstream-boundary-in-qual","title":"7. Add Downstream Boundary in QUAL","text":"<ol> <li>In <code>input_qual_ts_tutorial.inp</code>, add the following to the <code>NODE_CONCENTRATION</code> table:</li> </ol> <pre><code>NODE_CONCENTRATION\nNAME NODE_NO VARIABLE FILLIN FILE PATH\ndownstream_stage 7 ec last ${TUTORIALINPUT} /TUTORIAL/DOWNSTREAM/EC//15MIN/REALISTIC/\nEND\n</code></pre>"},{"location":"tutorials/Tutorial_4_Time_Varying_Data/#8-add-a-gate-time-series-to-hydro","title":"8. Add a Gate Time Series to HYDRO","text":"<ol> <li>Create a new file called <code>input_gate_tutorial.inp</code>.</li> <li>Add the <code>INPUT_GATE</code> table:</li> </ol> <pre><code>INPUT_GATE\nGATE DEVICE VARIABLE FILLIN FILE PATH\ngate_1 weir op_from_node none ${TUTORIALINPUT} /TUTORIAL/GATE/FLAP_OP//IR-YEAR/TIMEVAR/\nEND\n</code></pre> <ol> <li>Update the <code>HYDRO_TIME_SERIES</code> block in <code>hydro.inp</code>:</li> </ol> <pre><code>HYDRO_TIME_SERIES\ninput_boundary_hydro_tutorial.inp\ninput_transfer_flow_tutorial.inp\ninput_hydro_ts_tutorial.inp\ninput_gate_tutorial.inp\nEND\n</code></pre> <ol> <li>Save the files.</li> </ol>"},{"location":"tutorials/Tutorial_4_Time_Varying_Data/#9-running-hydro-and-qual","title":"9. Running HYDRO and QUAL","text":"<ol> <li>Navigate to the directory: <code>${DSM2_home}\\tutorial\\simple\\t4_timevar</code>.</li> <li>Open a command window in the directory.</li> <li>Run the following commands:</li> </ol> <pre><code>hydro hydro.inp\nqual qual.inp\n</code></pre> <ol> <li>Open the <code>output.dss</code> file in the <code>t4_timevar</code> directory and examine the results.</li> </ol>"},{"location":"tutorials/Tutorial_5_Advanced_Output_and_Source_Tracking/","title":"Tutorial 5: Advanced Output and Source Tracking","text":""},{"location":"tutorials/Tutorial_5_Advanced_Output_and_Source_Tracking/#task","title":"Task","text":"<ul> <li>Create boundary and source groups.</li> <li>Request output for constituent source tracking.</li> </ul>"},{"location":"tutorials/Tutorial_5_Advanced_Output_and_Source_Tracking/#skills-gained","title":"Skills Gained","text":"<p>Learn how to use advanced output options in DSM2, including source tracking.</p> <p>The purpose of this tutorial is to provide instruction on advanced output options in DSM2. Basic outputs include flow, stage, and constituent concentrations at nodes and channel locations. Advanced outputs include creating output groups and source tracking.</p>"},{"location":"tutorials/Tutorial_5_Advanced_Output_and_Source_Tracking/#add-output-paths-to-hydroinp","title":"Add Output Paths to <code>hydro.inp</code>","text":"<ol> <li>Navigate to the directory: <code>{DSM2_home}\\tutorial\\simple\\t5_output</code>.</li> <li>Open the file <code>addin.inp</code> and note the new output paths for the channels and reservoir.</li> <li>Copy the entire file contents to the clipboard.</li> <li>Open the file <code>hydro.inp</code>.</li> <li>Navigate to the bottom of the file and paste the information.</li> </ol> <p>Note: The output request in the launch file (e.g., <code>hydro.inp</code> or <code>qual.inp</code>) supersedes all other output requests that have the same identifier. To ensure outputs at both locations, assign unique identifiers to each location, e.g., <code>bnd_1</code> for location 0 and <code>bnd_100</code> for location 100.</p>"},{"location":"tutorials/Tutorial_5_Advanced_Output_and_Source_Tracking/#add-boundary-and-source-groups","title":"Add Boundary and Source Groups","text":"<p>GROUPS are user-defined groups of model objects, for instance groups of water bodies or groups of boundary inputs. Groups are used a number of places in DSM2, including: tracking of constituents originated from grouped sources, tracking of particles as they reside or move between groups of water bodies and/or boundaries, and assignment of rate coefficients in QUAL to groups of water bodies. In the output specifications, groups are used to define aggregate sources for source tracking. For example, output groups could be used to track mass originating from all the boundaries, or from all Delta Island Consumptive Use (DICU) diversions, etc. In this section, we will create two output groups: boundary locations and water quality constituent source locations.</p> <ol> <li>In the study directory, create a file called <code>group_tutorial.inp</code>.</li> <li> <p>In the <code>group_tutorial.inp</code> file, add a group table. Note that this is a parent table for overwriting/layering purposes. Define a boundary and a sources group:</p> <p><code>GROUP NAME boundary sources END</code></p> </li> <li> <p>Now define the group members. Create the GROUP_MEMBER table below the GROUP table:</p> <p><code>GROUP_MEMBER GROUP_NAME MEMBER_TYPE PATTERN END</code></p> </li> <li> <p>In the Group Members table:</p> <ol> <li> <p>Enter a row with the following values in the appropriate fields:</p> <ul> <li>GROUP_NAME: <code>boundary</code></li> <li>MEMBER_TYPE: <code>stage</code></li> <li>PATTERN: <code>.*stream.*</code></li> </ul> <p>Note: The dot-star <code>.*</code> in the above pattern is a \"regular expression\" wildcard. You can use any standard Perl-style regular expression in groups, but the html documentation for GROUPS describes most of the patterns you can put in a GROUP_MEMBER that are really useful.</p> </li> </ol> </li> <li> <p>Enter another row with the following values in the appropriate fields:</p> <ul> <li>GROUP_NAME: <code>boundary</code></li> <li>MEMBER_TYPE: <code>flow_boundary</code></li> <li>PATTERN: <code>.*stream.*</code></li> </ul> </li> <li> <p>In the Group Members table insert another row with the following values in the appropriate fields:</p> <ul> <li>GROUP_NAME: <code>sources</code></li> <li>MEMBER_TYPE: <code>source_sink</code></li> <li>PATTERN: <code>source1</code></li> </ul> </li> <li> <p>In the <code>qual.inp</code> file, create the GROUPS (note the plural) include block that will reference this file:</p> <p><code>GROUPS group_tutorial.inp END</code></p> </li> <li> <p>Save the current settings.</p> </li> </ol>"},{"location":"tutorials/Tutorial_5_Advanced_Output_and_Source_Tracking/#source-tracking","title":"Source Tracking","text":"<p>Source tracking (aka fingerprinting) determines the amount of water or of a constituent at one location that originated from a specified location. For constituent fingerprinting, 1) define a source group (e.g. boundaries or DICU locations), and then 2) request output for that group. For volumetric fingerprinting that indicates the percentage of flow that originated from each boundary location, 1) create a fingerprinting constituent and set its value equal to 100 at all boundaries, 2) define a source group for all boundaries, and 3) request output from that source group.</p>"},{"location":"tutorials/Tutorial_5_Advanced_Output_and_Source_Tracking/#add-source-tracking-output-for-channel-5","title":"Add Source Tracking Output for Channel 5","text":"<p>To demonstrate source tracking, this part of the tutorial examines how much of the EC in channel 5 (see Figure 1) came from the boundaries and from other sources. For comparison purposes, the EC from all sources will also be output. Create a new file called <code>output_qual_sourcetrack.inp</code>.</p> <ol> <li>In this file, create an OUTPUT_CHANNEL_SOURCE_TRACK table. Refer to the documentation to create the header.</li> <li>In the Channel Output table create 3 rows:<ol> <li>For the first new row, enter the following values into the appropriate fields:<ul> <li>Name: <code>ch5</code></li> <li>Channel: <code>5</code></li> <li>Distance: <code>5000</code></li> <li>Variable: <code>ec</code></li> <li>Source Group: <code>all</code> (this will track ec from all sources)</li> <li>Output File: <code>${QUALOUTDSSFILE}</code></li> <li>Time Interval: <code>15min</code></li> <li>Period Op: <code>inst</code></li> </ul> </li> <li>For the second new row, enter the following values into the appropriate fields:<ul> <li>Name: <code>ch5</code></li> <li>Channel: <code>5</code></li> <li>Distance: <code>5000</code></li> <li>Variable: <code>ec</code></li> <li>Source Group: <code>boundary</code></li> <li>Output File: <code>${QUALOUTDSSFILE}</code></li> <li>Time Interval: <code>15min</code></li> <li>Period Op: <code>inst</code></li> </ul> </li> <li>For the third new row, enter the following values into the appropriate fields:<ul> <li>Name: <code>ch5</code></li> <li>Channel: <code>5</code></li> <li>Distance: <code>5000</code></li> <li>Variable: <code>ec</code></li> <li>Source Group: <code>sources</code></li> <li>Output File: <code>${QUALOUTDSSFILE}</code></li> <li>Time Interval: <code>15min</code></li> <li>Period Op: <code>inst</code></li> </ul> </li> </ol> </li> <li>Save the current settings.</li> </ol>"},{"location":"tutorials/Tutorial_5_Advanced_Output_and_Source_Tracking/#running-hydro-and-qual","title":"Running HYDRO and QUAL","text":"<ol> <li>Open a command window for the <code>t5_output</code> directory.</li> <li>In the command window, type: <code>hydro hydro.inp</code>.</li> <li>In the command window, type: <code>qual qual.inp</code>.</li> <li>Open the hydro echo file <code>output_tutorial_hydro_echo.inp</code>. Which version of <code>bnd_1</code> got picked up by the model, the one in <code>hydro.inp</code> or the one in <code>output_hydro_tutorial.inp</code>.</li> <li>Open the <code>output.dss</code> file in the <code>t5_output</code> directory, and examine the results. Do a mass balance to make sure the source tracking adds up.</li> </ol>"},{"location":"tutorials/Tutorial_5_Advanced_Output_and_Source_Tracking/#brain-teaser","title":"Brain Teaser","text":"<p>How would you set up a source tracking simulation to determine what percentage of water/flow at a given location originated from a specified boundary?</p>"},{"location":"tutorials/Tutorial_6_Operating_Rules/","title":"Tutorial 6: Operating Rules","text":""},{"location":"tutorials/Tutorial_6_Operating_Rules/#task","title":"Task","text":"<ul> <li>Operate a gate based on stage criteria.</li> <li>Regulate a source/sink inflow.</li> </ul>"},{"location":"tutorials/Tutorial_6_Operating_Rules/#skills-gained","title":"Skills Gained","text":"<p>Learn how to use Operating Rule Language (ORL) statements to set gate operations and flows dynamically.</p> <p>The purpose of this tutorial is to practice using ORL statements to set gate operations and flows dynamically. For example, a gate can automatically close when stage conditions reach a threshold. In this tutorial, we will create operating rules to operate a gate and regulate a source/sink inflow.</p> <p>Extensive documentation on DSM2 operating rules can be found in the DSM2 documentation under the START menu.</p> <p>Figure 1: Simple channel network with a reservoir, gate, flow transfer, and dummy reservoir.</p>"},{"location":"tutorials/Tutorial_6_Operating_Rules/#adding-a-second-gate-where-op-rule-will-be-applied","title":"Adding a Second Gate Where Op Rule Will Be Applied","text":"<ol> <li>Navigate to the directory: <code>{DSM2_home}\\tutorial\\simple\\t6_oprule</code>.</li> <li>Create a file <code>grid_tutorial_opitems.inp</code>.</li> <li>Open <code>grid_tutorial_base.inp</code> and copy the GATE table with <code>gate_1</code>. Paste it into <code>grid_tutorial_opitems.inp</code> and make the following changes:</li> <li>NAME: <code>gate_2</code></li> <li>TO_NODE: 5</li> </ol>"},{"location":"tutorials/Tutorial_6_Operating_Rules/#adding-output-for-the-second-gate","title":"Adding Output for the Second Gate","text":"<ol> <li>Create a file called <code>output_oprule_tutorial.inp</code>.</li> <li>Create the OUTPUT_GATE table:</li> </ol> <pre><code>OUTPUT_GATE\nNAME GATE_NAME DEVICE VARIABLE INTERVAL PERIOD_OP FILE\nEND\n</code></pre> <ol> <li>In the output table, enter the following values into the appropriate fields:</li> <li>Output Name: <code>gate_2_weirop</code></li> <li>Gate Name: <code>gate_2</code></li> <li>Device: <code>weir</code></li> <li>Variable: <code>op-from-node</code></li> <li>Time Interval: <code>15min</code></li> <li>Period Op: <code>inst</code></li> <li> <p>File: <code>${HYDROOUTDSSFILE}</code></p> </li> <li> <p>Add the following channel outputs in a new OUTPUT_CHANNEL table:</p> </li> </ol> <pre><code>OUTPUT_CHANNEL\nNAME CHAN_NO DISTANCE VARIABLE INTERVAL PERIOD_OP FILE\ntrigger_loc 4 7500 stage 15min inst ${HYDROOUTDSSFILE}\nds_gate2 5 0 flow 15min inst ${HYDROOUTDSSFILE}\nEND\n</code></pre> <ol> <li>Add the output layer to the list of include files in <code>hydro.inp</code> and save your work.</li> </ol>"},{"location":"tutorials/Tutorial_6_Operating_Rules/#create-an-operating-rule-to-close-the-weir-when-stage-is-low","title":"Create an Operating Rule to Close the Weir when Stage is Low","text":"<ol> <li>Create a file called <code>oprule_tutorial.inp</code>.</li> <li>Create the <code>Operating Rules</code> table:</li> </ol> <pre><code>OPERATING_RULE\nNAME ACTION TRIGGER\nEND\n</code></pre> <ol> <li>Enter the following values into the appropriate fields:</li> <li>Name: <code>weir_close</code></li> <li>Action Definition: <code>\"SET gate_op(gate=gate_2, device=weir, direction=from_node) TO CLOSE RAMP 30MIN\"</code></li> <li> <p>Trigger Definition: <code>\"stage_critical AND op_applies\"</code></p> </li> <li> <p>Create an OPERATION include block in <code>hydro.inp</code> and add the new file so that it will be used by DSM2-HYDRO:</p> </li> </ol> <pre><code>OPERATION\noprule_tutorial.inp\nEND\n</code></pre> <ol> <li>Save the current settings.</li> </ol> <p>Note that the expressions <code>stage_critical</code> and <code>op_applies</code> will be created in a later step.</p>"},{"location":"tutorials/Tutorial_6_Operating_Rules/#create-an-operating-rule-to-open-the-weir-when-stage-is-high","title":"Create an Operating Rule to Open the Weir when Stage is High","text":"<p>As before, we will enter the rule to open the weir first in terms of the expressions <code>stage_relax</code> (a condition where stage is safely above a threshold where we can open the gate) and <code>op_applies</code>. In the next step, we will define these expressions.</p> <ol> <li>In the <code>Operating Rules</code> table, enter the following values into the appropriate fields:</li> <li>Name: <code>weir_open</code></li> <li>Action Definition: <code>\"SET gate_op(gate=gate_2, device=weir, direction=from_node) TO OPEN RAMP 30MIN\"</code></li> <li> <p>Trigger Definition: <code>\"(stage_relax AND op_applies) OR NOT(op_applies)\"</code></p> </li> <li> <p>Save the current settings.</p> </li> <li> <p>In the <code>hydro.inp</code> file, add the following environmental variables and values into the ENVVAR section:</p> </li> </ol> <pre><code>STAGE_CRITICAL 1.4\nSTAGE_RELAX 1.6\n</code></pre>"},{"location":"tutorials/Tutorial_6_Operating_Rules/#define-expressions-used-in-the-rule","title":"Define Expressions used in the rule","text":"<ol> <li>In the file <code>oprule_tutorial.inp</code>, create the <code>OPRULE_EXPRESSION</code> table:</li> </ol> <pre><code>OPRULE_EXPRESSION\nNAME DEFINITION\nEND\n</code></pre> <ol> <li>Enter the following values into the appropriate fields:</li> <li>Name: <code>op_applies</code></li> <li> <p>Definition: <code>\"SEASON &lt; 01FEB\"</code></p> </li> <li> <p>Enter the following values into the appropriate fields. Don't forget quotes!!</p> </li> <li>Name: <code>stage_critical</code></li> <li> <p>Definition: <code>\"chan_stage(channel=4, dist=7500) &lt; ${STAGE_CRITICAL}\"</code></p> </li> <li> <p>Enter the following values into the appropriate fields:</p> </li> <li>Name: <code>stage_relax</code></li> <li> <p>Definition: <code>\"chan_stage(channel=4, dist=7500) &gt; ${STAGE_RELAX}\"</code></p> </li> <li> <p>Save the current settings.</p> </li> <li> <p>Now run HYDRO and QUAL:</p> </li> <li>Open a command window for the <code>t6_oprule</code> directory.</li> <li>In the command window, type: <code>hydro hydro.inp</code>.</li> <li>In the command window, type: <code>qual qual.inp</code>.</li> <li>Open the <code>output.dss</code> file in the <code>t6_oprule</code> directory, and examine the results.</li> </ol>"},{"location":"tutorials/Tutorial_6_Operating_Rules/#add-a-reduced-flow-operating-rule","title":"Add a Reduced Flow Operating Rule","text":"<p>In our next operating rule, we will control the inflow to a node by having it toggle back and forth between a larger \"full flow\" and a \"reduced flow\". First, we will enter the rule and then we will define the full and reduced flows.</p> <ol> <li>In the <code>Operating Rules</code> table, enter the following values into the appropriate fields:</li> <li>Name: <code>flow_reduce</code></li> <li>Action Definition: <code>SET ext_flow(name=source1) TO ifelse(stage_critical,reduced_flow,full_flow)</code></li> <li> <p>Trigger Definition: <code>TRUE</code></p> </li> <li> <p>Now create the expressions that define <code>full_flow</code> and <code>reduced_flow</code>. In the <code>Oprule Expressions</code> table:</p> </li> <li> <p>Enter the following values into the appropriate fields that define <code>full_flow</code>. This will involve the time series <code>source_flow</code> which we will enter later:</p> <ul> <li>Input Name: <code>full_flow</code></li> <li>Definition: <code>ts(name=source_flow) [note: this is a reference to a time series we haven't defined yet].</code></li> </ul> </li> <li> <p>Do the same for <code>reduced_flow</code>. Note: we are defining <code>reduced_flow</code> in terms of the time series. There is no guarantee of what order expressions will be evaluated, so you cannot safely define <code>reduced_flow</code> in terms of another expression such as <code>full_flow</code>. Enter the following values into the appropriate fields:</p> </li> <li>Input Name: <code>reduced_flow</code></li> <li> <p>Definition: <code>0.5*ts(name=source_flow).</code></p> </li> <li> <p>Save the current settings.</p> </li> <li> <p>Now we will define the <code>source_flow</code> time series upon which the <code>full_flow</code> and <code>reduced_flow</code> expressions are based.</p> </li> <li>Create the <code>Operation Time Series</code> table:</li> </ol> <pre><code>OPRULE_TIME_SERIES\nNAME FILLIN FILE PATH\n</code></pre> <ol> <li>Enter the following values into the appropriate fields:</li> <li>Input Name: <code>source_flow</code></li> <li>Input File: <code>${TUTORIALINPUT}</code></li> <li>Path: <code>/TUTORIAL/SOURCE/FLOW//15MIN/CONSTANT/ [Note: there are two forward slashes between FLOW and 15MIN]</code></li> <li> <p>Fillin: <code>none</code></p> </li> <li> <p>Save the current settings.</p> </li> </ol>"},{"location":"tutorials/Tutorial_6_Operating_Rules/#override-the-expression-op_applies","title":"Override the Expression op_applies","text":"<p>Recall that <code>op_applies</code> is used to determine when the weir is operated. Previously the definition of this expression was seasonal: the expression was <code>SEASON &lt; 01FEB</code>. The goal now is to make the same expression depend on a time series. Rather than change the expression, we will override it in a new layer.</p> <ol> <li>Add a new Operating Rules Layer:</li> <li> <p>Create a file called <code>oprule_tutorial_revision.inp</code></p> </li> <li> <p>Redefine the expressions that define <code>op_applies</code>. In the <code>Expressions</code> table:</p> </li> <li>Create the <code>OPRULE_EXPRESSION</code> table.</li> <li> <p>Enter the following values into the appropriate fields:</p> <ul> <li>Input Name: <code>op_applies</code></li> <li>Definition: <code>\"ts(name=op_used)&gt;0.0\" [note: this is a reference to a time series we will define in the next step].</code></li> </ul> </li> <li> <p>Define the time series <code>op_used</code> on which the <code>op_applies</code> expression depends. In the <code>Operation Time Series</code> table:</p> </li> <li>Right-click and select <code>Insert row</code>.</li> <li> <p>Enter the following values into the appropriate fields:</p> <ul> <li>Input Name: <code>op_used</code></li> <li>Input File: <code>${TUTORIALINPUT}</code></li> <li>Path: <code>/TUTORIAL/GATE/FLAP_OP//IR-YEAR/TIMEVAR/</code></li> <li>Fillin: <code>none</code></li> </ul> </li> <li> <p>Add <code>oprule_tutorial_revision.inp</code> after <code>oprule_tutorial.inp</code> in the OPERATIONS block of <code>hydro.inp</code> so that it will be used by HYDRO.</p> </li> <li>Run HYDRO and QUAL and examine the results.</li> </ol>"},{"location":"tutorials/Tutorial_6_Operating_Rules/#attachments","title":"Attachments","text":""}]}